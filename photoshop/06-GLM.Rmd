# Régressions linéaires généralisées (GLM) {#chap06}

**Les modèles linéaires généralisés**

Dans ce chapitre, nous présenterons les modèles linéaires généralisés plus communément appelés GLM (*generalized linear models* en anglais). Il s’agit d’une extension directe du modèle de régression linéaire multiple (LM) basé sur la méthode des moindres carrés ordinaires, décrit dans le chapitre précédent. Pour aborder cette section sereinement, il est important d’avoir bien compris le concept de distribution présenté dans la section (section \@ref(sect024)). À la fin de cette section, vous serez en mesure de :

* comprendre la distinction entre un modèle LM classique et un GLM
* identifier les composantes d’un GLM
*	interpréter les résultats d’un GLM
*	effectuer les diagnostics d’un GLM


::: {.bloc_package data-latex=""}
Dans cette section, nous utiliserons principalement les packages suivants : 

* Pour créer des graphiques :
  - **ggplot2**, le seul, l'unique
  - **ggpubr** pour combiner des graphiques et réaliser des diagrammes
  
* Pour ajuster des modèles GLM :
  - **VGAM** et **gamlss** offrent tous les deux un très large choix de distributions et de fonctions de diagnostic, mais nécessitent plusieurs manipulations manuelles
  - **mgcv** offre moins de distributions que les deux précédents mais est plus simple d'utilisation

 * Pour analyser des modèles GLM :
  - **car** essentiellement pour la fonction `vif`
  - **DHARMa** pour le diagnostic des résidus simulés 
  - **DescTools** pour les tests de Lilliefors, Shapiro-Wilk, Anderson-Darling et Jarque-Bera
  - **ROCR** et **caret** pour l'analyse de la qualité d'ajustement de modèles pour des variables qualitatives
  - **sandwich** pour générer des erreurs standards robustes pour le modèle GLM logistique binomial
:::

## Qu'est qu'un modèle GLM? {#sect061}

Nous avons vu qu’une régression linéaire multiple (LM) ne peut être appliquée que si la variable dépendante analysée est continue et si elle est normalement distribuée, une fois les variables indépendantes contrôlées. Il s’agit d’une limite très importante puisqu’elle ne peut être utilisée pour modéliser et prédire des variables binaires, multinomiales, de comptage, ordinales ou plus simplement des données anormalement distribuées. Une seconde limite importante des LM est que l’influence des variables indépendantes sur la variable dépendante ne peut être que linéaire. L’augmentation d’une unité de *X* conduit à une augmentation (ou diminution) de $\beta$ (coefficient de régression) unités de *Y*, ce qui n’est pas toujours représentatif des phénomènes étudiés. Afin de dépasser ces contraintes, @GLMnelder ont proposé une extension des modèles LM, soit les modèles linéaires généralisés (GLM).

### Formulation d'un GLM {#sect0611}

Puisqu’un modèle GLM est une extension des modèles LM, il est possible de traduire un modèle LM sous forme d’un GLM. Nous utilisons ce point de départ pour détailler la morphologie d’un GLM. Nous avons vu dans la section précédente qu’un modèle LM correspond à la formule suivante (notation matricielle) :

\footnotesize
\begin{equation}
Y = \beta_0 + X\beta + \epsilon
(\#eq:regmultiple5)
\end{equation}
\normalsize

Avec $\beta_0$ la constante (*intercept* en anglais) et $\beta$ un vecteur de coefficients de régression pour les *k* variables indépendantes (*X*).

D'après cette formule, nous modélisons la variable *Y* avec une équation de régression linéaire et un terme d’erreur que l’on estime être normalement distribué. Nous pouvons reformuler ce simple LM sous forme d’un GLM avec l’écriture suivante : 

\footnotesize
\begin{equation}
\begin{aligned}
&Y \sim Normal(\mu,\sigma)\\
&g(\mu) = \beta_0 + \beta X\\
&g(x) = x
\end{aligned}
(\#eq:glm1)
\end{equation}
\normalsize

Pas de panique ! Cette écriture se lit comme suit : La variable *Y* est issue d’une distribution normale $(Y \sim Normal)$ avec deux paramètres : $\mu$ (sa moyenne) et $\sigma$ (son écart type). $\mu$ varie en fonction d’une équation de régression linéaire ($\beta_0 + \beta X$) transformée par une fonction de lien *g* (définie plus tard). Dans ce cas précis, la fonction de lien est appelée fonction identitaire puisqu’elle n’applique aucune transformation ($g(x) = x$). Vous noterez ici que le second paramètre de la distribution normale $\sigma$ (paramètre de dispersion) est fixé et ne dépend donc pas des variables indépendantes à la différence de $\mu$. Dans ce modèle spécifiquement, les paramètres à estimer sont : $\sigma$, $\beta_0$, et $\beta$.
Notez que dans la notation traditionnelle, la fonction de lien est appliquée au paramètre modélisé. Il est possible de renverser cette notation en utilisant la réciproque ($g'$) de la fonction de lien ($g$) :

\footnotesize
\begin{equation}
g(\mu) = \beta_0 + \beta X \text{ <==> } \mu = g'(\beta_0 + \beta X)
\text{ si : }g'(g(x)) = x
(\#eq:glm2)
\end{equation}
\normalsize

Dans un modèle GLM, la distribution attendue de la variable *Y* est déclarée de façon explicite ainsi que la façon dont nos variables indépendantes influencent cette distribution. Ici, c’est la moyenne ($\mu$) de la distribution qui est modélisée, on s’intéresse donc au changement moyen de *Y* provoqué par les variables *X*.

Avec cet exemple, nous voyons les deux composantes supplémentaires d’un modèle GLM :

*	La distribution supposée de la variable *Y* (ici, la distribution normale)
*	Une fonction de lien associant l’équation de régression formée par les variables indépendantes et un paramètre de la distribution retenue (ici la fonction identitaire et le paramètre $\mu$).

Notez également que l’estimation des paramètres d’un modèle GLM (ici $\beta_0$, $\beta X$ et $\sigma$) ne se fait plus avec la méthode des moindres carrés ordinaires utilisée pour les modèles LM. À la place, la méthode par maximum de vraisemblance (*maximum likelihood*) est le plus souvent utilisée, mais certains *packages* utilisent également la méthode des moments (*method of moments*). Dans les deux cas, ces méthodes nécessitent des échantillons plus grands que la méthode des moindres carrés.

### Autres distributions et rôle de la fonction de lien {#sect0612}

À première vue, on pourrait se demander pourquoi rajouter ces deux éléments puisqu’ils ne font que complexifier le modèle. Prenons donc un exemple appliqué au cas d’une variable binaire pour souligner la capacité de généralisation des modèles GLM. Admettons que nous souhaitons modéliser / prédire la probabilité qu’un cycliste décède lors d’une collision avec un véhicule motorisé. Notre variable dépendante est donc binaire (0 = survie, 1 = décès), et nous souhaitons la prédire avec trois variables continues que sont : la vitesse de déplacement du cycliste ($x_1$), la vitesse de déplacement du véhicule ($x_2$) et la masse du véhicule ($x_3$). Puisque *Y* n’est pas continue, il ne fait aucun sens d’assumer qu’elle est issue d’une distribution normale. Cependant, il est logique de supposer qu’elle provient d’une distribution de Bernoulli (pour rappel, une distribution de Bernoulli permet de modéliser un phénomène ayant deux issues possibles comme un lancer de pièce de monnaie, section \@ref(sect0241)). Plus spécifiquement, nous pourrions formuler l’hypothèse que nos trois variables $x_1$, $x_2$ et $x_3$ influencent le paramètre *p* (la probabilité d’occurrence de l’évènement) d’une distribution de Bernoulli. À partir de ces premières hypothèses, nous pouvons écrire le modèle suivant : 

\footnotesize
\begin{equation}
\begin{aligned}
&Y \sim Bernoulli(p)\\
&g(p) = \beta_0 + \beta X\\
&g(x) = x
\end{aligned}
(\#eq:glm3)
\end{equation}
\normalsize

Toutefois, le résultat n’est pas entièrement satisfaisant. En effet, *p* est une probabilité et, par nature, ce paramètre devrait être compris entre 0 et 1 (entre 0 et 100% de « chances de décès », ni plus ni moins). L’équation de régression que nous utilisons actuellement peut produire des résultats compris en $+\infty$ et $-\infty$ pour *p* puisque rien ne contraint la somme $\beta_0+ \beta_1x_1+\beta_2x_2+ \beta_3x_3$ à être comprise entre 0 et 1. Il est possible de visualiser le problème soulevé par cette situation avec les figures suivantes. Admettons que nous ayons observé une variable *Y* binaire et que nous savons qu’elle est influencée par une variable *X* qui, plus elle augmente, plus la chance que *Y* soit 1 augmente (figure \@ref(fig:linearbinom)).

```{r linearbinom, echo=FALSE, fig.align='center', fig.cap="Exemple de données issues d'une distribution de Bernoulli", message=FALSE, warning=FALSE, out.width='70%'}

x1 <- rnorm(150)
z <- 1 + rnorm(150,mean = 2, sd = 0.5)*x1
pr <- 1/(1+exp(-z))
y <- ifelse(pr > 0.5,1,0)

df <- data.frame(x1 = x1, 
                 y = y)

ggplot(data = df)+
  geom_point(aes(x = x1, y = y), size = 0.5) + 
  labs(x = "X", y = "Y") + 
  ylim(c(-0.25,1.25))
```

Si l’on utilise l’équation de régression actuelle, cela revient à trouver la droite la mieux ajustée passant dans ce nuage de points (figure \@ref(fig:linearbinom2)).

```{r linearbinom2, echo=FALSE, fig.align='center', fig.cap="Ajustement d'une droite de régression aux données issues d'une distribution de Bernoulli", message=FALSE, warning=FALSE, out.width='70%'}

ggplot(data = df)+
  geom_point(aes(x = x1, y = y), size = 0.5) + 
  geom_smooth(aes(x = x1, y = y),method='lm', formula= y~x,se = F) +
  labs(x = "X", y = "Y") + 
  ylim(c(-0.25,1.25))

```

Ce modèle semble bien cerner l’influence positive de *X* sur *Y*, mais la droite est au final très éloignée de chaque point, indiquant un faible ajustement du modèle. De plus, la droite prédit des probabilités négatives lorsque *X* est inférieur à −2,5 et des probabilités supérieures à 1 quand *X* est supérieur à 1. Elle est donc loin de bien représenter les données.

C’est ici qu’intervient la fonction de lien. La fonction identitaire n’est pas satisfaisante, nous devons la remplacer par une fonction qui conditionnera la somme $\beta_0+ \beta_1x_1+\beta_2x_2+ \beta_3x_3$ pour donner un résultat entre 0 et 1. Une candidate toute désignée est la fonction *sigmoidale*, plus souvent appelée la fonction *logistique* !

\footnotesize
\begin{equation}
\begin{aligned}
&Y \sim Bernoulli(p)\\
&S(p) = \beta_0 + \beta X\\
&S(x) = \frac{e^{x}}{e^x+1}
\end{aligned}
(\#eq:glm4)
\end{equation}
\normalsize

La fonction logistique prend la forme d’un *S*. Plus la valeur entrée dans la fonction est grande, plus le résultat produit par la fonction est proche de 1 et inversement. Si l’on reprend l’exemple précédent, on obtient le modèle à la figure \@ref(fig:linearbinom3).

```{r linearbinom3, echo=FALSE, fig.align='center', fig.cap="Utilisation de la fonction de lien logistique", message=FALSE, warning=FALSE, out.width='70%'}

ggplot(data = df)+
  geom_point(aes(x = x1, y = y), size = 0.5) + 
  geom_smooth(aes(x = x1, y = y),method='lm', formula= y~x,se = F) +
  geom_smooth(aes(x = x1, y = y),method='glm', 
              method.args = list(family = "binomial"),
              se = F, color = "red") + 
  labs(x = "X", y = "Y") + 
  ylim(c(-0.25,1.25))

```

Une fois cette fonction insérée dans le modèle, on constate qu’une augmentation de la somme $\beta_0+ \beta_1x_1+\beta_2x_2+ \beta_3x_3$ conduit à une augmentation de la probabilité *p* et inversement, et que cet impact est non linéaire. Nous avons donc maintenant un GLM permettant de prédire la probabilité d’un décès lors d’un accident en combinant une distribution et une fonction de lien adéquates.

### Conditions d'application {#sect063}

La famille des GLM englobe de (très) nombreux modèles du fait de la diversité de distributions existantes et des fonctions de liens utilisables. Cependant, certaines combinaisons sont plus souvent utilisées que d’autres. Nous présentons donc dans les prochaines sections les modèles GLM les plus communs. Les conditions d’application varient d’un modèle à l’autre en fonction du choix de la distribution, il existe cependant quelques conditions d’application communes à tous ces modèles : 

*	L’indépendance des observations (et donc des erreurs). 
*	L’absence de valeurs aberrantes / fortement influentes. 
*	L’absence de multicolinéarité excessive entre les variables indépendantes.

Ces trois conditions sont également valables pour les modèles LM tel qu'abordés dans le chapitre. La distance de *Cook* peut ainsi être utilisée pour détecter les potentielles valeurs aberrantes et le facteur d’inflation de la variance (*VIF*) pour détecter la multicolinéarité.
Les conditions d’application particulières seront détaillées dans les sections dédiées à chaque modèle.

### Résidus et déviance {#sect0614}

Dans la section sur la régression linéaire simple, nous avions présenté la notion de résidu, soit l’écart entre la valeur prédite par le modèle et la valeur observée (réelle) de *Y*. Pour un modèle GLM, ces résidus traditionnels ne sont pas très informatifs si la variable à modéliser est binaire, multinomiale ou même de comptage. Lorsque l’on travaille avec des GLM, on préférera utiliser trois autres formes des résidus, soit les résidus de Pearson, les résidus de déviance et les résidus simulés.

**Les résidus de Pearson** sont une forme ajustée des résidus classiques. On soustrait à la valeur observée la valeur attendue divisée par la racine carrée de la variance modélisée. Leur formule varie donc d’un modèle à l’autre puisque l’expression de la variance change en fonction de la distribution du modèle. Pour un modèle GLM gaussien, il s'écrit :

\footnotesize
\begin{equation}
r_i = \frac{y_i - \mu_i}{\sigma}
(\#eq:glm5)
\end{equation}
\normalsize

Pour un modèle GLM de Bernoulli, il s'écrit : 

\footnotesize
\begin{equation}
r_i = \frac{y_i - p_i}{\sqrt{p_i(1-p_i)}}
(\#eq:glm6)
\end{equation}
\normalsize

**Les résidus de déviance** sont basés sur le concept de *likelihood* présenté dans la section \@ref(sect02adjdistrib). Pour rappel, le *likelihood*, ou la vraisemblance d’un modèle, correspond à la probabilité conjointe d’avoir observé les données *Y* selon le modèle étudié. Pour des raisons mathématiques (voir section \@ref(sect02adjdistrib)), on préfère généralement calculer le *log likelihood.* Plus cette valeur est forte, moins le modèle se trompe. Cette interprétation est donc inverse à celle des résidus classiques, c’est pourquoi on multiplie le *log likelihood* par −2 pour retrouver une interprétation intuitive. Ainsi, pour chaque observation *i*, on peut calculer :

\footnotesize
\begin{equation}
d_i = -2 * log(P(y_i|M_e))
(\#eq:glm7)
\end{equation}
\normalsize

Avec $d_i$ le résidu de déviance, et $P(y_i|M_e)$ la probabilité d’avoir observé la valeur $y_i$ selon le modèle étudié ($M_e$).

La somme de tous ces résidus est appelée la déviance totale du modèle.

\footnotesize
\begin{equation}
D(M_e) = \sum_{i=1}^n -2 * log(P(y_i|M_e))
(\#eq:glm8)
\end{equation}
\normalsize

Il s’agit donc d’une quantité représentant à quel point le modèle est erroné vis-à-vis des données. Notez qu’en tant que telle, la déviance n’a pas d’interprétation directe, en revanche, elle est utilisée pour calculer des mesures d’ajustement des modèles GLM.

**Les résidus simulés** sont une avancée récente dans le monde des GLM, ils fournissent une définition et une interprétation harmonisée des résidus pour l’ensemble des modèles GLM. Dans la section sur les LM (**ref**), nous avions vu comment interpréter les graphiques des résidus pour détecter d’éventuels problèmes dans le modèle. Cependant, cette technique est bien plus compliquée à mettre en œuvre pour les GLM puisque la forme attendue des résidus varie en fonction de la distribution choisie pour modéliser *Y*. La façon la plus efficace de procéder est d’interpréter les graphiques des résidus simulés qui ont la particularité d’être **identiquement distribués, quel que soit le modèle GLM construit**. Ces résidus simulés sont compris entre 0 et 1, et sont calculés de la manière suivante :

* À partir du modèle GLM construit, simuler *S* fois (généralement 1000) une variable *Y’* avec autant d’observation (*n*) que *Y*. Cette variable simulée est une combinaison de la prédiction du modèle (coefficient et variables indépendantes) et de sa dispersion (variance). Ces simulations représentent des variations vraisemblables de la variable *Y* si le modèle est correctement spécifié. En d’autres termes, si le modèle représente bien le phénomène à l’origine de la variable *Y*, alors les simulations *Y’* issues du modèle devraient être proches de la variable *Y* originale. Pour une explication plus détaillée de ce que signifie simuler des données à partir d’un modèle, référez-vous au *bloc attention* intitulé *distinction entre simulation et prédiction*.

* Pour chaque observation, on obtient ainsi *S* valeurs formant une distribution, $Ds_i$, des valeurs simulées par le modèle pour cette observation.

* Pour chacune de ces distributions, on calcule la probabilité cumulative d’observer la vraie valeur $Y_i$ d'après la distribution $Ds_i$. Cette valeur est comprise entre 0 (toutes les valeurs simulées sont plus grandes que $Y_i$) et 1 (toutes les valeurs simulées sont inférieures à $Y_i$).

Si le modèle est correctement spécifié, le résultat attendu est que la distribution de ces résidus est uniforme. En effet, il y a autant de chances que les simulations produisent des résultats supérieurs ou inférieurs à $Y_i$ si le modèle représente bien le phénomène [@RandomizedResid, @gelman2006data]). Si la distribution des résidus ne suit pas une loi uniforme, cela signifie que le modèle échoue à reproduire le phénomène à l’origine de *Y*, ce qui doit nous alerter sur sa pertinence.

### Vérifier l’ajustement {#sect0615}

Il existe trois façons de vérifier l’ajustement d’un modèle GLM : 

* utiliser des mesures d’ajustement (AIC, pseudo R<sub>2</sub>, déviance expliquée, etc.)
* comparer les distributions de la variable originale et celle des prédictions
* comparer les prédictions du modèle avec les valeurs originales.

Notez d’emblée que vérifier la qualité d’ajustement d’un modèle (ajustement aux données originales) ne revient pas à vérifier la validité d’un modèle (respect des conditions d’application). Cependant, ces dernières sont généralement liées, car un modèle mal ajusté a peu de chances d’être valide et inversement.

#### Les mesures d'ajustement

Les mesures d’ajustement sont des indicateurs plus ou moins arbitraires dont le principal intérêt est de faciliter la comparaison entre plusieurs modèles similaires. Il est nécessaire de les reporter, car dans certains cas, ils peuvent indiquer que des modèles sont très mal ajustés.

##### La déviance expliquée 

Rappelons que la déviance d’un modèle est une quantité représentant à quel point le modèle est erroné. L’idée de l’indicateur de la déviance expliquée est d’estimer le pourcentage de la déviance maximale observable dans les données que le modèle est parvenu à expliquer. La déviance maximale observable dans les données est obtenue en utilisant la déviance totale du modèle nul (noté $M_n$, soit un modèle dans lequel aucune variable indépendante n’est ajoutée et ne comportant qu’une constante). Cette déviance est maximale puisqu’aucun prédicteur n’est présent dans le modèle. On calcule ensuite le pourcentage de cette déviance totale qui a été contrôlée par le modèle étudié ($M_e$).

\footnotesize
\begin{equation}
\text{déviance expliquée} = \frac{D(M_n) - D(M_e)}{D(M_n)} = 1- \frac{D(M_e)}{D(M_n)}
(\#eq:glm9)
\end{equation}
\normalsize

Il s’agit donc d’un simple calcul de pourcentage entre la déviance maximale ($D(M_n)$) et la déviance expliquée par le modèle étudié ($D(M_n )-D(M_e)$). Cet indicateur est compris entre 0 et 1, plus il est petit, plus la capacité de prédiction du modèle est faible. Attention, cet indicateur ne tient pas compte de la complexité du modèle. Ajouter une variable indépendante supplémentaire ne fait qu’augmenter la déviance expliquée, ce qui ne signifie pas que la complexification du modèle soit justifiée (**voir l'encadré sur le principe de parcimonie**, section \@ref(sect0532)).

##### Les pseudo $R^2$

Le $R^2$ est une mesure d’ajustement représentant la part de la variance expliquée dans un modèle linéaire classique. Cette mesure n’est pas directement transposable au cas des GLM puisqu’ils peuvent être appliqués à des variables non continues et anormalement distribuées. Toutefois, il existe des mesures semblables appelées pseudo $R^2$, remplissant un rôle similaire. Notez cependant qu’ils ne peuvent pas être interprétés comme le $R^2$ classique (d'une régression linéaire multiple) : **ils ne représentent pas la part de la variance expliquée**. Ils sont compris dans l’intervalle 0 et 1; plus leurs valeurs s’approchent de 1, plus l’ajustement est élevé.

```{r tablepseudor2, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

df <- data.frame(
  Nom = c("McFadden", "McFadden ajusté", "Efron", "Cox & Snell", "Nagelkerke"),
  formule = c("$1-\\frac{loglike(M_e)}{loglike(M_n)}$", 
              "$1-\\frac{loglike(M_e)-K}{loglike(M_n)}$",
              "$1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^n(y_i-\\bar{y}_i)^2}$",
              "$1-e^{-\\frac{2}{n}({loglike(M_e) - loglike(M_n))}}$",
              "$\\frac{1-e^{-\\frac{2}{n}({loglike(M_e) - loglike(M_n))}}}{1-e^{\\frac{2*loglike(M_n)}{n}}}$"),
  Commentaire = c("Le rapport des loglikelihood, très proche de la déviance expliquée", "Version ajustée du R^2^ de McFadden tenant compte du nombre de paramètres (K) dans le modèle", "Rapport entre la somme des résidus classiques au carré (numérateur) et de la somme des écarts au carré à la moyenne (dénominateur). Notez que pour un GLM gaussien, ce pseudo R^2^ est identique au R^2^ classique", "Transformation de la déviance afin de la mettre sur une échelle de 0 à 1 (mais ne pouvant atteindre exactement 1)", "Ajustement du R^2^ de Cox et Snell pour que l’échelle de valeurs possibles puisse comporter 1 (attention car les valeurs de ce R^2^ tendent à être toujours plus fortes que les autres)")
)

show_table(df, 
      col.names = c("Nom","Formule", "Commentaire"),
      caption = 'Principaux pseudo $R^2$', 
      col.to.resize = 3,
      col.width = "8cm")

```

En dehors du pseudo R^2^ de McFadden ajusté, aucune de ces mesures ne tient compte de la complexité du modèle. Il est cependant intéressant de les reporter, car des valeurs très faibles indiquent vraisemblablement un modèle avec une moindre capacité informative. À l’inverse, des valeurs trop fortes pourraient également indiquer un problème de surajustement (**voir de nouveau l'encadré sur le principe de parcimonie**, section \@ref(sect0532)).

##### Le critère d'information d'Akaike (*AIC*)

Probablement l’indicateur le plus répandu, sa formule est relativement simple car il s’agit seulement d’un ajustement de la déviance :

\footnotesize
\begin{equation}
AIC = D(M_e) + 2K
(\#eq:glm10)
\end{equation}
\normalsize

Avec *K* le nombre de paramètres à estimer dans le modèle (coefficients, paramètres de distribution, etc.). 

Le *AIC* n’a pas d’interprétation directe, mais permet de comparer deux modèles imbriqués (voir section \@ref(sect0532)). Plus le *AIC* est petit, mieux le modèle est ajusté. L’idée derrière cet indicateur est relativement simple. Si la déviance *D* est grande, alors le modèle est mal ajusté. Ajouter des paramètres (des coefficients pour de nouvelles variables *X* par exemple) ne peut que réduire *D*, mais cette réduction n’est pas forcément suffisamment grande pour justifier la complexification du modèle. Le *AIC* pondère donc *D* en lui ajoutant 2 fois le nombre de paramètres du modèle. Un modèle plus simple (avec moins de paramètres) parvenant à une même déviance est préférable à un modèle complexe (principe de parcimonie ou du rasoir d’Ockham), ce que permet de « quantifier » le *AIC*. Attention, le *AIC* **ne peut pas être utilisé pour comparer des modèles non imbriqués**. Notez que d’autres indicateurs similaires comme le *WAIC*, le *BIC* et le *DIC* sont utilisés dans un contexte d’inférence bayésienne. Retenez simplement que ces indicateurs sont conceptuellement proches du *AIC* et s’interprètent (à peu de choses près) de la même façon.

#### Comparer les distributions originale et prédites

Une façon rapide de vérifier si un modèle est mal ajusté est de comparer la forme de la distribution originale et celle capturée par le modèle. L’idée est la suivante, si le modèle est bien ajusté aux données, il est possible de se servir de celui-ci pour générer de nouvelles données dont la distribution ressemble à celle des données originales. Si une différence importante est observable, alors les résultats du modèle ne sont pas fiables car le modèle échoue à reproduire le phénomène étudié. Cette lecture graphique ne permet pas de s’assurer que le modèle est valide ou bien ajusté, mais simplement d’écarter rapidement les mauvais candidats. Notez que cette méthode ne s’applique pas lorsque la variable modélisée est binaire, multinomiale ou ordinale.
Le graphique à réaliser comprend donc la distribution de la variable dépendante Y (représentée avec un histogramme ou un graphique de densité) et plusieurs distributions simulées à partir du modèle. Cette approche est plus répandue dans la statistique bayésienne, mais elle reste pertinente dans l’approche fréquentiste. Il est rare de reporter ces figures, mais elles doivent faire partie de votre diagnostic.

::: {.bloc_attention data-latex=""}
**Distinction entre simulation et prédiction**

Notez ici que **simuler des données** à partir d’un modèle et **effectuer des prédictions** à partir d’un modèle sont deux opérations différentes. Prédire une valeur à partir d’un modèle revient simplement à appliquer son équation de régression à des données. Si l’on réutilise les mêmes données, la prédiction renvoie toujours le même résultat, il s’agit de la partie systématique du modèle. 
Pour illustrer cela, admettons que nous ayons ajusté un modèle GLM de type gaussien (fonction de lien identitaire) avec trois variables continues $X_1$, $X_2$ et $X_3$ et des coefficients respectifs de 0,5, 1,2 et 1,8 ainsi qu’une constante de 7. Nous pouvons utiliser ces valeurs pour prédire la valeur attendue de $Y$ quand $X_1$ = 3, $X_2$ = 5 et $X_3$ = 5 : 

$Prediction = 7 + 3*0,5 + 5*1,2 + 1,8*5 = 23,5$

En revanche, simuler des données à partir d’un modèle revient à ajouter la dimension stochastique (aléatoire) du modèle. Puisque notre modèle GLM est gaussien, il comporte un paramètre $\sigma$ (son écart type); admettons pour cet exemple qu’il soit de 1,2. Ainsi, avec les données précédentes, il est possible de simuler un ensemble infini de valeurs dont la distribution est la suivante : $Normal(\mu = 23,5 \text{ , } \sigma = 1,2)$. 95% du temps, ces valeurs simulées se trouveront dans l’intervalle 21,1-25,9 ($\mu - 2\sigma \text{ ; } \mu + 2\sigma$), puisque cette distribution est normale. Les valeurs simulées dépendent donc de la distribution choisie pour le modèle et de l’ensemble des paramètres du modèle, pas seulement de l’équation de régression.

Si vous aviez à ne retenir qu’une seule phrase de ce bloc, retenez que la prédiction ne se réfère qu’à la partie systématique du modèle (équation de régression), alors que la simulation incorpore la partie stochastique (aléatoire) de la distribution du modèle. Deux prédictions effectuées sur des données identiques donnent des résultats identiques, ce qui n’est que rarement le cas pour la simulation.
:::

#### Comparer les prédictions du modèle avec les valeurs originales

Dans le meilleur des mondes, les prédictions d’un modèle devraient être proches des valeurs réelles observées. Si ce n’est pas le cas, alors le modèle n’est pas fiable et ses paramètres ne sont pas informatifs. Dépendamment de la nature de la variable modélisée (quantitative ou qualitative), plusieurs approches peuvent être utilisées pour quantifier l’écart entre valeurs réelles et valeurs prédites.

##### Pour une variable quantitative

La mesure la plus couramment utilisée pour une variable continue est l’erreur moyenne quadatrique (*Root Mean Square Error* – *RMSE* en anglais). 

\footnotesize
\begin{equation}
RMSE = \sqrt{\frac{\sum_{i=1}^n(y_i - \hat{y_i})^2}{n}}
(\#eq:glm11)
\end{equation}
\normalsize

Il s’agit donc de la racine carrée de la moyenne des écarts au carré entre valeurs réelles et prédites. Le *RMSE* est exprimé dans la même unité que la donnée originale et nous donne donc une indication sur l’erreur moyenne de la prédiction du modèle. Admettons par exemple que nous modélisions les niveaux de bruit environnemental en ville en décibels et que notre modèle de régression ait un RMSE de 3,5. Cela signifierait qu’en moyenne notre modèle se trompe de 3,5 décibels (erreur pouvant être négative ou positive) ce qui serait énorme (3 décibels correspond à une multiplication par deux de l’intensité sonore) et nous amènerait à reconsidérer la fiabilité du modèle. Notez que l’usage d’une moyenne quadratique plutôt qu’une moyenne arithmétique permet de donner plus d’impact aux larges erreurs et donc de pénaliser davantage des modèles faisant parfois des grosses erreurs de prédiction. Le *RMSE* est donc très sensible à la présence de valeurs aberrantes.
À la place de la moyenne quadratique, il est possible d’utiliser la simple moyenne arithmétique des valeurs absolues des erreurs (*MAE*). Cette mesure est cependant moins souvent utilisée :

\footnotesize
\begin{equation}
MAE = \frac{\sum_{i=1}^n|y_i - \hat{y_i|}}{n}
(\#eq:glm12)
\end{equation}
\normalsize

Ces deux mesures peuvent être utilisées pour comparer la capacité de prédiction de deux modèles appliqués aux mêmes données, même s’ils ne sont pas imbriqués. Elles ne permettent cependant pas de prendre en compte de la complexité du modèle. Un modèle plus complexe aura toujours un *RMSE* et un *MAE* plus petits.

##### Pour une variable qualitative

Lorsque l’on modélise une variable qualitative, une erreur revient à prédire la mauvaise catégorie pour une observation. Il est ainsi possible de compter, pour un modèle, le nombre de bonnes et de mauvaises prédictions et d’organiser cette information dans une **matrice de confusion**. Cette dernière prend la forme suivante pour un modèle binaire : 

```{r confusmat1, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

df <- data.frame(
  C1 = c("A","B", "Total (%)"),
  C2 = c("15","5", "20 (46,6)"),
  C3 = c("3","20", "23 (53,5)"),
  C4 = c("18 (41,9)","25 (51,1)", "43 (81,4)")
)

show_table(df, 
    caption = 'Exemple de matrice de confusion',
    col.names = c("Valeur prédite / Valeur réelle","A", "B", "Total (%)")
)

```
En colonne du tableau \@ref(tab:confusmat1), nous avons les catégories observées et en ligne, les catégories prédites. La diagonale représente les prédictions correctes. Dans le cas présent, le modèle a bien catégorisé 35 (15 + 20) observations sur 43, soit une précision totale de 81,4%; huit sont mal classifiées (18,6%); cinq avec la modalité A ont été catégorisées comme des B, soit 20% des A, et seuls trois B ont été catégorisées comme des A (13%).

Le matrice ci-dessus \@ref(tab:confusmat1) ne comporte que deux catégories possibles, la variable *Y* modélisée était donc une variable binaire. Il est possible d'étendre facilement le concept de matrice de confusion au cas des variables avec plus de deux modalités (multinomiale). Le tableau \@ref(tab:confusmat2) est un exemple de matrice de confusion multinomiale.

```{r confusmat2, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}
df <- data.frame(
  C1 = c("A","B","C", "D", "Total (%)"),
  C2 = c("15","5", "2", "1" , "23 (18,1)"),
  C3 = c("3", "20", "10", "0", "33 (25,7)"),
  C4 = c("1", "2", "25", "5", "33 (25,7)"),
  C5 = c("5", "12","8","14","39 (30,5)"),
  C6 = c("24 (18,7)", "39 (30,4)", "45 (35,2)", "20 (15,6)", "128")
)


show_table(df, 
    caption = 'Exemple de matrice de confusion multinomiale',
    col.names = c("Valeur prédite / Valeur réelle","A", "B","C", "D", "Total (%)")
)
```

Trois mesures pour chaque catégorie peuvent être utilisées pour déterminer la capacité de prédiction du modèle : 

* La précision (*precision* en anglais), soit le nombre de fois où une classe a été correctement prédite, divisée par le nombre de fois ou la classe a été prédite.
* Le rappel (*recall* en anglais), soit le nombre de fois où une classe a été correctement prédite, divisée par le nombre de fois où elle se trouve dans les données originales.
* Le score *F1* est la moyenne harmonique entre la précision et le rappel :

\footnotesize
\begin{equation}
F1 = 2 * \frac{\text{précision * rappel}}{\text{précision + rappel}}
(\#eq:glm13)
\end{equation}
\normalsize

Il est possible de calculer les moyennes pondérées des différents indicateurs (macro-indicateurs) afin de disposer d’une valeur d’ensemble pour le modèle. La pondération est faite en fonction du nombre de cas réel de chaque catégorie; l’idée étant qu’il est moins grave d’avoir des indicateurs plus faibles pour des catégories moins fréquentes. Cependant, il est tout à fait possible que cette pondération ne soit pas souhaitable. C’est par exemple le cas dans de nombreuses études en santé portant sur des maladies rares où l’attention est concentrée sur ces catégories peu fréquentes.
Le coefficient de Kappa (indicateur de 0 à 1) peut aussi être utilisé pour quantifier la fidélité générale de la prédiction du modèle. Pour l'interprétation du coefficient de Kappa, référez-vous au tableau \@ref(tab:Kappvals). Enfin, un test statistique basé sur la distribution binomiale peut être utilisé pour vérifier que le modèle atteint un niveau de précision supérieur au seuil de non-information. Ce seuil correspond à la proportion de la modalité la plus présente dans le jeu de données. Dans la matrice de confusion utilisée ci-dessus, ce seuil est de 30,5% (catégorie D), ce qui signifie qu’un modèle prédisant tout le temps la catégorie D aurait une précision de 30,5% pour cette catégorie. Il est donc nécessaire que notre modèle fasse mieux que ce seuil.

Dans le cas de la matrice de confusion du tableau \@ref(tab:confusmat2), nous obtenons donc les valeurs affichées dans le tableau \@ref(tab:confusIndic).

```{r confusIndic, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}
indicators <- data.frame(
  C1 = c("A", "B", "C", "D", "macro", "Kappa", "Valeur de p  (précision > NIR)"),
  C2 = c("65,2", "60,6", "75,8", "35,9", "57,8", "0.44", "<0.0001"),
  C3 = c("31,3", "25,6", "27,8", "35,0", "30,0", "", ""),
  C4 = c("42,3", "36,0", "40,7" ,"35,4" ,"38,2","","")
)

show_table(indicators, 
    caption = 'Indicateurs de qualité de prédiction',
    col.names = c("", "précision", "rappel", "F1")
)
```

À la lecture du tableau \@ref(tab:confusIndic), nous remarquons que : 

* la catégorie D est la moins bien prédite des quatre catégories (faible précision et faible rappel).
* La catégorie C a une forte précision, mais un faible rappel, ce qui signifie que de nombreuses observations étant originalement des A, B ou D ont été prédites comme des C. Ce constat est également vrai pour la catégorie B.
* Le coefficient de Kappa indique un accord modéré entre les valeurs originales et la prédiction.
* La probabilité que la précision du modèle ne dépasse pas le seuil de non information est inférieure à 0.001, indiquant que le modèle à une précision supérieure à ce seuil.

```{r Kappvals, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}
indicators <- data.frame(
 K = c("< 0", "0 - 0,20", "0,21 - 0,40", "0,41 - 0,60", "0,61 - 0,80", "0,81 - 1"),
 Inter = c("Désaccord", "Accord très faible", "Accord faible", "Accord modéré", "Accord fort", "Accord presque parfait")
)

show_table(indicators, 
    caption = 'Inteprétation des valeurs du coefficient de Kappa',
    col.names = c("K","Interprétation")
)
```

### Comparer deux modèles GLM  {#sect0616}

Tel qu'abordé dans le chapitre sur les régressions linéaires classiques, il est courant de comparer plusieurs modèles imbriqués (section \@ref(sect0414)). Cette procédure permet de déterminer si l'ajout d'une ou de plusieurs variables contribue à significativement améliorer le modèle. Il est possible d'appliquer la même démarche aux GLM à l'aide du test de rapport de vraisemblance (*likelihood ratio test*). L'idée derrière ce test est de comparer le *likelihood* de deux modèles GLM imbriqués, la valeur de ce test se calcule avec l'équation suivante :

\footnotesize
\begin{equation}
LR = 2(loglik(M_2) - loglik(M_1))
(\#eq:glm14)
\end{equation}
\normalsize

Avec $M_2$ un modèle reprenant toutes les variables du modèle $M_1$, impliquant donc que  $loglik(M_2) >= loglik(M_1))$. 

L'idée derrière ce test est que le modèle $M_2$ comporte plus de paramètres que le modèle $M_1$ et devrait donc être mieux ajusté aux données. Si c'est bien le cas, la différence entre les *loglikelihood* de deux modèles devrait être supérieure à 0. La valeur calculée *LR* suit une distribution du Chi^2^ avec un nombre de degrés de liberté égal au nombre de paramètres supplémentaires dans le modèle $M_2$ comparativement à $M_1$. Avec ces deux informations, il est possible de déterminer la valeur de *p* associée à ce test et de déterminer si $M_2$ est significativement mieux ajusté que $M_1$ aux données. Notez qu'il existe aussi deux autres tests (test de Wald et test de Lagrange) ayant la même fonction. Il s'agit, dans les deux cas, d'approximation du test de rapport des vraisemblances dont la puissance statistique est inférieure au test de rapport de vraisemblance [@NeymanLemma].

## Les principaux modèles GLM {#sect062}

Dans cette section, nous décrivons les principaux modèles GLM utilisés. Il en existe de nombreuses variantes que nous ne pouvons pas toutes mentionner ici. L'objectif est donc de comprendre les rouages de ces modèles afin de pouvoir en cas de besoin reporter ces connaissances sur des modèles plus spécifiques. Pour faciliter la lecture de cette section, nous vous proposons une carte d’identité de chacun des modèles présentés. Elles contiennent l’ensemble des informations pertinentes à retenir pour chaque modèle.

### Les modèles GLM pour des variables qualitatives {#sect0621}

Nous abordons en premier les principaux GLM utilisés pour modéliser des variables binaires, multinomiales et ordinales. Prenez bien le temps de saisir le fonctionnement du modèle logistique binomial, car il sert de base pour les trois autres modèles présentés.

#### Le modèle logistique binomial {#sect06211}

Le modèle logistique binomial est une généralisation du modèle de Bernoulli que nous avons présenté dans l’introduction de cette section. Le modèle logistique binomiale couvre donc deux cas de figure : 

1.	La variable observée est binaire (0 ou 1). Dans ce cas, le modèle logistique binomiale devient un simple modèle de Bernoulli.
2.	La variable observée est un comptage (nombre de réussites) et on dispose d’une autre variable avec le nombre de réplications de l’expérience. Par exemple, pour chaque intersection d’un réseau routier, nous pourrions avoir le nombre de décès à vélo (variable *Y* de comptage) et le nombre de collisions vélo / automobile (variable quantifiant le nombre d’expérience, chaque collision étant une expérience).
Spécifiquement, on tente de prédire le paramètre *p* de la distribution binomiale à l’aide de notre équation de régression et la fonction logistique comme fonction de lien.

```{r binomidentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim Binomial(p)$ \\newline $g(p) = \\beta_0 + \\beta X$ \\newline $g(x) = log(\\frac{x}{1-x})$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable binaire (0 ou 1) ou comptage de réussite à une expérience (ex : 3 réussites sur 5 expériences)", "Binomiale", model_formula , "logistique", "p", "$\\beta_0$, $\\beta$", "Non-séparation complète, Absence de surdispersion ou sousdispersion")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle logistique binomial",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

##### Interprétation des paramètres

Les seuls paramètres à estimer du modèle sont les coefficients $\beta_0$ et $\beta$. La fonction de lien logistique transforme la valeur de ces coefficients, en conséquence, ils ne peuvent plus être interprétés simplement. $\beta_0$ et $\beta$ sont des logarithmes de rapports de cote (*log odd ratio*). Le rapport de cote est relativement facile à interpréter. Pour l’obtenir, il suffit d’utiliser la fonction exponentielle (l’inverse de la fonction logarithmique) pour passer des log rapport de cote à de simples rapports de cote.
Donc si $exp(\beta)$ est inférieur à 1, il réduit la probabilité d’observer l’évènement et inversement si $exp(\beta)$ est supérieur à 1. 

Par exemple, admettons que nous ayons eu un coefficient $\beta_1$ de 1,2 pour une variable $X_1$ dans une régression logistique. Il est nécessaire d'utiliser son exponentiel pour l'interpréter de façon intuitive. $exp(1,2) = 3,32$, ce qui signifie que lorsque l'on augmente $X_1$ d'une unité, on multiplie les chances par 3,32 d'observer 1 plutôt que 0 comme valeur de *Y*. Admettons maintenant que $\beta_1$ vaille -1,2, on calcule donc $exp(-1,2) = 0,30$, ce qui signifie qu'à chaque augmentation d'une unité de $X_1$, on multiplie les chances par 0,30 d'observer 1 plutôt que 0 comme valeur de *Y*. En d'autres termes, on divise par 3,33 ($1/0,30 = 3,33$) les chances d'observer 1 plutôt que 0, soit une diminution de 70% ($1-0,3 = 0,7$) des chances d'observer 1 plutôt que 0.

::: {.bloc_aller_loin data-latex=""}
**Les rapports de cotes**

Le rapport de cote ou rapport des chances est une mesure utilisée pour exprimer l’effet d’un facteur sur une probabilité très utilisée dans le domaine de la santé, mais aussi des paris. Prenons un exemple concret avec le port du casque à vélo. Si sur 100 accidents impliquant des cyclistes portant un casque on observe seulement 3 cas de blessures graves à la tête, contre 15 dans un second groupe de 100 cyclistes ne portant pas de casques, on peut calculer le rapport de cote suivant : 

\footnotesize
\begin{equation}
\frac{p(1-q)}{q(1-p)} = \frac{0,15 * (1-0,03)}{0,03 * (1-0,15)} = 5,71
\end{equation}
\normalsize

avec *p* la probabilité d'observer le phénomène (ici la blessure grave à la tête) dans le groupe 1 (ici les cyclistes sans casque) et *q* la probabilité d'observer le phénomène dans le groupe 2 (ici les cyclistes avec un casque). Ce rapport de cote indique que les cyclistes sans casques ont 5,71 fois plus de chances de se blesser gravement à la tête lors d’un accident que ceux portant un casque.
:::

##### Les conditions d'application {#sectcondapp}

La non-séparation complète signifie qu’aucune des variables *X* n'est, à elle seule, capable de parfaitement distinguer les deux catégories 0 et 1 de la variable *Y*. Dans un tel cas de figure, les algorithmes d’ajustement utilisés pour estimer les paramètres des modèles sont incapables de converger. Notez aussi l’absurdité de créer un modèle pour prédire une variable *Y* si une variable X est capable à elle seule de la prédire à coup sûr. Ce problème est appelé un effet de Hauck-Donner, il est assez facile de le repérer, car la plupart du temps les fonctions de R signalent ce problème (message d'erreur sur la convergence). Sinon, des valeurs extrêmement élevées ou faibles pour certains rapports de cote peuvent aussi indiquer un effet de Hauck-Donner.

La surdispersion est un problème spécifique aux distributions n’ayant pas de paramètre de dispersion (binomiale, poisson, exponentielle, etc.), pour lesquelles la variance dépend directement de la moyenne. On parle de surdispersion lorsque dans un modèle les résidus (ou erreurs) sont plus dispersés que ce que suppose la distribution utilisée. À l’inverse, il est aussi possible (mais rare) d’observer des cas de sous-dispersion (lorsque la dispersion des résidus est plus petite que ce que suppose la distribution choisie). Ce cas de figure se produit généralement lorsque le modèle parvient à réaliser une prédiction trop précise pour être fiable. Si vous rencontrez une forte sous-dispersion, cela signifie souvent que l’un de vos prédicteurs provoque une séparation complète. La meilleure option dans ce cas est de supprimer le prédicteur en question du modèle. La variance attendue d’une distribution binomiale est $nb * p *(1-p)$, soit le produit entre le nombre de tirages, la probabilité de réussite et la probabilité d'échec. À titre d'exemple, si l'on considère une distribution binomiale avec un seul tirage et 50% de chances de réussite, sa variance serait : $1 \times 0,5 \times (1-0,5) = 0,25$.

On peut observer de la surdispersion dans un modèle binomial logistique pour plusieurs raisons : 

* Il manque des variables importantes dans le modèle, conduisant à un mauvais ajustement et donc une surdispersion des erreurs.
* Les observations ne sont pas indépendantes, impliquant qu’une partie de la variance n’est pas contrôlée et augmente les erreurs.
* La probabilité de succès de chaque expérience varie d’une répétition à l’autre (différentes distributions).

La conséquence directe de la surdispersion est la sous-estimation de la variance des coefficients de régression. En d’autres termes, la surdispersion conduit à sous-estimer notre incertitude quant aux coefficients obtenus et réduit les valeurs de *p* calculées pour ces coefficients. Les risques de trouver des résultats significatifs à cause des fluctuations d'échantillonnage augmentent.

Pour détecter de la surdispersion ou de la sousdispersion dans un modèle logistique binomial, il est possible d'observer les résidus de déviance du modèle. Ces derniers sont supposés suivre une distribution du Chi^2^ avec *n−k* degrés de liberté (avec *n* le nombre d'observations et *k* le nombre de coefficients dans le modèle). Par conséquent, la somme des résidus de déviance d'un modèle logistique binomiale divisée par le nombre de degrés de liberté devrait être proche de 1. Une légère déviation (jusqu'à 0,15 au-dessus ou au-dessous de 1) n'est pas alarmante, au-delà, il est nécessaire d'ajuster le modèle.

Notez que si la variable *Y* modélisée est exactement binaire (chaque expérience est indépendante et n'est composée que d'un seul tirage) et que le modèle utilise donc une distribution de Bernoulli, le test précédent pour détecter une éventuelle surdispersion n'est pas valide. @hilbe2009logistic parle de surdispersion implicite pour le modèle de Bernoulli et recommande notamment de toujours ajuster les erreurs standards des modèles utilisant des distributions de Bernoulli, binomiale et de Poisson. Pour cela, il est possible d'utiliser ce que l'on appelle des quasi-distributions, ou des estimateurs robustes [@zeileis2004econometric].

##### Exemple appliqué dans R {#sectexemplebinom}

**Présentation des données**

Pour illustrer le modèle logistique binomial, nous utilisons ici un jeu de données proposé par l’Union européenne : [l’enquête de déplacement sur la demande pour des systèmes de transports innovants](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/P82V9X). Pour cette enquête, un échantillon de 1000 individus représentatifs de la population a été constitué dans chacun des 26 États membres de l’UE, soit un total de 26000 observations. Pour chaque individu ont été collectées plusieurs informations relatives à la catégorie socioprofessionnelle, le mode de transport le plus fréquent, le temps du trajet de son déplacement le plus fréquent et son niveau de sensibilité à la cause environnementale. Nous modélisons ici la probabilité qu’un individu déclare utiliser le plus fréquemment le vélo comme moyen de transport. Les variables explicatives sont résumées au tableau \@ref(tab:binomdata). Il existe bien évidemment un grand nombre de facteurs au niveau individuel qui impactent la prise de décision sur le mode de transport. Les résultats de ce modèle ne doivent donc pas être pris avec un grand sérieux; il est uniquement construit à des fins pédagogiques, et ce, sans cadre conceptuel solide.

```{r binomdata, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}
tableau <- data.frame(
 C1 = c("Pays", "Sexe", "Age", "Education", "StatutEmploi", "Revenu", "Residence", "Duree", "ConsEnv"),
 C2 = c("Pays de résidence", "Sexe biologique", "Âge biologique", "Niveau d’éducation maximum atteint", "Employé ou non", "Niveau de revenu autodéclaré", "Lieu de résidence", "Durée du voyage le plus fréquent autodéclarée (en minutes)", "Préoccupation environnementale"),
 C3 = c("Variable multinomiale", "Variable binaire", "Variable continue", "Variable multinomiale", "Variable binaire", "Variable multinomiale", "Variable multinomiale", "Variable continue", "Variable ordinale"),
 C4 = c("Le nom d’un des 26 pays membres de l’UE", "Homme ou Femme", "L’âge en nombre d’années variant de 16 à 84 ans dans le jeu de données", "Premier cycle, secondaire inférieur (classes supérieures de l’école élémentaire), secondaire,  troisième cycle", "Employé ou non", "Très faible revenu, faible revenu, revenu moyen, revenu élevé, revenu très élevé, sans reponse", "Zone rurale, petite ou moyenne ville (moins de 250 000 habitants), grande ville (entre 250 000 et 1 million d’habitants) , aire métropolitaine (plus d’un million d’habitants)", "Nombre de minutes", "Échelle de likerte et 1 à 10")
)

show_table(tableau, 
    caption = "Variables indépendantes utilisées pour prédire le mode de transport le plus utilisé",
    col.names = c("Nom de la variable","signification","Type de variable","Mesure"), 
    col.to.resize = c(2,4),
    col.width = "5cm"
)
```

**Vérification des conditions d'application**

La première étape de la vérification des conditions d'application est de calculer les valeurs du facteur d'inflation de variance (VIF) pour s'assurer de l'absence de multicolinéarité trop forte entre les variables indépendantes. L'ensemble des valeurs de VIF sont inférieures à 5, indiquant l'absence de multicolinéarité excessive dans le modèle. 

```{r message=FALSE, warning=FALSE, out.width='50%'}
library(car)

# Chargement des données
dfenquete <- read.csv("data/glm/enquete_transport_UE.csv")
dfenquete$Pays <- relevel(as.factor(dfenquete$Pays), ref = "Germany")

# Vérification du VIF

model1 <- glm(y ~
              Pays + Sexe + Age + Education + StatutEmploi + Revenu +
              Residence + Duree + ConsEnv,
            family = binomial(link="logit"),
            data = dfenquete
)
vif(model1)
```

La seconde étape de vérification est le calcul des distances de Cook et l'identification d'éventuelles valeurs aberrantes.

```{r message=FALSE, warning=FALSE, out.width='60%', fig.cap="Distances de Cook pour le modèle binomial avec toutes les observations", fig.align="center"}
# calcul et représentation des distances de Cook
cookd <- data.frame(
  dist = cooks.distance(model1),
  oid = 1:nrow(dfenquete)
)

ggplot(cookd) + 
  geom_point(aes(x = oid, y = dist ), color = rgb(0.1,0.1,0.1,0.4), size = 1)+
  geom_hline(yintercept = 0.002, color = "red")+
  labs(x = "observations", y = "distance de Cook") + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

```

Le calcul de la distance de Cook révèle un ensemble d'observations se démarquant nettement des autres (délimitées dans le graphique par la ligne rouge). Nous les isolons dans un premier temps pour les analyser.

```{r message=FALSE, warning=FALSE, out.width='80%'}
# Isoler les observations avec de très fortes valeurs de Cook
# valeur seuil choisie : 0,002

cas_etranges <- subset(dfenquete, cookd$dist>=0.002)
cat(nrow(cas_etranges),'observations se démarquant dans le modèle')

print(cas_etranges)
```

À la lecture des valeurs pour ces 19 cas étranges, nous remarquons que la plupart des observations proviennent de Malte et de Chypre. Ces deux petites îles constituent des cas particuliers en Europe et devraient vraisemblablement faire l'objet d'une analyse séparée. Nous décidons donc de les retirer du jeu de données. Deux autres observations étranges sont observables en Slovaquie et au Luxembourg. Dans les deux cas, les répondants ont renseigné des temps de trajet fantaisiste de respectivement 775 et 720 minutes. Nous les retirons donc également de l'analyse.

```{r message=FALSE, warning=FALSE, out.width='60%', fig.cap="Distances de Cook pour le modèle binomial sans les valeurs aberrantes", fig.align="center"}
# Retirer les observations aberrantes
dfenquete2 <- subset(dfenquete, (dfenquete$Pays %in% c("Malta", "Cyprus")) == F & 
                  dfenquete$Duree < 400)

# Réajuster le modèle
model2 <- glm(y ~
              Pays + Sexe + Age + Education + StatutEmploi + Revenu +
              Residence + Duree + ConsEnv,
            family = binomial(link="logit"),
            data = dfenquete2)

# Recalculer la distance de Cook
cookd <- data.frame(
  dist = cooks.distance(model2),
  oid = 1:nrow(dfenquete2)
)

ggplot(cookd) + 
  geom_point(aes(x = oid, y = dist ), color = rgb(0.1,0.1,0.1,0.4), size = 1)+
  labs(x = "observations", y = "distance de Cook") + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

Après avoir retiré ces valeurs aberrantes, nous n'observons plus de nouveaux cas singuliers avec les distances de Cook.

La prochaine étape de vérification des conditions d'application est l'analyse des résidus simulés. Nous commençons donc par calculer ces résidus et afficher leur histogramme.

```{r figresidsimbinomHistoUnif, message=FALSE, warning=FALSE, out.width='60%', fig.cap="Distribution des résidus simulé pour le modèle binomial", fig.align="center"}

library(DHARMa)

#Extraire les probabilités prédites par le modèle
probs <- predict(model2, type = "response")

#Calculer 1000 simulations a partir du modele ajuste
sims <- lapply(1:length(probs), function(i){
  p <- probs[[i]]
  vals <- rbinom(n = 1000, size = 1,prob = p)
})
matsim <- do.call(rbind,sims)

#utiliser le package DHARMa pour calculer les résidus simulés
sim_res <- createDHARMa(simulatedResponse = matsim, 
                            observedResponse = dfenquete2$y,
                            fittedPredictedResponse = probs,
                            integerResponse = T)

ggplot()+
  geom_histogram(aes(x = residuals(sim_res)),
                 bins = 30, fill = "white", color = rgb(0.3,0.3,0.3))+
  labs(x="résidus simulés", y="fréquence")

```

L'histogramme indique clairement que les résidus simulés suivent une distribution uniforme (figure \@ref(fig:figresidsimbinomHistoUnif)). Il est possible d'aller plus loin dans le diagnostic en utilisant la fonction `plot` sur l'objet `sim_res`. La partie de droite de la figure ainsi obtenue (figure \@ref(fig:figresidsimbinom)) est un diagramme de quantiles-quantiles (ou Q-Q plot). Les points du graphique sont supposés suivre une ligne droite matérialisée par la ligne rouge. Une déviation de cette ligne indique un éloignement des résidus de leur distribution attendue. Trois tests sont également réalisés par la fonction.

* Le premier (Test de Kolmogorov-Smirnov, *KS test*) permet justement de tester si les points dévient significativement de la ligne droite. Dans notre cas, la valeur de *p* n'est pas significative, indiquant que les résidus ne dévient pas de la distribution uniforme.
* Le second test permet de vérifier la présence de sur ou sous dispersion. Dans notre cas, ce test n'est pas significatif, indiquant aucun problème de surdispersion ou sous-dispersion.
* Le dernier test permet de vérifier si des valeurs aberrantes sont présentes dans les résidus. Une valeur non significative indique une absence de valeurs aberrantes.

Le second graphique permet de comparer les résidus et les valeurs prédites. L'idéal est donc d'observer une ligne droite horizontale rouge au milieu du graphique qui indiquerait une absence de relation entre les valeurs prédites et les résidus (ce que nous observons bien ici).

```{r figresidsimbinom, message=FALSE, warning=FALSE, out.width='100%', fig.cap="Diagnostic des résidus simulés par le package DHARMa", fig.align="center"}
plot(sim_res)
```

L'analyse approfondie des résidus nous permet donc de conclure que le modèle respecte les conditions d'application et que nous pouvons passer à la vérification de la qualité d'ajustement du modèle.

**Vérification de la qualité d'ajustement**

Pour calculer les différents R<sup>2</sup> d'un modèle GLM, nous proposons la fonction suivante : 
```{r message=FALSE, warning=FALSE, out.width='100%'}
rsqs <- function(loglike.full, loglike.null,full.deviance, null.deviance, nb.params, n){
  #calcul de la déviance expliquée
  explained_dev <- 1-(full.deviance / null.deviance)

  K <- nb.params
  #R2 de McFadden ajusté
  r2_faddenadj <- 1- (loglike.full - K) / loglike.null

  Lm <- loglike.full
  Ln <- loglike.null
  #R2 de Cox and Snell
  Rcs <- 1 - exp((-2/n) * (Lm-Ln))
  #R2 de Nagelkerke
  Rn <- Rcs / (1-exp(2*Ln/n))
  return(
    list("deviance expliquee" = explained_dev,
         "McFadden ajuste" = r2_faddenadj,
         "Cox and Snell" = Rcs,
         "Nagelkerke" = Rn
    )
  )
}
```

Nous pourrons l'utiliser pour l'ensemble des modèles GLM de la section. Dans le cas du modèle binomial, nous obtenons : 

```{r message=FALSE, warning=FALSE, out.width='100%'}

#Ajuster un modele null avec seulement une constante
model2.null <- glm(y ~1,
            family = binomial(link="logit"),
            data = dfenquete2)

#Calculer les R2
rsqs(loglike.full = as.numeric(logLik(model2)), # loglikelihood du modèle complet
     loglike.null = as.numeric(logLik(model2.null)), # loglikelihood du modèle nul
     full.deviance = deviance(model2), # déviance du modèle complet
     null.deviance = deviance(model2.null), # déviance du modèle nul
     nb.params = model2$rank, # nombre de paramètres dans le modèle
     n = nrow(dfenquete2) # nombre d'observations
     )
```

La déviance expliquée par le modèle est de 8,8%, les pseudos R^2^ de McFadden (ajusté), Efron et Nagelkerke sont respectivement 0,084, 0,069 et 0,123. L'ensemble de ces valeurs sont relativement faibles et indiquent qu'une large partie de la variabilité de *Y* reste inexpliquée.

Pour vérifier la qualité de prédiction du modèle, nous devons comparer les catégories prédites et les catégories réelles de notre variable dépendante et construire une matrice de confusion. Cependant, un modèle GLM binomiale prédit **la probabilité d’appartenance au groupe 1** (ici les personnes utilisant le vélo pour effectuer leur déplacement le plus fréquent). Pour convertir ces probabilités prédites en catégories prédites, il faut choisir une probabilité seuil au-delà de laquelle on considère que la valeur attendue est 1 (cycliste) plutôt que 0 (autre). Un exemple naïf serait de prendre le seuil 0,5, ce qui signifierait que si le modèle prédit qu’une observation a au moins 50% de chance d’être un cycliste, alors nous l’attribuons à cette catégorie. Cependant, cette méthode est rarement optimale; il est donc plus judicieux de le fixer en trouvant le point d’équilibre entre la sensibilité (proportion de 1 correctement identifiés) et la spécificité (proportion de 0 correctement identifiés). Ce point d’équilibre est identifiable graphiquement en calculant la spécificité et la sensibilité de la prédiction selon toutes les valeurs possibles du seuil.

```{r equlibresensispeci, message=FALSE, warning=FALSE, out.width='100%', fig.cap="Point d'équilibre entre sensibilité et spécificité", fig.align="center"}

library(ROCR)

#Obtention des prédictions du modèle
prob <- predict(model2, type = "response")

#calcul de la sensibilité et de la spécificité (package ROCR)
predictions <- prediction(prob, dfenquete2$y)

sens <- data.frame(x=unlist(performance(predictions, "sens")@x.values),
                   y=unlist(performance(predictions, "sens")@y.values))
spec <- data.frame(x=unlist(performance(predictions, "spec")@x.values),
                   y=unlist(performance(predictions, "spec")@y.values))

# trouver numériquement la valeur seuil (minimiser la différence absolue
# entre sensibilité et spécificité)
real <- dfenquete2$y

find_cutoff <- function(seuil){
  pred <- ifelse(prob>seuil,1,0)
  sensi <- sum(real==1 & pred==1) / sum(real==1)
  spec <- sum(real==0 & pred==0) / sum(real==0)
  return(abs(sensi-spec))
}

prob_seuil <- optimize(find_cutoff,interval = c(0,1), maximum = F)$minimum

cat("Le seuil de probabilité à retenir équilibrant",
  "la sensibilité et la spécificité est de",prob_seuil)

# affichage du graphique
ggplot() +
  geom_line(data = sens, mapping = aes(x = x, y = y)) +
  geom_line(data = spec, mapping = aes(x = x,y = y,col="red")) +
  scale_y_continuous(sec.axis = sec_axis(~., name = "Spécificité")) +
  labs(x='Seuil de probabilité', y="Sensibilité") +
  geom_vline(xintercept = prob_seuil, color = "black", linetype = "dashed") + 
  annotate(geom = "text", x = prob_seuil, y = 0.01, label = round(prob_seuil,3))+
  theme(axis.title.y.right = element_text(colour = "red"), legend.position="none")

```

On constate avec la figure \@ref(fig:equlibresensispeci) que si la valeur du seuil est 0 %, alors la prédiction a une sensibilité parfaite (le modèle prédit toujours 1, donc tous les 1 sont détectés); à l’inverse, si le seuil choisi est 100% alors la prédiction à une spécificité parfaite (le modèle prédit toujours 0, donc tous les 0 sont détectés). Dans notre cas, la valeur d’équilibre est d'environ 0,148, donc si le modèle prédit une probabilité au moins égale à 14,8% qu’un individu utilise le vélo pour son déplacement le plus fréquent, nous l’attribuerons à cette catégorie. Avec ce seuil, nous pouvons convertir les probabilités prédites en classes prédites et construire notre matrice de confusion.

```{r message=FALSE, warning=FALSE, out.width='60%'}

library(caret) # pour la matrice de confusion

#calcul des catégories prédites
ypred <- ifelse(predict(model2,type="response")>0.148,1,0)
info <- confusionMatrix(as.factor(dfenquete2$y), as.factor(ypred))

# affichage des valeurs brutes de la matrice de confusion
print(info)
```

Les résultats proposés par le package **caret** sont exhaustifs; nous vous proposons ici une façon de les présenter dans deux tableaux : l'un présente la matrice de confusion (tableau \@ref(tab:confusmatbinom)) et l'autre, les indicateurs de qualité de prédiction (tableau \@ref(tab:confusmatbinom2)).

```{r confusmatbinom, echo=FALSE, message=FALSE, warning=FALSE, out.width='60%'}
mat <- info[[2]]
rs <- rowSums(mat)
rp <- round(rowSums(mat) / sum(mat) * 100,1)
cs <- colSums(mat)
cp <- round(colSums(mat) / sum(mat) * 100,1)
mat2 <- cbind(mat,rs,rp)
mat3 <- rbind(mat2,c(cs,sum(mat),NA),c(cp,NA,NA))

mat4 <- cbind(c("0 (prédit)", "1 (prédit)", "Total", "%"), mat3)
show_table(mat4,
      col.names = c("","0 (réel)", "1 (réel)", "Total", "%"),
      caption = "Matrice de confusion pour le modèle binomial")
```

D’après ces indicateurs, nous constatons que le modèle a une capacité de prédiction relativement faible, mais tout de même significativement supérieurr au seuil de non information. La valeur de rappel pour la catégorie 1 (cycliste) est faible, indiquant que le modèle a manqué un nombre important de cyclistes lors de sa prédiction. 

```{r confusmatbinom2, echo=FALSE, message=FALSE, warning=FALSE, out.width='60%'}
precision <- diag(mat) / rowSums(mat)
rappel <- diag(mat) / colSums(mat)
F1 <- 2*((precision*rappel)/(precision + rappel))

macro_scores <- c(weighted.mean(precision,colSums(mat)),
                  weighted.mean(rappel,colSums(mat)),
                  weighted.mean(F1,colSums(mat)))

final_table <- rbind(cbind(precision,rappel,F1),c(NA,NA,NA,NA),macro_scores)
final_table <- rbind(final_table, c(info[[3]][[2]],NA,NA), c(info[[3]][[6]],NA,NA))
rnames <- c(rownames(mat),"","macro","Kappa","Valeur de p  (précision > NIR)")
final_table <- cbind(rnames,round(final_table,2))

show_table(final_table,
      col.names = c("","Précision", "Rappel", "F1"),
      caption = "Matrice de confusion pour le modèle binomial")
```

**Interprétation des résultats du modèle**

L'interprétation des résultats d'un modèle binomial passe par la lecture des rapports de cotes et de leurs intervalles de confiance. Nous commençons donc par calculer la version robuste des erreurs standards des coefficients : 

```{r  message=FALSE, warning=FALSE}
library(sandwich) #pour calculer les erreurs standards robustes

covModel2 <- vcovHC(model2, type = "HC0") # méthode HC0, basée sur les résidus
stdErrRobuste <- sqrt(diag(covModel2)) # extraire la diagonale

#extraction des coefficients
coeffs <- model2$coefficients
#recalcul des scores Z
zvalRobuste <- coeffs / stdErrRobuste
#recalcul des valeurs de P
pvalRobuste <- 2 * pnorm(abs(zvalRobuste), lower.tail = FALSE)
#calcul des rapports de cote
oddRatio <- exp(coeffs)
#calcul des intervales de confiance à 95% des rapports de cote
lowerBound <- exp(coeffs - 1.96 * stdErrRobuste)
upperBound <- exp(coeffs + 1.96 * stdErrRobuste)
#étoiles pour les valeurs de p
starsp <- case_when(pvalRobuste <=  0.001  ~ "***",
                    pvalRobuste >  0.001 & pvalRobuste <= 0.01 ~ "**",
                    pvalRobuste >  0.01 & pvalRobuste <= 0.05 ~ "*",
                    pvalRobuste >  0.05 & pvalRobuste <= 0.1 ~ ".",
                    TRUE ~ ""
                    )

#compilation des résultats dans un tableau
tableau_binom <- data.frame(
  coefficients = coeffs,
  rap.cote = oddRatio,
  err.std = stdErrRobuste,
  score.z = zvalRobuste,
  p.val = pvalRobuste,
  rap.cote.2.5 = lowerBound,
  rap.cote.97.5 = upperBound,
  sign = starsp
)
```

Considérant que la variable *Pays* a 24 modalités, il est plus judicieux de présenter ces rapports de cote sous forme d’un graphique. Nous avons choisi l’Allemagne comme catégorie de référence puisqu’elle fait partie des pays avec une importante part modale pour le vélo sans constituer un cas extrême comme le Danemark.

```{r figpaysbinom, message=FALSE, warning=FALSE, fig.align="center", fig.cap= "Rapports de cote pour les différents pays de l'UE"}

#isoler les ligne du tableau récapitualtif pour les pays
paysdf <- subset(tableau_binom, grepl("Pays",row.names(tableau_binom), fixed = T))
paysdf$Pays <- gsub("Pays","",row.names(paysdf),fixed=T)

ggplot(data = paysdf) +
  geom_vline(xintercept = 1, color = "red")+ #afficher la valeur de référence
  geom_errorbarh(aes(xmin = rap.cote.2.5, xmax = rap.cote.97.5, 
                     y = reorder(Pays, rap.cote)), height = 0)+
  geom_point(aes(x = rap.cote, y = reorder(Pays, rap.cote))) +
  geom_text(aes(x = rap.cote.97.5, y = reorder(Pays, rap.cote), 
                label = paste("RC : ",round(rap.cote,2),sep="")),
                size = 3, nudge_x = 0.25)+
  labs(x = "Rapports de cote", y = "Pays (référence : Allemagne)")
```

Dans la figure \@ref(fig:figpaysbinom), la barre horizontale pour chaque pays représente l’intervalle de confiance de son rapport de cote (le point); plus cette ligne est longue, plus grande est l’incertitude autour de ce paramètre. Lorsque les lignes de deux pays se chevauchent, cela signifie qu’il n’y a pas de différence significative au seuil 0,05 entre les rapports de cotes des deux pays. La ligne rouge tracée à x = 1, représente le rapport de cote du pays de référence (ici l’Allemagne). On constate ainsi que comparativement à un individu vivant en Allemagne, ceux vivant au Danemark et au Pays-Bas ont 2,4 fois plus de chance d’utiliser le vélo pour leur déplacement le plus fréquent. Les Pays de l’Ouest (France, Luxembourg, Royaume-Uni, Irlande) et du Sud (Grèce, Italie, Espagne, Portugal) ont en revanche des rapports de cotes plus faibles. En France, les chances qu’un individu utilise le vélo pour son trajet le plus fréquent sont 3,22 (1/0,31) fois plus faibles que si l’individu vivait en Allemagne.

Pour le reste des coefficients et rapport de cote, nous les rapportons dans le tableau \@ref(tab:tablcoeffbinom)).

```{r tablcoeffbinom, echo=FALSE, fig.align="center",  warning=FALSE,  message=FALSE}

tableau_comp <- build_table(model2, confid = T, sign = T, coef_digits = 3, std_digits = 3, z_digits = 3, p_digits = 3, OR_digits = 3, robust_se = "HC0")

ok_tableau <- tableau_comp[c(1,29:54),c(1,2,3,4,5,6,9,10)]

show_table(ok_tableau,
      col.names = c("Variable","Coefficient", "Rapport de cote", "Err.std","Val.z", "P","RC 2,5%", "RC 97,5%"),
      caption = "Résultats du modèle binomial")
```

Les chances pour un individu d’utiliser le vélo pour son trajet le plus fréquent sont augmentées de 45% s’il s’agit d’un homme plutôt qu’une femme. Pour l’âge, nous constatons un effet relativement faible puisque chaque année supplémentaire réduit les chances qu’un individu utilise le vélo comme mode de transport pour son trajet le plus fréquent de 0,9% ( (0.991-1)*100). Le fait d’être sans emploi augmente les chances d’utiliser le vélo de 29% comparativement au fait d’avoir un emploi. Concernant le niveau d’éducation, seul le coefficient pour le groupe des personnes de la catégorie secondaire inférieure est significatif, indiquant que les personnes de ce groupe ont 35% de chances en plus d'utiliser le vélo comme mode de transport pour leur déplacement le plus fréquent comparativement aux personnes du groupe premier cycle. Pour le revenu, seul le groupe avec de très faibles revenus se distingue significativement du groupe avec un revenu élevé avec un rapport de cote de 1,27, soit 27% de chances en plus d'utiliser le vélo.

Comparativement à ceux vivant dans une aire métropolitaine, les personnes vivant dans des petites, moyennes et grandes villes ont des chances accrues d’utiliser le vélo comme mode de déplacement pour leur trajet le plus fréquent. En revanche, on ne peut observer aucune différence entre la probabilité d’utiliser le vélo dans une métropole et en zone rurale. La figure \@ref(fig:figvillebinom) permet de clairement visualiser cette situation. Rappelons que la référence est la situation : vivre dans une région métropolitaine, et est représentée par la ligne verticale rouge. Plusieurs pistes d’interprétation peuvent être envisagées pour ce résultat : 

* En métropole et dans les zones rurales, les distances domicile-travail tendent à être plus grandes que dans les petites, moyennes et grandes villes.
* En métropole, le système de transport en commun est davantage développé et entre donc en concurrence avec les modes de transport actifs.

```{r figvillebinom, message=FALSE, warning=FALSE, fig.align="center", fig.cap= "Rapports de cote pour les différents lieux de résidence", out.width="85%"}

#isoler les lignes du tableau récapitualtif pour les lieux de résidence
residdf <- subset(tableau_binom, grepl("Residence",row.names(tableau_binom), fixed = T))
residdf$resid <- gsub("Residence","",row.names(residdf),fixed=T)

ggplot(data = residdf) +
  geom_vline(xintercept = 1, color = "red")+ #afficher la valeur de référence
  geom_errorbarh(aes(xmin = rap.cote.2.5, xmax = rap.cote.97.5, y = resid), height = 0)+
  geom_point(aes(x = rap.cote, y = resid)) +
  geom_text(aes(x = rap.cote.97.5, y = resid, 
                label = paste("RC : ",round(rap.cote,2),sep="")),
                size = 3, nudge_x = 0.1)+
  labs(x = "Rapports de cote",
       y = "Lieu de résidence (référence : aire métropolitaine)")
```

Il est aussi intéressant de noter que la durée des trajets ne semble pas impacter la probabilité d’utiliser le vélo. Enfin, une conscience environnementale plus affirmée semble être associée avec une probabilité supérieure d’utiliser le vélo pour son déplacement le plus fréquent, avec une augmentation des chances de 11% pour chaque point supplémentaire sur l’échelle de Likert.

Afin de simplifier la présentation de certains résultats, il est possible de calculer exactement les prédictions réalisées par le modèle. Un bon exemple ici est le cas de la variable âge. Quelle différence peut-on attendre entre deux individus identiques ayant seulement une différence d’âge de 15 ans ?

Prenons comme individu un homme de 30 ans, vivant dans une grande ville allemande, ayant un niveau d’éducation de niveau secondaire, employé, dans la tranche de revenu moyen, déclarant effectuer un trajet de 45 minutes et ayant rapporté un niveau de conscience environnementale de 5 (sur 10). Nous pouvons prédire la probabilité qu’il utilise le vélo pour son trajet le plus fréquent en utilisant la formule suivante : 


`logit(p) =  -2,497 + 1 * 0,372 + 30 * -0,009 + 1 * 0,193 + 1 * 0,042 + 1 * 0,273 + 45 * -0,001 + 5 * 0,102`

`p = exp(-2,497 + 1 * 0,372 + 30 * -0,009 + 1 * 0,193 + 1 * 0,042 + 1 * 0,273 + 45 * -0,001 + 5 * 0,102)/(1+exp(-2,497 + 1 * 0,372 + 30 * -0,009 + 1 * 0,193 + 1 * 0,042 + 1 * 0,273 + 45 * -0,001 + 5 * 0,102)) = 0,194`


Il y aurait donc 19,4% de chances pour que cette personne soit un cycliste. Cette probabilité dépasse le seuil que nous avons sélectionné, cet individu serait donc classé comme un cycliste. Si on augmente son âge de 15 ans, nous obtenons :

`p = exp(-2,497 + 1 * 0,372 + 45 * -0,009 + 1 * 0,193 + 1 * 0,042 + 1 * 0,273 + 45 * -0,001 + 5 * 0,102) / (1+exp(-2,497 + 1 * 0,372 + 45 * -0,009 + 1 * 0,193 + 1 * 0,042 + 1 * 0,273 + 45 * -0,001 + 5 * 0,102)) = 0,174`

Soit une réduction de 2 points de pourcentages. Il est également possible de représenter cette évolution sur un graphique pour montrer l’effet sur l’étendue des valeurs possibles. Sur ces graphiques des effets marginaux, il est essentiel de représenter notre incertitude quant à notre prédiction. En temps normal, la fonction `predict` calcule directement l'erreur standard de la prédiction et cette dernière peut être utilisée pour calculer l'intervalle de confiance de la prédiction. Cependant, nous voulons ici utiliser nos erreurs standards robustes. Nous devons donc procéder par simulation pour déterminer l'intervalle de confiance à 95% de nos prédictions. Cette opération nécessite de réaliser plusieurs opérations manuellement dans R.

```{r figagebinom, message=FALSE, warning=FALSE, fig.align="center", fig.cap= "Impact de l'âge sur la probabilité d'utiliser le vélo comme moyen de déplacement pour son trajet le plus fréquent", out.width="85%"}

# créer un jeu de données fictif pour la prédiction
mat <- model.matrix(model2$terms, model2$model)
age2seq <- seq(20,80)
mat2 <- matrix(mat[1,], nrow=length(age2seq), ncol=length(mat[1,]), byrow=TRUE)
colnames(mat2) <- colnames(mat)
mat2[,"Age"] <- age2seq
mat2[,"PaysBelgium"] <- 0
mat2[,"Duree"] <- 45
mat2[,"ConsEnv"] <- 5
mat2[,"StatutEmploisans emploi"] <- 0
mat2[,"Residencegrande ville"] <- 1
mat2[,"Educationsecondaire"] <- 1
mat2[,"Sexehomme"] <- 1
mat2[,"Revenumoyen"] <- 1
mat2[,"Revenufaible"] <- 0

# Calculer la prédiction comme un log de rapport de cote (avec les erreurs standards)
# en multipliant les coefficient par les valeurs des données fictives
coeffs <- model2$coefficients
pred <- coeffs %*% t(mat2)

# Simulation de prédictions (toujours en log de rapport de cote)

# Étape 1 : simuler 1000 valeurs pour chaque coefficient
sim_coeffs <- lapply(1:length(coeffs), function(i){
  coef <- coeffs[[i]]
  std.err <- stdErrRobuste[[i]]
  vals <- rnorm(n = 1000, mean = coef, sd = std.err)
  return(vals)
})
mat_sim_coeffs <- do.call(rbind,sim_coeffs)

# Étape 2 : effectuer les prédictions à partir des coefficients simulés
sim_preds <- lapply(1:ncol(mat_sim_coeffs),function(i){
  temp_coefs <- mat_sim_coeffs[,i]
  temp_pred <- as.vector(temp_coefs %*% t(mat2))
  return(temp_pred)
})

mat_sim_preds <- do.call(cbind,sim_preds)

# Étape 3 : extraire les intervalles de confiances pour les simulations
intervals <- apply(mat_sim_preds,MARGIN = 1, FUN = function(vec){
  return(quantile(vec,probs = c(0.05, 0.95)))
})

# Étape 4 : récupérer tous ces éléments dans un dataframe
df <- data.frame(
  Age = seq(20,80),
  pred = as.vector(pred),
  lower = as.vector(intervals[1,]),
  upper = as.vector(intervals[2,])
)

# Étape 5 : appliquer l'inverse de la fonction de lien pour
# obtenir les prédictions en termes de probabilité
ilink <- family(model2)$linkinv

df$prob_pred <- ilink(df$pred)
df$prob_lower <- ilink(df$lower)
df$prob_upper <- ilink(df$upper)

# Étape 6 : représenter le tout sur un graphique
ggplot(df) + 
  geom_ribbon(aes(x = Age, ymax = prob_upper, ymin = prob_lower), 
              fill = rgb(0.1,0.1,0.1,0.4)) + 
  geom_path(aes(x = Age, y = prob_pred), color = "blue", size = 1) +
  geom_hline(yintercept = 0.15, linetype = "dashed", size = 0.7) + 
  labs(x = "Âge", y = "Probabilité prédite (intervale de confiance 5% - 95%)")

```

La figure \@ref(fig:figagebinom) permet de bien constater la diminution de la probabilité d’utiliser le vélo pour son trajet le plus fréquent avec l’âge, mais cette réduction est relativement ténue. Dans le cas utilisé en exemple, l’individu ne serait plus classé cycliste qu’après 67 ans.

#### Le modèle probit binomial {#sect06212}

Le modèle GLM probit binomial est pour ainsi dire le frère du modèle logistique binomial. La seule différence entre les deux réside dans l'utilisation de deux fonctions de lien différentes probit et logistique. La fonction de lien probit (Φ) correspond à la fonction cumulative de la distribution normale et a également une forme de *S*. Cette version du modèle est plus souvent utilisée par les économistes. Le principal changement réside dans l’interprétation des coefficients $\beta_0$ et $\beta$. Du fait de la transformation probit, ces derniers indiquent le changement en termes de scores Z de la probabilité modélisée. Vous conviendrez qu’il ne s’agit pas d’une échelle très intuitive, la plupart du temps, seuls la significativité et le signe (positif ou négatif) des coefficients sont interprétés.

```{r probitdentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim Binomial(p)$ \\newline $g(p) = \\beta_0 + \\beta X$ \\newline $g(x) = \\Phi^-1(x)$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable binaire (0 ou 1) ou comptage de réussite à une expérience (ex : 3 réussites sur 5 expériences)", "Binomiale", model_formula , "probit", "p", "$\\beta_0$, $\\beta$", "Non-séparation complète, Absence de surdispersion ou sousdispersion")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle probit binomial",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

#### Le modèle logistique des cotes proportionnelles {#sect06213}

Le modèle logistique des cotes proportionnelles (aussi appelé modèle logistique cumulatif) est utilisé pour modéliser une variable qualitative ordinale. Un exemple classique de ce type de variable est une échelle de satisfaction (très insatisfait, insatisfait, mitigé, satisfait, très satisfait) qui peut être recodée avec des valeurs numériques (0, 1, 2, 3, 4; ces échelons étant notés *j*). Il n’existe pas à proprement parler de distribution pour représenter ces données, mais avec une petite astuce, il est possible de simplement utiliser la distribution binomiale. Cette astuce consiste à poser l’hypothèse de la proportionnalité des cotes, supposant que le passage de la catégorie 0 à la catégorie 1 est proportionnel au passage de la catégorie 1 à la catégorie 2 et ainsi de suite. Si cette hypothèse est respectée, alors les coefficients du modèle pourront autant décrire le passage de la catégorie *satisfait* à celle *très satisfait* que le passage de *insatisfait* à *mitigé*. Si cette hypothèse n’est pas respectée, il faudrait des coefficients différents pour représenter les passages d’une catégorie à l’autre (ce qui est le cas pour le modèle multinomial présenté juste après).

```{r cumuldentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim Binomial(p)$ \\newline $g(p \\leq j) = \\beta_0j + \\beta X$ \\newline $g(x) = log(\\frac{x}{1-x})$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable qualitative ordinale avec *j* catégories", "Binomiale", model_formula , "logistique", "p", "$\\beta$ et *j*-1 constantes $\\beta_0$", "Non-séparation complète, Absence de surdispersion ou sous-dispersion, Proportionnalité des cotes")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle logistique des cotes proportionnelles",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

Ainsi, dans le modèle logistique binomial vu précédemment, on modélise la probabilité d’observer un évènement $P(Y = 1)$. Dans un modèle logistique ordinal, on modélise la probabilité cumulative d’observer l’échelon *j* de notre variable ordinale $P(Y <= j)$. L’intérêt de cette reformulation est que l’on conserve la facilité d’interprétation du modèle logistique binomial classique avec les rapports de cotes, à ceci prêt qu’ils représentent maintenant la probabilité de passer à un échelon supérieur de *Y*. La différence pratique est que notre modèle se retrouve avec autant de constantes qu’il y a de catégories à prédire moins une, chacune de ces constantes contrôlant la probabilité de base de passer de la catégorie *j* à la catégorie *j*+1.

##### Conditions d'application

Les conditions d’application sont les mêmes que pour un modèle binomial, avec bien sûr l’ajout de l’hypothèse sur la proportionnalité des cotes. Selon cette hypothèse, l'impact de chaque variable indépendante est identique sur la probabilité de passer d'un échelon de la variable *Y* au suivant. Afin de tester cette condition, deux approches sont envisageables : 

1. Utiliser l’approche de Brant [-@brant1990assessing]. Il s’agit d’un test statistique comparant les résultats du modèle ordinal avec ceux d’une série de modèles logistiques binomiaux (1 pour chaque catégorie possible de *Y*).
2. Ajuster un modèle ordinal sans l’hypothèse de proportionnalité des cotes et effectuer un test de ratio des *likelihood* pour vérifier si le premier est significativement mieux ajusté.

Si certaines variables ne respectent pas cette condition d’application, trois options sont possibles pour y remédier : 

1. Supprimer la variable du modèle (à éviter si cette variable est importante dans votre cadre théorique)
2. Autoriser la variable à avoir un effet différent entre chaque palier (possible avec le *package* **VGAM**).
3. Changer de modèle et opter pour un modèle des catégories adjacentes. Il s’agit du cas particulier où toutes les variables sont autorisées à changer à chaque niveau. Ne pas confondre ce dernier modèle et le modèle multinomial que nous présentons ensuite, le modèle des catégories adjacentes continue à prédire la probabilité de passer à une catégorie supérieure.


##### Exemple appliqué dans R

Pour cet exemple, nous allons analyser un jeu de données proposé par *Inside Airbnb*, une organisation sans but lucratif collectant des données des annonces sur le site d’Airbnb pour alimenter le débat sur l’impact de cette société sur les quartiers. Plus spécifiquement, nous utilisons le jeu de données pour Montréal [compilé le 30 juin 2020](http://insideairbnb.com/get-the-data.html). Nous modélisons ici le prix par nuit des logements, ce type d’exercice est appelé modélisation hédonique. Il est particulièrement utilisé en économie urbaine pour évaluer les déterminants du marché immobilier et prédire son évolution. Le cas de Airbnb a déjà été étudié dans plusieurs articles [@teubner2017price; @wang2017price; @zhang2017key], il en ressort notamment que le niveau de confiance inspiré par l’hôte, les caractéristiques intrinsèques du logement et sa localisation sont les principaux prédicteurs de son prix. Nous construirons donc notre modèle sur cette base. Notez que nous avons décidé de retirer les logements avec des prix supérieurs à 250\$ qui constituent des cas particuliers et qui devraient faire l'objet d'une analyse à part entière. Nous avons également retiré les observations pour lesquelles certaines données sont manquantes, et obtenons un nombre final de 9051 observations.

La distribution originale du prix des logements dans notre jeu de données est présentée à la figure \@ref(fig:histopriceairbnb).

```{r histopriceairbnb, message=FALSE, warning=FALSE, fig.align="center", fig.cap= "Distribution des prix des logements Airbnb", out.width="65%"}

# Charger le jeu de données
data_airbnb <- read.csv("data/glm/airbnb_data.csv")

# Afficher la distribution du prix
ggplot(data = data_airbnb) + 
  geom_histogram(aes(x = price), bins = 30, 
                 color = "white", fill = "#1d3557", size = 0.02)+
  labs(x="Prix (en dollars)", y="Fréquence")
```

Nous avons ensuite découpé le prix des logements en trois catégories : inférieur à 50\$, entre 50\$ et 99\$ et entre 100\$ et 249\$. Ces catégories forment une variable ordinale de trois échelons que nous modélisons à partir de trois catégories de variables : 

* les caractéristiques propres au logement
* les caractéristiques environnementales autour du logement
* les notes obtenues par le logement sur le site d’Airbnb.


```{r message=FALSE, warning=FALSE, fig.align="center"}
# afficher le nombre de logement par catégories
table(data_airbnb$fac_price_cat)
```

Le tableau \@ref(tab:variablecumul) présente l’ensemble des variables utilisées dans le modèle.

```{r variablecumul, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}
df <- data.frame(
  C1 = c("Lit", "Jardin", "Privee", "Parking", "Accueil", "Vegetation", "Metro", "Commercial", "Note", "Propriétaire"),
  C2 = c("Nombre de lits dans le logement", "Présence d’un jardin ou d’une arrière-cour", "Le logement est entièrement à disposition du locataire ou seulement une pièce", "Une place de stationnement gratuite est disponible sur la rue", "L’hôte accueille personnellement les locataires", "Végétation dans les environs du logement", "Présence d’une station de métro à proximité du logement", "Commerce dans les environs du logement", "Évaluation de la qualité du logement par les usagers", "Nombre total de logements détenus par l’hôte sur Airbnb"),
  C3 = c("Variable de comptage", "Variable binaire", "Variable binaire", "Variable binaire", "Variable binaire", "Variable continue", "Variable binaire", "Variable continue", "Variable ordinale", "Variable de comptage"),
  C4 = c("Nombre de lits dans le logement", "Oui ou Non", "Privé ou partagé", "Oui ou Non", "Oui ou Non", "Pourcentage de surface végétale dans un rayon de 500m autour du logement", "Présence d’une station de métro dans un rayon de 500 mètres autour du logement", "Pourcentage de surface dédiée au commerce (mode d’occupation du sol) dans un rayon d'un kilomètre autour du logement", "Note obtenu par le logement sur une échelle allant de 1 (très mauvais) à 5 (parfait)", "Nombre total de logements détenus par l’hôte sur Airbnb")
)

show_table(df, 
    caption = "Variables indépendantes utilisées pour prédire la catégorie de prix de logements Airbnb",
    col.names = c("Nom de la variable","Description","Type de variable","Mesure"), 
    col.to.resize = c(2,4),
    col.width = "5cm"
)

```

**Vérification des conditions d'application**
Avant d'ajuster le modèle, il convient de vérifier l'absence de multicolinéarité excessive entre les variables indépendantes.

```{r message=FALSE, warning=FALSE}
# Notez que la fonction vif ne s'intéresse qu'aux variables indépendantes.
# Vous pouvez ainsi utiliser la fonction glm avec la fonction vif 
# pour n'importe quel modèle glm
vif(glm(price ~ beds +
    Garden_or_backyard + Host_greets_you + Free_street_parking + 
      prt_veg_500m + has_metro_500m + commercial_1km +host_total_listings_count +
      private + cat_review, data = data_airbnb))
```

Toutes les valeurs de VIF sont inférieures à 2, indiquant une absence de multicolinéarité excessive. Nous pouvons alors ajuster le modèle et analyser des distances de Cook afin de vérifier la présence ou non d'observations très influentes. Pour ajuster le modèle, nous utilisons le package **VGAM** et la fonction `vglm` qui nous donnent accès à la famille `cumulative` pour ajuster des modèles logistiques ordinaux.

```{r message=FALSE, warning=FALSE}
library(VGAM)

modele <- vglm(fac_price_cat ~ beds +
                Garden_or_backyard + Free_street_parking + 
                prt_veg_500m + has_metro_500m + commercial_1km +
                private + cat_review + host_total_listings_count ,
             family = cumulative(link="logitlink", # fonction de lien 
                                 parallel = TRUE, # cote proportionelle
                                 reverse = TRUE),
             data = data_airbnb, model = T)
```

Notez que puisque la variable *Y* a trois catégories différentes et que nous modélisons la probabilité de passer à une catégorie supérieure, chaque observation a alors deux (3-1) valeurs de résidus différentes. Par conséquent, nous calculons deux distances de Cook différentes que nous devons analyser conjointement. Malheureusement, la fonction `cook.distance` ne fonctionne pas avec les objets `vglm`, nous devons donc les calculer manuellement.

```{r message=FALSE, warning=FALSE, fig.align="center", fig.cap = "Distances de Cook pour le modèle logistique des cotes proportionnelles", out.width = "80%"}

# Extraction des résidus
res <- residuals(modele, type = "pearson")
# Extraction de la hat matrix (nécessaire pour calculer la distance de Cook)
hat <- hatvaluesvlm(modele)

# Calcul des distances de Cook
cooks <- lapply(1:ncol(res),function(i){
  r <- res[,i]
  h <- hat[,i]
  cook <- (r/(1 - h))^2 * h/(1 * modele@rank)
})

# Structuration dans un dataframe
matcook <- data.frame(do.call(cbind, cooks))
names(matcook) <- c("dist1","dist2")
matcook$oid <- 1:nrow(matcook)

# Afficher les distances de Cook
plot1 <- ggplot(data = matcook) + 
  geom_point(aes(x = oid, y = dist1), size = 0.2, color = rgb(0.1,0.1,0.1,0.4)) + 
  labs(x = "", y = "", subtitle = "distance de Cook P(Y>=2)")+
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank())

plot2 <- ggplot(data = matcook) + 
  geom_point(aes(x = oid, y = dist2), size = 0.2, color = rgb(0.1,0.1,0.1,0.4)) + 
  labs(x = "", y = "", subtitle = "distance de Cook P(Y>=3)")+
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank())

ggarrange(plot1, plot2, ncol = 2, nrow = 1)

```

Les distances de Cook nous permettent d'identifier quelques observations potentiellement trop influentes, mais elles semblent être différentes d'un graphique à l'autre. Nous décidons donc de ne pas retirer d'observations à ce stade et de passer à l'analyse des résidus simulés. Pour effectuer des simulations à partir de ce modèle, nous nous basons sur les probabilités d'appartenance prédites par le modèle.

```{r message=FALSE, warning=FALSE}
# extraire les probabilités prédites
predicted <- predict(modele,type = "response")
round(head(predicted,n = 4),3)
```

On constate ainsi que pour la première observation, la probabilité prédite d'appartenir au groupe 1 est de 69,4%, 27,7% pour le groupe 2 et 2,9% pour le groupe 3. Si nous effectuons 1000 simulations, on peut s'attendre à ce qu'en moyenne, sur ces 1000 simulations, 694 indiqueront 1 comme catégorie prédite, 277 indiqueront 2 et seulement 29 indiqueront 3.

```{r message=FALSE, warning=FALSE}
# Nous effectuerons 1000 simulations
nsim <- 1000

# Lancement des simulations pour chaque observation (lignes dans predicted)
simulations <- lapply(1:nrow(predicted), function(i){
  probs <- predicted[i,]
  sims <- sample(c(1,2,3), size = nsim, replace = T, prob = probs)
  return(sims)
})

# Combiner les prédictions dans un tableau
matsim <- do.call(rbind, simulations)

# Observons si nos simulations sont proches de ce que nous attendions
table(matsim[1,])
```

À partir de ces simulations de prédiction, nous pouvons réaliser un diagnostic des résidus simulés grâce au *package* **DHARMa**.

```{r diagressimcumul, message=FALSE, warning=FALSE, fig.cap = "Diagnostic général des résidus simulés du modèle des cotes proportionnelles", out.width="65%", fig.align = "center"}

# Extraction de la prédiction moyenne du modèle
pred_cat <- unique(data_airbnb$fac_price_cat)[max.col(predicted)]

# Préparer les données avec le package DHARMa
sim_res <- createDHARMa(simulatedResponse = matsim, 
                            observedResponse = as.numeric(data_airbnb$fac_price_cat),
                            fittedPredictedResponse = as.numeric(pred_cat),
                            integerResponse = T)

# Afficher le graphique de diagnostic général
plot(sim_res)
```

La figure \@ref(fig:diagressimcumul) nous indique que les résidus simulés suivent bien une distribution uniforme et qu'aucune valeur aberrante n'est observable. Pour affiner notre diagnostic, nous vérifions également si aucune relation ne semble exister entre chaque variable indépendante et les résidus. 

```{r diagressimcumul2, message=FALSE, warning=FALSE, fig.cap = "Diagnostic variable par variable des résidus simulés du modèle des cotes proportionnelles", out.width="95%", fig.align = "center"}

# préparons un plot multiple
par(mfrow=c(3,4))
vars <- c("beds","Garden_or_backyard", "Host_greets_you",
          "Free_street_parking", "prt_veg_500m", 
          "has_metro_500m", "commercial_1km","private",
          "cat_review","host_total_listings_count")

for(v in  vars){
  plotResiduals(sim_res, data_airbnb[[v]], main = v)
}
```

La figure \@ref(fig:diagressimcumul2) indique qu'aucune relation marquée n'existe entre nos variables indépendante et nos résidus simulés, sauf pour la variable nombre de lits. En effet, nous pouvons constater que les résidus ont tendance à être toujours plus faible quand le nombre de lit augmente. Cet effet est sûrement lié au fait qu'au-delà de cinq lits, le logement en question est vraisemblablement un dortoir. Il pourrait être judicieux de retirer ces observations de l'analyse, considérant qu'elles sont peu nombreuses et constituent une forme de logement particulière.

```{r message=FALSE, warning=FALSE}

data_airbnb2 <- subset(data_airbnb, data_airbnb$beds <=5)

modele2 <- vglm(fac_price_cat ~ beds  + 
                Garden_or_backyard + Free_street_parking + 
                prt_veg_500m + has_metro_500m + commercial_1km +
                private + cat_review + host_total_listings_count ,
             family = cumulative(link="logitlink", # fonction de lien 
                                 parallel = TRUE, # cote proportionelle
                                 reverse = TRUE),
             data = data_airbnb2, model = T)
```

Nous pouvons ensuite recalculer les résidus simulés pour observer si cette tendance a été corrigée. La figure \@ref(fig:diagressimcumul3) montre qu'une bonne partie du problème a été corrigé, cependant il semble tout de même que les résidus soient plus fort pour les logements avec un seul lit.

```{r diagressimcumul3, message=FALSE, warning=FALSE, echo=FALSE, fig.cap = "Diagnostic variable par variable des résidus simulés du modèle des cotes proportionnelles (arpès correction)", out.width="95%", fig.align = "center"}
# Nous effectuerons 1000 simulations
nsim <- 1000
predicted <- predict(modele2, type = "response")

# Lancement des simulations pour chaque observation (lignes dans predicted)
simulations <- lapply(1:nrow(predicted), function(i){
  probs <- predicted[i,]
  sims <- sample(c(1,2,3), size = nsim, replace = T, prob = probs)
  return(sims)
})

# Combiner les prédictions dans un tableau
matsim <- do.call(rbind, simulations)

# Extraction de la prédiction moyenne du modèle
pred_cat <- unique(data_airbnb2$fac_price_cat)[max.col(predicted)]

# Préparer les donnees avec le package DHARMa
sim_res <- createDHARMa(simulatedResponse = matsim, 
                            observedResponse = as.numeric(data_airbnb2$fac_price_cat),
                            fittedPredictedResponse = as.numeric(pred_cat),
                            integerResponse = T)

par(mfrow=c(3,4))
vars <- c("beds","Garden_or_backyard", "Host_greets_you",
          "Free_street_parking", "prt_veg_500m", 
          "has_metro_500m", "commercial_1km","private",
          "cat_review","host_total_listings_count")

for(v in  vars){
  plotResiduals(sim_res, data_airbnb2[[v]], main = v)
}
```

La prochaine étape du diagnostic est de vérifier si nous n'avons pas de séparation parfaite provoquée par un de nos prédicteurs. Le package **VGAM** propose pour cela la fonction `hdeff`.

```{r message=FALSE, warning=FALSE}
tests <- hdeff(modele2)
problem <- table(tests)
problem
```

La fonction nous informe qu'aucun de nos prédicteurs ne provoque de séparation parfaite : toutes les valeurs renvoyées par la fonction `hdeff` sont égales à `FALSE.`

Il ne nous reste donc plus qu'à vérifier que l'hypothèse de proportionnalité des cotes est respectée, soit que l'impact de chacune des variables indépendantes est bien le même pour passer de la catégorie 1 à 2 que pour passer des catégories 2 à 3. Pour cela, deux approches sont possibles : le test de Brant ou la réalisation d'une séquence de tests de rapport de vraisemblance.

Le package **brant** propose une implémentation du test de Brant, mais elle ne peut être appliquée qu'à des modèles construits avec la fonction `polr` du package **MASS**. Nous avons donc récupéré le code source de la fonction `brant` du package **brant** et apporté quelques modifications pour qu'elle soit utilisable sur un objet `vglm`. Cette nouvelle fonction appelée `brant.vglm` est disponible dans le code source de ce livre.

```{r message=FALSE, warning=FALSE}
tableau_brant <- round(brant.vglm(modele2),3)
```

Ce premier tableau nous indique que seule la variable indiquant si le logement est disponible en entier ou partagé contrevient à l'hypothèse de proportionnalité des cotes (la seule valeur de p significative). Pour confirmer cette observation, nous pouvons réaliser un ensemble de tests de rapport de vraisemblance. Pour chaque variable du modèle, nous allons créer un second modèle dans lequel cette variable est autorisée à varier pour chaque catégorie et comparer les niveaux d'ajustement des modèles. Nous avons implémenté cette procédure dans la fonction `parallel.likelihoodtest.vglm` présente dans le code source de ce livre.

```{r message=FALSE, warning=FALSE}
tableau_likelihood <- parallel.likelihoodtest.vglm(modele2)
print(tableau_likelihood)
```

Les résultats de cette seconde série de tests confirment les précédents, la variable concernant le type de logement doit être autorisée à varier en fonction de la catégorie. Ce second tableau nous indique que la variable concernant la densité de végétation pourrait aussi être amenée à varier en fonction du groupe, mais ce changement a un effet très marginal (différence de AIC de seulement 8 points). Nous ajustons donc un nouveau modèle autorisant la variable `private` à changer en fonction de la catégorie prédite.

```{r message=FALSE, warning=FALSE}
modele3 <- vglm(fac_price_cat ~ beds + 
                  Garden_or_backyard + Host_greets_you + Free_street_parking + 
                  prt_veg_500m + has_metro_500m + commercial_1km +
                  private + cat_review + host_total_listings_count,
                family = cumulative(link="logitlink",
                                    parallel = FALSE ~ private ,
                                    reverse = TRUE),
                data = data_airbnb2, model = T)
```

**Vérifier l'ajustement du modèle**

Maintenant que toutes les conditions d'application ont été passée en revue, nous pouvons passer à la vérification de l'ajustement du modèle

```{r message=FALSE, warning=FALSE}

modelenull <- vglm(fac_price_cat ~ 1,
                family = cumulative(link="logitlink",
                                    parallel = TRUE,
                                    reverse = TRUE),
                data = data_airbnb2, model = T)

rsqs(loglike.full = logLik(modele3),
     loglike.null = logLik(modelenull),
     full.deviance = deviance(modele3),
     null.deviance = deviance(modelenull),
     nb.params = modele3@rank,
     n = nrow(data_airbnb2)
)
```

Le modèle final parvient à expliquer 21% de la déviance originale, il obtient un R^2^ ajusté de McFadden de 0,21, et des R^2^ de Cox et Snell et Nagelkerke de respetivement 0,36 et 0,41. Construisons à présent la matrice de confusion de la prédiction du modèle (nous utilisons ici la fonction `nice_confusion_matrix` également disponible dans le code source de ce livre).

```{r message=FALSE, warning=FALSE}

preds_probs <- fitted(modele3)
pred_cat <- c(1,2,3)[max.col(preds_probs)]

library(caret)
matrices <- nice_confusion_matrix(data_airbnb2$fac_price_cat,pred_cat)

# Afficher la matrice de confusion
print(matrices$confusion_matrix)

# Afficher les indicateurs de qualité de prédiction
print(matrices$indicators)

```
Le modèle a une précision totale de 61% (61% des observations ont été correctement prédites). La catégorie 1 a de loin la meilleure précision (72%) et la 3 a la pire (48%), ce qui indique qu’il manque vraisemblablement des variables indépendantes contribuant à prédire les prix des logements les plus chers. Le coefficient de Kappa indique un niveau d’accord entre modéré et faible, mais le modèle parvient à une prédiction significativement supérieure au seuil de non-information. Si l'ajustement du modèle est imparfait, il est suffisamment fiable pour nous donner des renseignements pertinents sur le phénomène étudié.

**Interprétation des résultats**

L'ensemble des coefficients du modèle sont accessibles via la fonction `summary`. À partir des coefficients et de leurs erreurs standards, il est possible de calculer les rapports de cotes ainsi que leurs intervalles de confiances.

```{r message=FALSE, warning=FALSE}
tableau <- summary(modele3)@coef3
rappCote <- exp(tableau[,1])
rappCote2.5 <- exp(tableau[,1] - 1.96 * tableau[,2])
rappCote97.5 <- exp(tableau[,1] + 1.96 * tableau[,2])
tableau <- cbind(tableau, rappCote, rappCote2.5, rappCote97.5)
print(round(tableau,3))
```

Pour faciliter la lecture des résultats, nous proposons le tableau \@ref(tab:TabFinalOrdinale).

```{r TabFinalOrdinale, message=FALSE, warning=FALSE, echo=FALSE}
tableau <- build_table(modele3, confid = T, sign = T, coef_digits = 2, std_digits = 3, z_digits = 2, p_digits = 3, OR_digits = 2)

tab <- tableau[,c(1,2,3,6,9,10,11)]

show_table(tab,
           col.names = c("Variable", "Coefficient", "RC", "P", "RC 2,5%", "RC 97,5%", "sign."),
           caption = "Coefficients du modèle logistique des cotes proportionnelles",
           )

```

Sans surprise, chaque lit supplémentaire contribue à augmenter les chances que le logement soit dans une catégorie de prix supérieure (multiplication par deux à chaque lit supplémentaire). En revanche, la présence d’un stationnement gratuit, d'un jardin et l’accueil en personne par l’hôte n’ont pas d’impacts significatifs. Comme l'indiquaient les articles mentionnés en début de section, les revues positives augmentent la probabilité d'appartenir à une catégorie supérieure de prix. Pour chaque point supplémentaire sur l'échelle de 1 à 5 augmente la probabilité d'appartenir à une catégorie de prix supérieure de 22,2$. Il est intéressant de noter que le fait de disposer du logement entier plutôt que d'une simple chambre augmente davantage les chances de passer du groupe de prix 1 à 2 (multiplication par 2,45) que du groupe 2 à 3 (multiplication par 1,58). Il semblerait également que si l’hôte possède plusieurs logements, la probabilité d’avoir une classe de prix supérieure diminue légèrement. Cependant l’effet est trop petit pour pouvoir se livrer à des interprétations.

Les variables environnementales ont peu d’impact : le pourcentage de surface commerciale dans un rayon d’un kilomètre et la présence d’une station de métro ne sont pas significatifs. En revanche, une augmentation de la surface végétale dans un rayon de 500 mètres tend à réduire la probabilité d’appartenir à une classe supérieure. Notre hypothèse concernant ce résultat est que cette variable représente un effet associé à la position des Airbnb, les plus centraux ayant tendance à être plus chers, mais avec un environnement moins vert et inversement. Pour l’illustrer, prédisons les probabilités d’appartenance aux différents niveaux de prix d’un logement avec les caractéristiques suivantes : entièrement privé, 2 lits, un jardin, une place de stationnement gratuite, l’hôte ne dispose que d’un logement sur Airbnb et accueille les arrivants en personne, 10% de surface commerciale dans un rayon d'un kilomètre, noté 2 comme catégorie de revue, absence de métro dans un rayon de 500 mètres.

```{r predpricecatveg, message=FALSE, warning=FALSE, out.width = "90%", fig.align = "center", fig.cap = "Prédiction de la probabilité d'appartenance aux trois catégories de prix en fonction de la densité de végétation"}

# Créer un jeu de données pour effectuer des prédictions
df <- data.frame(
  prt_veg_500m = seq(5,90),
  beds = 2, 
  Garden_or_backyard = "YES",
  Host_greets_you = "YES",
  Free_street_parking = "YES",
  has_metro_500m = "NO",
  commercial_1km = 10,
  private = "Entier",
  cat_review = 2,
  host_total_listings_count = 1
)

# Effectuer les prédictions (dans l'échelle log)
preds <- predict(modele3, newdata = df, type = "link", se.fit=T)

# Définir l'inverse de la fonction de lien
ilink <- function(x){exp(x)/(1+exp(x))}

# Calculer les probabilités et leurs intervalles de confiance
df[["P[Y>=2]"]] <- ilink(preds$fitted.values[,1])
df[["P[Y>=2] 2,5"]] <- ilink(preds$fitted.values[,1] - 1.96 * preds$se.fit[,1])
df[["P[Y>=2] 97,5"]] <- ilink(preds$fitted.values[,1] + 1.96 * preds$se.fit[,1])

df[["P[Y>=3]"]] <- ilink(preds$fitted.values[,2])
df[["P[Y>=3] 2,5"]] <- ilink(preds$fitted.values[,2] - 1.96 * preds$se.fit[,2])
df[["P[Y>=3] 97,5"]] <- ilink(preds$fitted.values[,2] + 1.96 * preds$se.fit[,2])

df[["P[Y=1]"]] = 1-df[["P[Y>=2]"]]
df[["P[Y=1] 2,5" ]] = 1-df[["P[Y>=2] 2,5"]]
df[["P[Y=1] 97,5"]] = 1-df[["P[Y>=2] 97,5"]]

# Afficher les résultats

ggplot(data = df) + 
  geom_ribbon(aes(x = prt_veg_500m, 
                         ymin = `P[Y>=2] 2,5`,
                         ymax = `P[Y>=2] 97,5`),fill ="#f94144", alpha = 0.4)+
  geom_path(aes(x = prt_veg_500m, y = `P[Y>=2]`,color="Y2")) + 
  geom_ribbon(aes(x = prt_veg_500m, 
                         ymin = `P[Y>=3] 2,5`,
                         ymax = `P[Y>=3] 97,5`), fill ="#90be6d", alpha = 0.4)+
  geom_path(aes(x = prt_veg_500m, y = `P[Y>=3]`, color = "Y3" )) + 
  geom_ribbon(aes(x = prt_veg_500m, 
                         ymin = `P[Y=1] 2,5`,
                         ymax = `P[Y=1] 97,5`),fill ="#277da1" , alpha = 0.4)+
  geom_path(aes(x = prt_veg_500m, y = `P[Y=1]`, color = "Y1")) + 
  scale_color_manual(name = "Probabilités prédites",
                     breaks = c("Y1", "Y2", "Y3"),
                     labels = c("P[Y=1]", "P[Y>=2]", "P[Y>=3]"),
                     values = c("Y2" = "#f94144", "Y3" = "#90be6d",
                                   "Y1" = "#277da1")) + 
  labs(x = "Densité de végétation (%)",
       y = "Probabilité",
       subtitle = "Y1 : moins de 50$; Y2 : 50 à 99$; Y3 : 100 à 249$")
  

```

On constate sur la figure \@ref(fig:predpricecatveg) que les probabilités d’appartenir aux niveaux 2 et 3 diminuent à mesure qu’augmente le pourcentage de végétation. La probabilité d’appartenir à la classe 2 et plus (en rouge), passe de plus de 95% en cas d’absence de végétation, à environ 75% avec 80% de végétation dans un rayon de 500m. Comme vous pouvez le constater, la probabilité $P[Y=1]$ est la symétrie de $P[Y>=2]$ puisque $P[Y=1]$ + $P[Y>=2]$ = 1.

#### Le modèle logistique multinomial {#sect06214}

La régression logistique multinomiale est utilisée pour modéliser une variable *Y* qualitative multinomiale, c’est-à-dire une variable dont les catégories ne peuvent pas être ordonnées. Dans le modèle précédent, nous avons vu qu’il était possible de modéliser une variable ordinale avec une distribution binomiale en formulant l’hypothèse de la proportionnalité des cotes. Avec une variable multinomiale, cette hypothèse ne tient plus car les catégories ne sont plus ordonnées. Il faut donc formuler le modèle différemment.
L’idée derrière un modèle multinomial est de choisir une catégorie de référence, puis de modéliser les probabilités d’appartenir à chaque autre catégorie plutôt qu’à cette catégorie de référence. Si nous avons *K* catégories possibles dans notre variable *Y*, nous obtenons *K*-1 comparaisons. Chaque comparaison est modélisée avec sa propre équation ce qui génère de nombreux paramètres. Par exemple, admettons que notre variable *Y* ait cinq catégories et que nous disposons de six variables *X* prédictives. Nous avons ainsi 4 (5-1) équations de régression avec 7 paramètres (6 coefficients et une constante), soit 28 coefficients à analyser. 

Considérant cette tendance à la multiplication des coefficients, il est fréquent de recourir à une méthode appelée *Analyse de type 3* pour limiter au maximum le nombre de variables indépendantes (VI) dans le modèle. L'idée de cette méthode est de recalculer plusieurs versions du modèle dans lesquelles une variable indépendante est retirée, puis de réaliser un test de rapport de vraisemblance en comparant ce nouveau modèle (complet moins une VI) au modèle complet (toutes les VI) pour vérifier si la variable en question améliore significativement le modèle. Il est alors possible de retirer toutes les variables dont l'apport est négligeable si elles étaient également peu intéressantes du point de vue théorique.

```{r multinomdentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim Binomial(p)$ \\newline $g(p = k\\text{ avec }ref = a) = \\beta_{0k} + \\beta X_{k}$ \\newline $g(x) = \\frac{log(x)}{1-x}$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable qualitative multinomiale avec *K* catégories", "Binomiale", model_formula , "logistique", "p", "$\\beta_{0k}$, $\\beta$k pour $k \\in [2,...,K]$", "Non-séparation complète, Absence de surdispersion ou sousdispersion, ’Indépendance des alternatives non pertinentes")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle probit binomial",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

##### Conditions d'application

Les conditions d’application sont les mêmes que pour un modèle binomial, avec l’ajout de l’hypothèse sur l’indépendance des alternatives non pertinentes. Cette dernière suppose que le choix entre deux catégories est indépendant des catégories proposées. Voici un exemple simple pour illustrer cette hypothèse : admettons que nous disposons d’une trentaine de personnes et que nous leur demandions la couleur de leurs yeux. Cette variable ne serait pas affectée par la présence de nouvelles couleurs. En revanche, si nous leur demandions de choisir un mode de transport parmi une liste pour se rendre à leur lieu de travail, leur réponse serait nécessairement affectée par la liste des modes de transport disponibles. Les tests développés pour vérifier cette hypothèse sont connus pour leur [faible fiabilité](https://statisticalhorizons.com/iia). Il est plus pertinent de décider théoriquement si cette hypothèse est valide ou non. Dans le cas contraire, il est possible d’utiliser une classe de modèle logistique plus rare : le modèle logistique imbriqué. 

Notez également que le grand nombre de paramètres dans ce type de modèle implique de disposer d’un plus grand nombre d’observations afin de disposer de suffisamment d’information pour ajuster tous les paramètres.

Pour vérifier la présence de surdispersion, il est possible, dans le cas du modèle multinomial, de calculer le rapport entre le Chi^2^ de Pearson et le nombre de degrés de liberté du modèle. Si ce rapport est supérieur à 1 (des valeurs jusqu'à 1,2 ne sont pas problématiques), alors le modèle souffre de surdispersion [@Sasmultinom]. Le Chi^2^ de Pearson permet de comparer les catégories observées dans la variable *Y* et les catégories prédites par le modèle. Il est possible de le calculer avec la formule suivante :

\footnotesize
\begin{equation}
\chi^2 = \sum_k{\frac{(O_k - E_k)^2}{E_k}}
(\#eq:glm15)
\end{equation}
\normalsize

Avec $O_k$ la proportion d'observations dans le groupe *k* et $E_k$ la proportion de valeurs prédites dans le groupe *k*.

Le ratio est ensuite calculé comme suit ainsi : $\frac{\chi^2}{(N - p)(K-1)}$, avec *N* le nombre d'observation et *K* le nombre de groupe dans la variable *Y*.

::: {.bloc_aller_loin data-latex=""}
**Le modèle logistique imbriqué**

Du fait de sa proximité avec les modèles à effets mixtes que nous aborderons plus tard (section X), nous ne détaillons pas ici le modèle logistique imbriqué, mais présentons plutôt son principe général. Il s’agit d’une généralisation du modèle logistique multinomial basé sur l’idée que certaines catégories pourraient être regroupées dans des « nids » (*nest* en anglais). Dans ces groupes, les erreurs peuvent être corrélées, indiquant ainsi que si une catégorie est manquante, une autre catégorie du même groupe sera préférée. Un paramètre $\lambda$ contrôle spécifiquement cette corrélation et permet donc de mesurer sa force une fois le modèle ajusté. Il peut donc être pertinent de comparer un modèle imbriqué à un modèle multinomial pour déterminer lequel des deux est le mieux ajusté aux données.
:::

##### Exemple appliqué dans R

Pour cet exemple, nous reproduisons une partie de l'analyse effectuée dans l'étude de @mcfadden2016examining. Cet article s'intéresse aux écarts entre les croyances des individus et les connaissances scientifiques sur les sujets des OGM et du réchauffement climatique. Les auteurs utilisent pour cela des données issues d'une enquête auprès de 961 individus formant un échantillon représentatif de la population des États-Unis. Les données issues de cette enquête sont téléchargeables sur le [site de l'éditeur](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0166140), ce qui nous permet ici de reproduire l'analyse effectuée par les auteurs. Deux questions sont centrales dans l'enquête : 

* Dans quelle mesure êtes-vous en accord ou en désaccord avec la phrase suivante : les plantations génétiquement modifiées sont sans danger pour la consommation.
* Dans quelle mesure êtes-vous en accord ou en désaccord avec la phrase suivante : la Terre se réchauffe du fait des activités humaines". Pour ces deux questions.

Les répondants devaient sélectionner leur degré d'accord sur une échelle de Likert allant de 1 (fortement en désaccord) à 5 (fortement en accord). Les réponses à ces deux questions ont été utilisées pour former une variable multinomiale à quatre modalités :

* A. Les individus sont en accord avec les deux propositions.
* B. Les individus sont en désaccord sur les OGM, mais en accord sur le réchauffement climatique.
* C. Les individus sont en accord sur les OGM, mais en désaccord sur le réchauffement climatique.
* D. Les individus sont en désaccord avec les deux propositions.

Un modèle logistique multinomial a été utilisé pour déterminer quels facteurs contribuent à la probabilité d'appartenir à ces différentes catégories. Les variables indépendantes présentes dans le modèle sont détaillées dans le tableau \@ref(tab:variablemultinom). Les auteurs avaient notamment conclu que : 

* Les effets des connaissances (réelles ou perçues) n'étaient pas uniformes et pouvaient varier en fonction du sujet.
* L'orientation politique avait une influence significative sur les croyances.
* Les répondants avec de plus hauts résultats au test de cognition CRT avaient plus souvent des opinions divergentes de la communauté scientifique.

```{r variablemultinom, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}
df <- data.frame(
  C1 = c("PercepOGM", "PercepRechClim", "ConnaisOGM", "ConnaisRechClim", "CRT", "Parti", "Sexe", "Age", "Revenu"),
  C2 = c("La recherche scientifique supporte ma vision sur la sécurité des plantes OGM", "La recherche scientifique supporte ma vision sur le réchauffement climatique", "Niveau de connaissance sur les OGM", "Niveau de connaissance sur le réchauffement climatique", "Score obtenu au Cognitive Reflection Test, utilisé pour déterminer la propension à faire preuve d’esprit d’analyse plutôt que choisir des réponses intuitives", "Orientation politique du répondant", "Sexe du répondant", "Âge du répondant", "Niveau de revenu du répondant"),
  C3 = c("Variable ordinale", "Variable ordinale", "Variable ordinale", "Variable ordinale", "Variable ordinale", "Variable multinomiale", "Variable binaire", "Variable continue", "Variable ordinale"),
  C4 = c("Échelle de Likert  de 1 (fortement en désaccord) à 5 (fortement en accord)", "Échelle de Likert  de 1 (fortement en désaccord) à 5 (fortement en accord)", "Nombre de réponses juste sur trois questions portant sur les OGM", "Nombre de réponses juste sur trois questions portant sur le réchauffement climatique", "Nombre de réponses juste sur trois questions pièges", "Républicain, Démocrate et autre", "Femme ou Homme", "Âge du répondant", "Échelle de 1 (moins de 20 000$) à 8 (140 000$ et plus)")
)

show_table(df, 
    caption = "Variables indépendantes utilisées dans le modèle logistique multinomial",
    col.names = c("Nom de la variable","signification","Type de variable","Mesure"), 
    col.to.resize = c(2,4),
    col.width = "5cm"
)

```

**Vérification des conditions d'application**

Avant d'ajuster le modèle, nous commençons par vérifier l'absence de multicolinéarité excessive entre les variables indépendantes. L'ensemble des valeurs de VIF sont inférieure à 2, indiquant bien une absence de multicolinéarité.

```{r vifmultinom, message=FALSE, warning=FALSE}
data_quest <- read.csv("data/glm/enquete_PublicOpinion_vs_Science.csv")

# choix des valeurs de références dans les facteurs
data_quest$Parti <- relevel(as.factor(data_quest$Parti), ref = "Democrate")
data_quest$Sexe <- relevel(as.factor(data_quest$Sexe), ref = "homme")

vif(glm(SCIGM ~ PercepOGM + PercepRechClim + ConnaisOGM + ConnaisRechClim +
          CRT + Parti + AGE + Sexe + Revenu, data = data_quest))
```

La seconde étape est d'ajuster le modèle et de vérifier l'absence de sur ou sous-dispersion. Pour ajuster le modèle, nous utilisons à nouveau la fonction `vglm` du *package* **VGAM**, avec le paramètre `family = multinomial()`. Le ratio entre la statistique de Pearson et le nombre de degrés de liberté du modèle n'indique pas de présence de surdispersion dans le modèle (0,124).


```{r message=FALSE, warning=FALSE}
# ajustement du modèle
modele <- vglm(Y ~ PercepOGM + PercepRechClim + ConnaisOGM + ConnaisRechClim + 
                 CRT + Parti + AGE + Sexe + Revenu,
               data = data_quest, 
               family = multinomial(refLevel="A"), model = T)

# calcul du Chi2 de Pearson
pred <- predict(modele, type= "response")
cat_predict <- colnames(pred)[max.col(pred)]

freq_real <- table(data_quest$Y)
freq_pred <- table(cat_predict)

chi2 <- sum(((freq_real - freq_pred)**2) / freq_pred)

N <- nrow(data_quest)
p <- modele@rank
r <- length(freq_real)
ratio <- chi2 / ((N-p)*(r-1))
print(ratio)
```

La troisième étape de la vérification des conditions d'application est l'analyse des distances de Cook. À nouveau, puisque le modèle évalue la probabilité d'appartenir à *K*-1 catégorie, nous pouvons calculer *K*-1 résidus par observation et par extension *K*-1 distances de Cook. Aucune observation ne semble se détacher nettement dans la figure \@ref(fig:cookmultinom). Nous décidons donc pour le moment de conserver toutes les observations.

```{r cookmultinom, message=FALSE, warning=FALSE, fig.cap = "Distances de Cook pour le modèle logistique multinomial", fig.align = "center", out.width = "95%"}

# Extraction des résidus
res <- residuals(modele, type = "pearson")
# Extraction de la hat matrix (nécessaire pour calculer la distance de Cook)
hat <- hatvaluesvlm(modele)

# Calcul des distances de Cook
vals <- c("A","B","C","D")

cooks <- lapply(1:ncol(res),function(i){
  r <- res[,i]
  h <- hat[,i]
  cook <- (r/(1 - h))^2 * h/(1 * modele@rank)
  df <- data.frame(
    oid = 1:length(cook),
    cook = cook
  )
  plot <- ggplot(data = df)+
    geom_point(aes(x = oid, y = cook), size = 0.2, color = rgb(0.1,0.1,0.1,0.4)) + 
    labs(x = "", y = "", subtitle = paste("distance de Cook P(",vals[[1]]," VS ",vals[[i+1]],")",sep=""))+
    theme(axis.ticks.x = element_blank(),
          axis.text.x = element_blank())
  return(plot)
})

ggarrange(plotlist = cooks, ncol = 2, nrow = 2)

```

Avant de passer à l'analyse de résidus simulés, il est pertinent de réaliser une analyse de type 3 afin de retirer les variables indépendantes dont l'apport au modèle est négligeable. La fonction `AnalyseType3` (disponible dans le code source de ce livre) permet d'effectuer cette opération automatiquement pour un objet de type `vglm`.

```{r  message=FALSE, warning=FALSE}
tableau <- AnalyseType3(modele, data_quest)
```

L'analyse de type 3 nous permet de déterminer que l'âge et le revenu sont deux variables dont la contribution au modèle est marginale. Nous décidons donc de les retirer afin d'alléger les tableaux de coefficients que nous présenterons plus loin. Nous pouvons également en conclure que ces deux variables ne jouent aucun rôle dans la propension à être en désaccord avec la recherche scientifique. Nous réajustons le modèle en conséquence.

```{r  message=FALSE, warning=FALSE}
modele2 <- vglm(Y ~ PercepOGM + PercepRechClim + ConnaisOGM + ConnaisRechClim + 
                 CRT + Parti + Sexe,
               data = data_quest, 
               family = multinomial(refLevel="A"), model = T)
```

Nous pouvons à présent passer à l'analyse des résidus simulés. Le problème avec ce modèle est que sa variable *Y* est qualitative alors que la méthode d'analyse des résidus du *package* **DHARMa** ne peut traiter que variable quantitative, binaire ou ordinale. Pour rappel, il est possible d'envisager la prédiction d'un modèle logistique multinomial comme la prédiction d'une série de modèles logistiques binomiaux. En représentant nos prédictions de cette façon, nous pouvons utiliser à nouveau utiliser le *package* **DHARMa** pour analyser nos résidus. Veuillez noter que cette approche n'est pas forcément optimale que et cette section du livre peut être amenée à changer.

La figure \@ref(fig:residusmultinom) indique que les résidus suivent bien une distribution uniforme et qu'aucune valeur aberrante n'est observable.
```{r residusmultinom, fig.align="center", fig.cap="Diagnostic général des résidus simulés pour le modèle multinomial", message=FALSE, warning=FALSE, out.width="85%"}

# extraire les prédictions du modèle
categories <- c("B","C","D")
predicted <- predict(modele2, type = "link")

nsim <- 1000
ilink <- function(x){exp(x)/(1+exp(x))}

# boucler sur chacune des catégories en dehors de la référence
data_sims <- lapply(1:ncol(predicted),function(i){
  categorie <- categories[[i]]
  # extraire les observation de la catégorie i et de la référence
  test <- data_quest$Y %in% c("A",categorie)
  # calculer les probabilité d'appartenance à i
  values <- predicted[test,i]
  probs <- ilink(values)
  # extraire les valeurs réelles et les convertir en 0 / 1
  real <- data_quest[test,]$Y
  real <- ifelse(real=="A",0,1)
  # enregistrer ces différents éléments
  all_probs <- cbind(1-probs,probs)
  sub_data <- subset(data_quest,test)
  return(list("real" = real, "probs" = all_probs, "data" = sub_data))
})

# extraire les probabilités prédites
all_probs <- do.call(rbind, lapply(data_sims, function(i){i$probs}))
# extraire les vrais catégories
all_real <- unlist(lapply(data_sims, function(i){i$real}))

# effectuer les simulations
simulations <- lapply(1:nrow(all_probs), function(i){
  probs <- all_probs[i,]
  sims <- sample(c(0,1), size = nsim, replace = T, prob = probs)
  return(sims)
})
matsim <- do.call(rbind, simulations)

# calculer les résidus simulés
sim_res <- createDHARMa(simulatedResponse = matsim, 
                            observedResponse = all_real,
                            fittedPredictedResponse = all_probs[,2],
                            integerResponse = T)

# afficher les résultats
plot(sim_res)
```

```{r eval=FALSE, include=FALSE}
# Extraction des prédictions (en probabilité)
# predicted <- predict(modele2,type = "response")
# 
# nsim <- 1000
# # Lancement des simulations pour chaque observation (chaque ligne dans predicted)
# simulations <- lapply(1:nrow(predicted), function(i){
#   probs <- predicted[i,]
#   sims <- sample(c(1,2,3,4), size = nsim, replace = T, prob = probs)
#   return(sims)
# })
# matsim <- do.call(rbind, simulations)
# 
# # On conserve la catégorie avec la plus haute probabilité 
# # comme catégorie prédite
# pred_cat <- colnames(predicted)[max.col(predicted)]
# 
# # On convertit les catégories en nombres entiers (nécessaire pour le package DHARMa)
# real <- case_when(data_quest$Y == "A" ~ 1,
#                   data_quest$Y == "B" ~ 2,
#                   data_quest$Y == "C" ~ 3,
#                   data_quest$Y == "D" ~ 4)
# 
# pred <- case_when(pred_cat == "A" ~ 1,
#                   pred_cat == "B" ~ 2,
#                   pred_cat == "C" ~ 3,
#                   pred_cat == "D" ~ 4)
# 
# # On calcule les résidus simulés
# sim_res <- createDHARMa(simulatedResponse = matsim, 
#                         observedResponse = real,
#                         fittedPredictedResponse = predicted,
#                         integerResponse = T)
# 
# DHARMa::plotQQunif(sim_res)
```

La figure \@ref(fig:residusmultinom2) permet d'affiner le diagnostic en s'assurant de l'absence de relation entre les variables indépendantes et les résidus. Il est possible de remarquer des irrégularités pour les variables de perceptions (premier et second panneaux). Dans les deux cas, la catégorie 1 (fort désaccord) se démarque nettement des autres catégories. Nous proposons donc de les recoder comme des variables binaires : en désaccord / pas en désaccord pour minimiser cet effet.

```{r residusmultinom2, message=FALSE, warning=FALSE, fig.cap = "Diagnostic par variable des résidus simulés pour le modèle multinomial", fig.align = "center", out.width = "95%"}

# recomposer les donnees pour coller au format
# etendu de la prediction

etend_data <- do.call(rbind, lapply(data_sims, function(i){i$data}))


par(mfrow=c(3,3))
vars <- c("PercepOGM", "PercepRechClim", "ConnaisOGM", 
          "ConnaisRechClim", "CRT", "Parti", "Sexe")

for(v in  vars){
  plotResiduals(sim_res, etend_data[[v]], main = v)
}

```

Nous réajustons donc le modèle et recalculons nos résidus ajustés (masqué ici pour alléger le document). La figure \@ref(fig:residusmultinom3) nous confirme que le problème a été corrigé.

```{r}
# Convertir les variables ordinales et variables binaires
data_quest$PercepOGMDes <- ifelse(data_quest$PercepOGM %in% c(1,2), 1,0)
data_quest$PercepRechClimDes <- ifelse(data_quest$PercepRechClim %in% c(1,2), 1,0)

# Réajuster le modèle
modele3 <- vglm(Y ~ PercepOGMDes + PercepRechClimDes + 
                  ConnaisOGM + ConnaisRechClim + 
                  CRT + Parti + Sexe,
               data = data_quest, 
               family = multinomial(refLevel="A"), model = T)
```


```{r residusmultinom3, message=FALSE, warning=FALSE, fig.cap = "Diagnostic général des résidus simulés pour le modèle multinomial (version 3)", fig.align = "center", out.width = "75%", echo = FALSE}

# extraire les prédictions du modèle
categories <- c("B","C","D")
predicted <- predict(modele3, type = "link")

nsim <- 1000
ilink <- function(x){exp(x)/(1+exp(x))}

# boucler sur chacune des catégories en dehors de la référence
data_sims <- lapply(1:ncol(predicted),function(i){
  categorie <- categories[[i]]
  # extraire les observation de la catégorie i et de la référence
  test <- data_quest$Y %in% c("A",categorie)
  # calculer les probabilité d'appartenance à i
  values <- predicted[test,i]
  probs <- ilink(values)
  # extraire les valeurs réelles et les convertir en 0 / 1
  real <- data_quest[test,]$Y
  real <- ifelse(real=="A",0,1)
  # enregistrer ces différents éléments
  all_probs <- cbind(1-probs,probs)
  sub_data <- subset(data_quest,test)
  return(list("real" = real, "probs" = all_probs, "data" = sub_data))
})

# extraire les probabilités prédites
all_probs <- do.call(rbind, lapply(data_sims, function(i){i$probs}))
# extraire les vrais catégories
all_real <- do.call(c, lapply(data_sims, function(i){i$real}))

# effectuer les simulations
simulations <- lapply(1:nrow(all_probs), function(i){
  probs <- all_probs[i,]
  sims <- sample(c(0,1), size = nsim, replace = T, prob = probs)
  return(sims)
})
matsim <- do.call(rbind, simulations)

# calculer les résidus simulés
sim_res <- createDHARMa(simulatedResponse = matsim, 
                            observedResponse = all_real,
                            fittedPredictedResponse = all_probs[,2],
                            integerResponse = T)

etend_data <- do.call(rbind, lapply(data_sims, function(i){i$data}))

par(mfrow=c(3,3))
vars <- c("PercepOGMDes", "PercepRechClimDes", "ConnaisOGM", 
          "ConnaisRechClim", "CRT", "Parti", "Sexe")

for(v in  vars){
  plotResiduals(sim_res, etend_data[[v]], main = v)
}

```

```{r eval=FALSE, include=FALSE}
# 
# # extraction des prédictions (en probabilité)
# predicted <- predict(modele3,type = "response")
# 
# nsim <- 1000
# # lancement des simulations pour chaque observation (chaque lignes dans predicted)
# simulations <- lapply(1:nrow(predicted), function(i){
#   probs <- predicted[i,]
#   sims <- sample(c(1,2,3,4), size = nsim, replace = T, prob = probs)
#   return(sims)
# })
# matsim <- do.call(rbind, simulations)
# 
# # on conserver la catégorie avec la plus haute probabilité 
# # comme catégorie prédite
# pred_cat <- colnames(predicted)[max.col(predicted)]
# 
# # on convertit les catégories en nombres entiers (nécessaire pour le package DHARMa)
# 
# real <- case_when(data_quest$Y == "A" ~ 1,
#                   data_quest$Y == "B" ~ 2,
#                   data_quest$Y == "C" ~ 3,
#                   data_quest$Y == "D" ~ 4)
# 
# pred <- case_when(pred_cat == "A" ~ 1,
#                   pred_cat == "B" ~ 2,
#                   pred_cat == "C" ~ 3,
#                   pred_cat == "D" ~ 4)
# 
# # on calcule les résidus simulés
# sim_res <- createDHARMa(simulatedResponse = matsim, 
#                         observedResponse = real,
#                         fittedPredictedResponse = pred,
#                         integerResponse = T)
# 
# par(mfrow=c(3,3))
# vars <- c("PercepOGMDes", "PercepRechClimDes", "ConnaisOGM", 
#           "ConnaisRechClim", "CRT", "Parti", "Sexe")
# 
# for(v in  vars){
#   plotResiduals(sim_res, data_quest[[v]], main = v)
# }
```

Profitons du fait que nous utilisons le package **VGAM** pour vérifier l'absence d'effet de Hauck-Donner qui indiquerait que des variables indépendantes provoquent des séparations parfaites. 

```{r  message=FALSE, warning=FALSE}
test <- hdeff(modele3)
test[test==TRUE]
```

La fonction nous informe que les constantes permettant de comparer le groupe C au groupe A, et le groupe D au groupe A provoquent des séparations parfaites. Ceci s'explique notamment par le faible nombre d'observations tombant dans ces catégories. Considérant que comparativement à la catégorie A, être dans les catégories B, C, ou D signifie remettre en cause au moins un consensus scientifique, il peut être raisonnable de fixer la constante pour qu'elle soit la même pour les trois comparaisons. Ainsi, les chances de passer de A à un autre groupe ne dépenderait pas du groupe en question, mais uniquement des prédicteurs. Pour cela, nous pouvons forcer le modèle à n'ajuster qu'une seule constante avec la syntaxe suivante :

```{r  message=FALSE, warning=FALSE}
modele4 <- vglm(Y ~ PercepOGMDes + PercepRechClimDes + 
                  ConnaisOGM + ConnaisRechClim + 
                  CRT + Parti + Sexe,
               data = data_quest, 
               family = multinomial(refLevel="A",
                                    parallel = TRUE ~1), model = T)
test <- hdeff(modele4)
print(table(test))
```

Nous n'avons donc plus de séparation complète, et les résidus simulés de la quatrième version du modèle sont toujours acceptables (figure \@ref(fig:residusmultinom4)).

```{r residusmultinom4, message=FALSE, warning=FALSE, fig.cap = "Diagnostic général des résidus simulés pour le modèle multinomial (version 4)", fig.align = "center", out.width = "75%", echo = FALSE}
# extraire les prédictions du modèle
categories <- c("B","C","D")
predicted <- predict(modele4, type = "link")

nsim <- 1000
ilink <- function(x){exp(x)/(1+exp(x))}

# boucler sur chacune des catégories en dehors de la référence
data_sims <- lapply(1:ncol(predicted),function(i){
  categorie <- categories[[i]]
  # extraire les observation de la catégorie i et de la référence
  test <- data_quest$Y %in% c("A",categorie)
  # calculer les probabilité d'appartenance à i
  values <- predicted[test,i]
  probs <- ilink(values)
  # extraire les valeurs réelles et les convertir en 0 / 1
  real <- data_quest[test,]$Y
  real <- ifelse(real=="A",0,1)
  # enregistrer ces différents éléments
  all_probs <- cbind(1-probs,probs)
  sub_data <- subset(data_quest,test)
  return(list("real" = real, "probs" = all_probs, "data" = sub_data))
})

# extraire les probabilités prédites
all_probs <- do.call(rbind, lapply(data_sims, function(i){i$probs}))
# extraire les vrais catégories
all_real <- do.call(c, lapply(data_sims, function(i){i$real}))

# effectuer les simulations
simulations <- lapply(1:nrow(all_probs), function(i){
  probs <- all_probs[i,]
  sims <- sample(c(0,1), size = nsim, replace = T, prob = probs)
  return(sims)
})
matsim <- do.call(rbind, simulations)

# calculer les résidus simulés
sim_res <- createDHARMa(simulatedResponse = matsim, 
                            observedResponse = all_real,
                            fittedPredictedResponse = all_probs[,2],
                            integerResponse = T)

plot(sim_res)
```

```{r eval=FALSE, include=FALSE}
# Extraction des prédictions (en probabilité)
# predicted <- predict(modele4,type = "response")
# 
# nsim <- 1000
# # Lancement des simulations pour chaque observation (chaque lignes dans predicted)
# simulations <- lapply(1:nrow(predicted), function(i){
#   probs <- predicted[i,]
#   sims <- sample(c(1,2,3,4), size = nsim, replace = T, prob = probs)
#   return(sims)
# })
# matsim <- do.call(rbind, simulations)
# 
# # On conserver la catégorie avec la plus haute probabilité 
# # comme catégorie prédite
# pred_cat <- colnames(predicted)[max.col(predicted)]
# 
# # On convertit les catégories en nombres entiers (nécessaire pour le package DHARMa)
# 
# real <- case_when(data_quest$Y == "A" ~ 1,
#                   data_quest$Y == "B" ~ 2,
#                   data_quest$Y == "C" ~ 3,
#                   data_quest$Y == "D" ~ 4)
# 
# pred <- case_when(pred_cat == "A" ~ 1,
#                   pred_cat == "B" ~ 2,
#                   pred_cat == "C" ~ 3,
#                   pred_cat == "D" ~ 4)
# 
# # On calcule les résidus simulés
# sim_res <- createDHARMa(simulatedResponse = matsim, 
#                         observedResponse = real,
#                         fittedPredictedResponse = pred,
#                         integerResponse = T)
# 
# par(mfrow=c(3,3))
# vars <- c("PercepOGMDes", "PercepRechClimDes", "ConnaisOGM", 
#           "ConnaisRechClim", "CRT", "Parti", "Sexe")
# 
# for(v in  vars){
#   plotResiduals(sim_res, data_quest[[v]], main = v)
# }
```

**Vérifier l'ajustement du modèle**

Puisque les conditions d'application du modèle sont respectées, nous pouvons à présent vérifier sa qualité d'ajustement.

```{r  message=FALSE, warning=FALSE}
modelenull <- vglm(Y ~ 1 ,
                   data = data_quest, 
                   family = multinomial(refLevel="A",
                                        parallel = TRUE ~ 1 + CRT)
                   , model = T)

rsqs(loglike.full = logLik(modele4),
     loglike.null = logLik(modelenull),
     full.deviance = deviance(modele4),
     null.deviance = deviance(modelenull),
     nb.params = modele4@rank,
     n = nrow(data_quest)
)
```

Le modèle parvient à expliquer 17,5% de la déviance totale, il obtient un R^2^ ajusté de McFadden de 0,15, et des R^2^ de Cox et Snell et de Nagelkerke de respectivement 0,34 et 0,37. Passons à la construction de la matrice de confusion pour analyser la capacité de prédiction du modèle.

```{r message=FALSE, warning=FALSE}
preds <- predict(modele4, type = "response")
pred_cats <- colnames(preds)[max.col(preds)]

real <- data_quest$Y

matrices <- nice_confusion_matrix(real, pred_cats)

# Afficher la matrice de confusion
print(matrices$confusion_matrix)

# Afficher les indicateurs de qualité de prédiction
print(matrices$indicators)
```
La précision globale du modèle est de 60% et dépasse significativement le seuil de non-information. L'indicateur de Kappa indique un accord modéré entre la prédiction et les valeurs réelles. Les catégories C et D sont les catégories avec la plus faible précision, indiquant ainsi que le modèle a tendance à manquer les prédictions pour les individus en désaccord avec le consensus scientifique sur le réchauffement climatique. Les indices de rappel sont également très faibles pour les catégories B, C et D, indiquant que très peu d'observations appartenant originalement à ces groupes ont bien été classées dans ces groupes. La capacité de prédiction du modèle est donc relativement faible.

**Interprétation des résulats**

Puisque nous disposons de quatre catégories dans notre variable *Y*, nous obtenons au final trois tableaux de coefficients. Il est possible de visualiser l'ensemble des coefficients du modèle avec la fonction `summary`, nous proposons les tableaux \@ref(tab:coeffsmultinom1), \@ref(tab:coeffsmultinom2) et \@ref(tab:coeffsmultinom3) pour présenter l'ensemble des résultats.

```{r  message=FALSE, warning=FALSE, echo = FALSE}
tables <- build_table(modele4,confid = T, sign = T, coef_digits = 2, std_digits = 2, z_digits = 2, p_digits = 3, OR_digits = 3)

tab1 <- tables[[1]][,c(1,2,3,6,9,10,11)]
tab2 <- tables[[2]][,c(1,2,3,6,9,10,11)]
tab3 <- tables[[3]][,c(1,2,3,6,9,10,11)]
```

```{r coeffsmultinom1, message=FALSE, warning=FALSE, echo=FALSE}
show_table(tab1,
           col.names = c("Variable", "Coefficient", "RC", "val.p", "RC 2,5%", "RC 97,5%", "sign."),
           caption = "Coefficients du modèle multinomial A versus B",
           )
```

Le tableau \@ref(tab:coeffsmultinom1) compare donc le groupe A (en accord avec la recherche scientifique sur les deux sujets) et le groupe B (en désaccord sur la question des OGM). Les résultats indiquent que le fait de se percevoir en désaccord avec le consensus scientifique sur la question des OGM multiple par sept les chances d'appartenir au groupe B comparativement au groupe A. Cependant, pour chaque bonne réponse supplémentaire sur les questions testant les connaissances sur les OGM, les chances d'appartenir au groupe B comparativement au groupe A diminue de 28%. Ainsi, un individu ayant répondu correctement aux trois questions verrait ses chances réduites de 63% d'appartenir au groupe B (exp(-0.33*3)). Il est intéressant de noter que les variables concernant le réchauffement climatique n'ont pas d'effet significatif ici. La variable CRT indique qu'à chaque bonne réponse supplémentaire au test de cognition, les chances d'appartenir au groupe B augment de 42%. Un individu qui aurait répondu juste aux trois questions du test aurait donc 2,9 fois plus de chances d'appartenir au groupe B qu'au groupe A. Concernant le parti politique, comparativement à une personne se déclarant plus proche du parti démocrate, les personnes proches du parti républicain ou d'un autre parti ont près de deux fois plus de chances d'appartenir au groupe B. Enfin, une femme, comparativement à un homme, à 3,6 fois plus de chance d'appartenir au groupe B.

```{r coeffsmultinom2, message=FALSE, warning=FALSE, echo=FALSE}
show_table(tab2,
           col.names = c("variable", "coefficient", "RC", "val.p", "RC 2,5%", "RC 97,5%", "sign."),
           caption = "Coefficients du modèle multinomial A versus C",
           )
```

Le tableau \@ref(tab:coeffsmultinom2) compare les groupes A et C (en désaccord sur le réchauffement climatique). Il est intéressant de noter ici que se percevoir en désaccord avec la recherche scientifique est associé avec une forte augmentation des chances d'appartenir au groupe C. Cependant, un plus grand nombre de bonnes réponses aux questions sur le réchauffement climatique est également associé avec une augmentation des chances (30% à chaque bonne réponse supplémentaire) d'appartenir au groupe C. Le CRT n'a cette fois-ci pas d'impact. Se déclarer proche du parti républicain, comparativement au parti démocrate, multiplie les chances par 2,5 d'appartenir au groupe C. Comparativement au tableau précédent, le fait d'être une femme diminue les chances de 36% d'appartenir au groupe C.

```{r coeffsmultinom3, message=FALSE, warning=FALSE, echo=FALSE}
show_table(tab3,
           col.names = c("variable", "coefficient", "RC", "val.p", "RC 2,5%", "RC 97,5%", "sign."),
           caption = "Coefficients du modèle multinomial A versus D",
           )
```

Le dernier tableau  \@ref(tab:coeffsmultinom3) compare le groupe A au groupe D (en désaccord sur les deux sujets). Les variables les plus importantes sont une fois encore le fait de se sentir en désaccord avec la recherche scientifique et le degré de connaissance sur les OGM. La variable concernant le parti politique est significative au seuil 0,05 et exprime toujours une tendance accrue pour les individus du parti républicain à appartenir au groupe D.

Nos propres conclusions corroborent celles de l'article original. Une des conclusions intéressantes est que le rejet du consensus scientifique ne semble pas nécessairement être associé à un déficit d'information ni à une plus faible capacité analytique, mais relèverait davantage d'une polarisation politique. Notez que cette littérature sur les croyances et la confiance dans la recherche est complexe, si le sujet vous intéresse, la discussion de l'article de @mcfadden2016examining est un bon point de départ.


#### Conclusion sur les modèles pour des variables qualitatives

Nous avons pu voir dans cette section les trois principales formes de modèles GLM pour modéliser une variable binaire (modèle binomial), une variable ordinale (modèle de cotes proportionnel) et une variable multinomiale (modèle multinomial). Pour ces trois modèles, nous avons vu que la distribution utilisée est toujours la distribution binomiale et la fonction de lien la fonction logistique. Les coefficients obtenus s'interprètent comme des rapports de cote, une fois qu'ils sont transformés avec la fonction exponentielle. Nous avons également vu le modèle binomial probit, une variante du modèle binomial logistique utilisant la fonction *probit* comme fonction de lien. Sachez qu'il est également possible d'utiliser la fonction de lien probit pour le modèle des cotes proportionnelles et le modèle multinomial.

### Les modèles GLM pour des variables de comptage {#sect0622}

Dans cette section, nous présentons les principaux modèles utilisés pour modéliser des variables de comptage. Il peut s'agir de variables comme le nombre d'accidents à une intersection, le nombre de cafés par quartier, le nombre de cas d'une maladie donnée par secteur de recensement, etc.

#### Le modèle de Poisson {#sect06221}

Le modèle GLM de base pour modéliser une variable de comptage est le modèle de Poisson. Pour rappel, la distribution de Poisson a un seul paramètre, soit $\lambda$. Il représente le nombre moyen d'évènements observés sur l’intervalle de temps retenu, ainsi que la variance de la distribution. En conséquence, $\lambda$ doit être un nombre strictement positif; autrement dit, on ne peut pas observer un nombre négatif d'évènements. Il est donc nécessaire d’utiliser une fonction de lien pour contraindre l’équation de régression sur l’intervalle $]0 ,+\infty]$. La fonction la plus utilisée est le logarithme naturel (log) dont la réciproque est la fonction exponentielle (exp).

```{r poissondentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim Poisson(\\lambda)$ \\newline $g(\\lambda) = \\beta_0 + \\beta X$ \\newline $g(x) =log(x)$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable de comptage", "Poisson", model_formula , "log", "$\\lambda$", "$\\beta_0$, $\\beta$", "Absence d’excès de zéros, absence de surdispersion ou de sousdispersion")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle de Poisson",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

##### Interprétation des paramètres

Les coefficients du modèle expriment l’impact du changement d’une unité des variables *X* sur $\lambda$ (le nombre de cas) dans l’échelle logarithmique (*log-scale*). Pour rappel, l’échelle logarithmique est multiplicative : si l’on convertit les coefficients dans leur échelle originale avec la fonction exponentielle, leur effet n’est plus additif, mais multiplicatif.
Prenons un exemple concret, admettons que nous ayons ajusté un modèle de Poisson à une variable de comptage *Y* avec deux variables $X_1$ et $X_2$ et que nous ayons obtenus les coefficients suivants :

$\beta_0 = 1,8 \text{ ; } \beta_1 = 0,5 \text{ ; } \beta_2 = -1,5$

L’interprétation basique (sur l’échelle logarithmique) est la suivante : une augmentation d'une unité de la variable $X_1$ est associée avec une augmentation de 0,5 du logarithmique du nombre de cas attendus. Une augmentation d'une unité de la variable $X_2$ est associée avec une réduction de 1,5 unités du logarithmique du nombre de cas attendus. Avec la conversion avec la fonction exponentielle, on obtient alors : 

* $exp(0,5) = 1,649$, soit une multiplication par 1,649 du nombre de cas attendu à chaque augmentation d’une unité de $X_1$.
* $exp(-1,5) = 0,223$, soit une division par 4,54 (1/0,223) du nombre de cas attendu à chaque augmentation d’une unité de $X_2$.

Utilisons maintenant notre équation pour effectuer une prédiction si $X_1 = 1$ et $X_2 = 1$.

$\lambda = exp(1,8 + (0,5\times 1) + (-1,5\times1)) = 2,225$

Si nous augmentons $X_1$ d’une unité, nous obtenons alors : 

$\lambda = exp(1,8 + (0,5\times 2) + (-1,5\times1)) = 3,670$

En ayant augmenté d’une unité $X_1$, nous avons multiplié notre résultat par 1,649 ($2,225 \times 1,649 = 3,670$)

Notez que ces effets se multiplient entre eux. Si nous augmentons à la fois $X_1$ et $X_2$ d'une unité chacune, nous obtenons : $\lambda = exp(1,8 + (0,5 \times 2) + (-1,5 \times 2)) = 0,818$, ce qui correspond bien à $2,225 \times 1,649 \text{ (effet de  }X_1\text{)} \times 0,223 \text{ (effet de }X_2\text{)} = 0,818$.

Il existe des fonctions dans R qui calculent ces prédictions à partir des équations des modèles. Il est cependant essentiel de bien saisir ce comportement multiplicatif induit par la fonction de lien log.

##### Conditions d'application

Puisque la distribution de Poisson n’a qu’un seul paramètre, le modèle GLM de Poisson est exposé au même problème potentiel de surdispersion que les modèles binomiaux de la section précédente. Référez-vous à la section \@ref(sectcondapp) pour davantage de détails sur le problème posé par la surdispersion. Pour détecter une potentielle surdispersion dans un modèle de Poisson, il est possible dans un premier temps de calculer le ratio entre la déviance du modèle et son nombre de degrés de liberté [@Saspoiss]. Ce ratio doit être proche de 1, s'il est plus grand, le modèle souffre de surdispersion.

\footnotesize
\begin{equation}
\hat{\phi} = \frac{D(modele)}{N-p}
(\#eq:glm16)
\end{equation}
\normalsize

avec $N$ et $p$ étant respectivement les nombres d'observations et de paramètres. Il est également possible de tester formellement si la surdispersion est significative avec un test de dispersion.

La question de l’excès de zéros a été abordée dans la section \@ref(sect02437) présentant les distributions. Il s’agit d’une situation où un plus grand nombre de zéros sont présents dans les données que supposé selon la distribution de Poisson. Dans ce cas, il convient d'utiliser la distribution de Poisson avec excès de zéros.

##### Exemple appliqué dans R

Pour cet exemple, nous allons reproduire l’analyse effectuée dans l’article de @cloutier2014carrefours. L’enjeu de cette étude était de modéliser le nombre de piétons blessés autour de plus de 500 carrefours dans les quartiers centraux de Montréal. Pour cela, trois types de variables étaient utilisées : des variables décrivant l’intersection, des variables décrivant les activités humaines dans un rayon d'un kilomètres autour de l’intersection et des variables représentant le trafic routier autour de l’intersection. Un impact direct de ce type d’étude est bien évidemment l’établissement de meilleures pratiques d’aménagement réduisant les risques encourus par les piétons lors de leurs déplacements en ville. Le tableau \@ref(tab:variablepoiss) présente l'ensemble des variables utilisées dans l'analyse.

```{r variablepoiss, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}
df <- data.frame(
  C1 = c("Feux_auto", " Feux_piet", " Pass_piet", " Terreplein", " Apaisement", " LogEmploi", " Densite_pop", " Entropie", " DensiteInter", " Artere", " Long_arterePS", " NB_voies5"),
  C2 = c("Présence de feux de circulation", " Présence de feux de traversée pour les piétons", " Présence d’un passage piéton", " Présence d’un terre-plein central", " Présence de mesure d’apaisement de la circulation", " Logarithme du nombre d’emplois dans un rayon d’un kilomètre", " Densité de population dans un rayon d’un kilomètre", " Diversité des occupations du sol dans un rayon d’un kilomètre (indice d'entropie)", " Densité d’intersections dans un rayon d’un kilomètre (connexité)", " Présence d’une artère à l’intersection", " Longueur d’artère dans un rayon d’un kilomètre", " Présence d’une cinq voies à l’intersection"),
  C3 = c("Variable binaire", " Variable binaire", " Variable binaire", " Variable binaire", " Variable binaire", " Variable continue", " Variable continue", " Variable continue", " Variable continue", " Variable binaire", " Variable continue", " Variable binaire"),
  C4 = c("0 = absence; 1 = présence", " 0 = absence; 1 = présence", " 0 = absence; 1 = présence", " 0 = absence; 1 = présence", " 0 = absence; 1 = présence", " Logarithme du nombre d’emploi. Utilisation du logarithme car la variable est fortement asymétrique", " Habitants par hectare", " Mesure de 0 à 1; 0 = spécialisation parfaite; 1 = diversité parfaite ", " Nombre d’intersection par km2 ", " 0 = absence; 1 = présence", " Exprimée en mètres", " 0 = absence; 1 = présence")
)

show_table(df, 
    caption = "Variables indépendantes utilisées dans le modèle de Poisson",
    col.names = c("Nom de la variable","signification","Type de variable","Mesure"), 
    col.to.resize = c(2,4),
    col.width = "5cm"
)

```

La distribution originale de la variable est décrite à la figure \@ref(fig:distripoissmod). Les barres grises représentent la distribution du nombre d'accidents et les barres rouges une distribution de Poisson ajustée sans prédicteur (modèle nul). Ce premier graphique peut laisser penser qu’un modèle de Poisson ne sera pas nécessairement le plus adapté considérant le grand nombre d’intersections pour lesquelles nous n’avons aucun accident. Cependant, rappelons que la variable *Y* n'a pas besoin de suivre une distribution de Poisson. Dans un modèle GLM, l'hypothèse que nous formulons est que la variable dépendante (*Y*) **conditionnée par les variables indépendantes (*X*)** suit une certaine distribution (ici de Poisson).

```{r distripoissmod, fig.cap = "Distribution originale du nombre d'accidents par intersection", fig.align = "center", message=FALSE, warning=FALSE, out.width='65%'}

# Chargement des données
data_accidents <- read.csv("data/glm/accident_pietons.csv")

# Ajustement d'une distribution de Poisson sans prédicteur
library(fitdistrplus)
model_poisson <- fitdist(data_accidents$Nbr_acci,distr = "pois")

# Création d'un graphique pour comparer les deux distributions
dfpoisson <- data.frame(x=c(0:19),
                        y=dpois(0:19, model_poisson$estimate)
)

counts <- data.frame(table(data_accidents$Nbr_acci))
names(counts) <- c("nb_accident",'frequence')
counts$nb_accident <- as.numeric(as.character(counts$nb_accident))
counts$prop <- counts$frequence / sum(counts$frequence)

ggplot() + 
  geom_bar(aes(x=nb_accident, weight = prop, fill = "real"), width = 0.6, data = counts)+
  geom_bar(aes(x=x, weight = y, fill = "adj"), width = 0.15, data = dfpoisson)+
  scale_x_continuous(limits = c(-0.5,7), breaks = c(0:7))+
  scale_fill_manual(name = "",
          breaks = c("real","adj"),
          labels = c("distribution originale", "distribution de Poisson"),
          values = c("real" = rgb(0.4,0.4,0.4), "adj" = "red"))+
  labs(subtitle = "",
       x = "nombre d'accidents",
       y = "")
```

**Vérification des conditions d'application**

Comme précédemment, notre première étape est de vérifier l'absence de multicolinéarité excessive avec la fonction `vif` du package **car**.

```{r message=FALSE, warning=FALSE}
vif(glm(Nbr_acci ~ Feux_auto + Feux_piet + Pass_piet + Terreplein + Apaisement +
          LogEmploi + Densite_pop + Entropie + DensiteInter +
          Long_arterePS + Artere + NB_voies5,
        family = poisson(link="log"),
        data = data_accidents))
```

Toutes les valeurs de VIF sont inférieures à 5, notons tout de même que le logarithme de l'emploi et la longueur d'artère dans un rayon d'un kilomètre ont des valeurs de VIF proches de 5. La seconde étape du diagnostic consiste à calculer et visualiser les distances de Cook.

```{r poisscookdist, fig.cap = "Distances de Cook pour le modèle de Poisson", fig.align = "center", message=FALSE, warning=FALSE, out.width='60%'}

# Ajustement d'une première version du modèle
modele <- glm(Nbr_acci ~ Feux_auto + Feux_piet + Pass_piet + Terreplein + Apaisement +
                LogEmploi + Densite_pop + Entropie + DensiteInter +
                Long_arterePS + Artere + NB_voies5,
              family = poisson(link="log"),
              data = data_accidents)

# Calcul des distances de Cook
cooksd <- cooks.distance(modele)
df <- data.frame(
  cook = cooksd,
  oid = 1:length(cooksd)
)

ggplot(data = df)+
  geom_point(aes(x = oid, y = cook), size = 0.5, alpha = 0.5)+
  labs(x = "",
       y = "Distance de Cook")
```

La figure \@ref(fig:poisscookdist) signale la présence de trois observations avec des valeurs extrêmement fortes de distance de Cook. Nous les isolons dans un premier temps pour les analyser.

```{r message=FALSE, warning=FALSE}
cas_etrange <- subset(data_accidents, cooksd>0.1)
print(cas_etrange)
```

Les deux premiers cas sont des intersections avec de nombreux accidents (respectivement 19 et 7) qui risquent de perturber les estimations du modèle. Le troisième cas ne comprend en revanche aucun accident. Puisqu'il ne s'agit que de trois observations et que leurs distances de Cook sont très nettement supérieures aux autres, nous les retirons du modèle.

```{r poisscookdist2, fig.cap = "Distances de Cook pour le modèle de Poisson après avoir retiré les valeurs aberrentes", fig.align = "center", message=FALSE, warning=FALSE, out.width='60%'}
data2 <- subset(data_accidents, cooksd<0.1)

# Ajustement d'une première version du modèle
modele <- glm(Nbr_acci ~ Feux_auto + Feux_piet + Pass_piet + Terreplein + Apaisement +
                LogEmploi + Densite_pop + Entropie + DensiteInter +
                Long_arterePS + Artere + NB_voies5,
              family = poisson(link="log"),
              data = data2)

# Calcul des distances de Cook
cooksd <- cooks.distance(modele)
df <- data.frame(
  cook = cooksd,
  oid = 1:length(cooksd)
)

ggplot(data = df)+
  geom_point(aes(x = oid, y = cook), size = 0.5, alpha = 0.5)+
  labs(x = "",
       y = "distance de Cook")
```

La figure \@ref(fig:poisscookdist2) montre que nous n'avons plus d'observations fortement influentes dans le modèle après avoir retiré les trois observations identifiées précédemment. Nous devons à présent vérifier l'absence de surdispersion.

```{r message=FALSE, warning=FALSE}
# calcul du rapport entre déviance et nombre de degrés de liberté du modèle
deviance(modele)/(nrow(data2) - modele$rank)
```

Le rapport entre la déviance et le nombre de degrés de liberté du modèle est nettement supérieur à 1, indiquant une surdispersion excessive. Nous pouvons confirmer ce résultat avec la fonction `dispersiontest` du *package* **AER**.

```{r message=FALSE, warning=FALSE}
library(AER)
# test de surdispersion
dispersiontest(modele)
```

Contrairement à la forme classique d’un modèle de poisson pour laquelle la dispersion attendue est de 1, le test nous indique qu’une dispersion de 1,89 serait mieux ajustée aux données. 

Il est également possible d'illuster cet écart à l'aide d'un graphique représentant les valeurs réelles, les valeurs prédites, ainsi que la variance (sous forme de barres d'erreurs) attendue par le modèle (figure \@ref(fig:surdisppoiss)). Nous constatons ainsi que les valeurs réelles ont largement tendance à dépasser la variance attendue par le modèle, surtout pour les valeurs les plus faibles de la distribution.

```{r surdisppoiss, message=FALSE, warning=FALSE, fig.aling = "center", fig.cap="Représentation de la surdispersion des données dans le modèle de Poisson", out.width='65%'}

# Extraction des prédictions du modèle
lambdas <- predict(modele, type = "response")

# Création d'un dataframe pour contenir la prédiction et les vraies valeurs
df1 <- data.frame(
  lambdas = lambdas,
  reals = data2$Nbr_acci
)

# Calcul de l'intervalle de confiance à 95% selon la distribution de Poisson
# et stockage dans un second dataframe
seqa <- seq(0,round(max(lambdas)),1)
df2 <- data.frame(
  lambdas = seqa,
  lower = qpois(p = 0.025, lambda = seqa),
  upper = qpois(p = 0.975, lambda = seqa)
)

# Affichage des valeurs réelles et prédites (en rouge)
# et de leur variance selon le modèle (en noir)
ggplot() + 
  geom_errorbar(data = df2,
                mapping = aes(x = lambdas, ymin = lower, ymax = upper),
                width = 0.2, color = rgb(0.4,0.4,0.4)) + 
  geom_point(data = df1, 
             mapping = aes(x = lambdas, y = reals),
             color ="red", size = 0.5) + 
  labs(x = 'valeurs prédites',
       y = "valeurs réelles")

```

Pour tenir compte de cette particularité des données, nous modifions légèrement le modèle pour utiliser une distribution de quasi-Poisson, intégrant spécifiquement un paramètre de dispersion. Cet ajustement ne modifie pas l'estimation des coefficients du modèle, mais modifie le calcul des erreurs standards et par extension les valeurs de *p* pour les rendre moins sensibles au problème de surdispersion. Une autre approche aurait été de calculer une version robuste des erreurs standards avec le *package* **sandwich** comme nous l'avons fait dans la section \@ref(sect06211) sur le modèle binomial. Après réajustement du modèle, le nouveau paramètre de dispersion estimé est de 1,92.


::: {.bloc_notes  data-latex=""}

**Les quasi-distributions**

Les distributions binomiale et de Poisson ne disposent chacune que d’un paramètre décrivant à la fois leur variance et leur espérance. Elles manquent donc de flexibilité et échouent parfois à représenter fidèlement des données avec une forte variance. Il existe donc des distributions alternatives, respectivement les distributions quasi-binomiale et quasi-Poisson comprenant chacune un paramètre supplémentaire pour contrôler la variance. Bien que cette solution soit attrayante, il ne faut pas perdre de vue que la sur ou la sous dispersion peuvent être causées par l’absence de certaines variables explicatives, la sur-représentation de zéros, ou encore une séparation parfaite de la variable dépendante causée par une variable indépendante.:::

```{r message=FALSE, warning=FALSE}
modele2 <- glm(Nbr_acci ~ Feux_auto + Feux_piet + Pass_piet + Terreplein + Apaisement +
                LogEmploi + Densite_pop + Entropie + DensiteInter +
                Long_arterePS + Artere + NB_voies5,
              family = quasipoisson(link="log"),
              data = data2)
```

Nous pouvons à présent comparer la distribution originale des données et les simulations issues du modèle. Notez que contrairement à la distribution de Poisson simple, il n'existe pas dans R de fonction pour simuler des valeurs issues d'une distribution de quasi-Poisson. Il est cependant possible d'exploiter sa proximité théorique avec la distribution négative binomiale pour définir notre propre fonction de simulation. La figure \@ref(fig:comppoissdistr) permet de comparer la distribution originale (en gris) et l'intervalle de confiance à 95% des simulations (en rouge). Nous remarquons que le modèle semble capturer efficacement la forme générale de la distribution originale. À titre de comparaison, nous pouvons effectuer le même exercice avec la distribution de Poisson classique (le code n'est pas montré pour éviter les répétitions). La figure \@ref(fig:comppoissdistr2) montre qu'un simple modèle de Poisson est très éloigné de la distribution originale de *Y*.

```{r comppoissdistr, message=FALSE, warning=FALSE, fig.align ="center", fig.cap="Comparaison de la distribution originale et des simulations pour le modèle de quasi-Poisson", out.width = "65%"}

# Définition d'une fonction pour simuler des données quasi-Poisson
rqpois <- function(n, lambda, disp) {
  rnbinom(n = n, mu = lambda, size = lambda/(disp-1))
}

# Extraction des valeurs prédites par le modèle
preds <- predict(modele2, type="response")

# Génération de 1000 simulations pour chaque prédiction
disp <- summary(modele2)$dispersion
nsim <- 1000
cols <- lapply(1:length(preds),function(i){
  lambda <- preds[[i]]
  sims <- round(rqpois(n = nsim, lambda = lambda, disp = disp))
  return(sims)
})
mat_sims <- do.call(rbind, cols)

# Préparation des données pour le graphique (valeurs réelles)
counts <- data.frame(table(data2$Nbr_acci))
names(counts) <- c("nb_accident",'frequence')
counts$nb_accident <- as.numeric(as.character(counts$nb_accident))
counts$prop <- counts$frequence / sum(counts$frequence)

# Préparation des données pour le graphique (valeurs simulées)
df1 <- data.frame(count = 0:25)

count_sims <- lapply(1:nsim, function(i){
  sim <- mat_sims[,i]
  cnt <- data.frame(table(sim))
  df2 <- merge(df1,cnt, by.x="count", by.y = "sim", all.x = T, all.y=F)
  df2$Freq <- ifelse(is.na(df2$Freq),0,df2$Freq)
  return(df2$Freq)
})

count_sims <- do.call(cbind,count_sims)

df_sims <- data.frame(
  val = 0:25,
  med = apply(count_sims, MARGIN = 1, median),
  lower = apply(count_sims, MARGIN = 1, quantile, probs = 0.025),
  upper = apply(count_sims, MARGIN = 1, quantile, probs = 0.975)
)

ggplot() + 
  geom_bar(aes(x=nb_accident, weight = frequence), width = 0.6, data = counts)+
  geom_errorbar(aes(x = val, ymin = lower, ymax = upper),
                data = df_sims, color = "red", width = 0.6)+
  geom_point(aes(x = val, y = med), color = "red", size = 1.3, data = df_sims)+
  scale_x_continuous(limits = c(-0.5,7), breaks = c(0:7))+
  xlim(-1,12)+
  labs(subtitle = "",
       x = "nombre d'accidents",
       y = "nombre d'occurrences")
```

```{r comppoissdistr2, message=FALSE, warning=FALSE, fig.align ="center", fig.cap="Comparaison de la distribution originale et des simulations pour le modèle de Poisson", echo=FALSE, out.width = "65%"}

# extraction des valeurs prédites par le modèle
preds <- predict(modele2, type="response")

# génération de 1000 simulations pour chaque prédiction
disp <- 1.918757 # trouvable dans le summary(modele2)
nsim <- 1000
cols <- lapply(1:length(preds),function(i){
  lambda <- preds[[i]]
  sims <- round(rpois(n=nsim, lambda = lambda))
  return(sims)
})
mat_sims <- do.call(rbind, cols)

# préparation des données pour le graphique (valeurs réelles)
counts <- data.frame(table(data2$Nbr_acci))
names(counts) <- c("nb_accident",'frequence')
counts$nb_accident <- as.numeric(as.character(counts$nb_accident))
counts$prop <- counts$frequence / sum(counts$frequence)

# préparation des données pour le graphique (valeurs simulées)
df1 <- data.frame(count = 0:25)

count_sims <- lapply(1:nsim, function(i){
  sim <- mat_sims[,i]
  cnt <- data.frame(table(sim))
  df2 <- merge(df1,cnt, by.x="count", by.y = "sim", all.x = T, all.y=F)
  df2$Freq <- ifelse(is.na(df2$Freq),0,df2$Freq)
  return(df2$Freq)
})

count_sims <- do.call(cbind,count_sims)

df_sims <- data.frame(
  val = 0:25,
  med = apply(count_sims, MARGIN = 1, median),
  lower = apply(count_sims, MARGIN = 1, quantile, probs = 0.025),
  upper = apply(count_sims, MARGIN = 1, quantile, probs = 0.975)
)

ggplot() + 
  geom_bar(aes(x=nb_accident, weight = frequence), width = 0.6, data = counts)+
  geom_errorbar(aes(x = val, ymin = lower, ymax = upper),
                data = df_sims, color = "blue", width = 0.6)+
  geom_point(aes(x = val, y = med), color = "blue", size = 1.3, data = df_sims)+
  scale_x_continuous(limits = c(-0.5,7), breaks = c(0:7))+
  xlim(-1,12)+
  labs(subtitle = "",
       x = "nombre d'accidents",
       y = "nombre d'occurrences")
```

La prochaine étape du diagnostic est l'analyse des résidus simulés. La figure \@ref(fig:simrespoiss) indique que les résidus du modèle suivent bien une distribution uniforme et qu'aucune valeur aberrante n'est observable.

```{r simrespoiss, message=FALSE, warning=FALSE, fig.align ="center", fig.cap="Analyse globale des résidus simulés pour le modèle de quasi-Poisson", out.width = "75%"}

# Génération de 1000 simulations pour chaque prédiction
disp <- 1.918757 # trouvable dans le summary(modele2)
nsim <- 1000
cols <- lapply(1:length(preds),function(i){
  lambda <- preds[[i]]
  sims <- rqpois(n = nsim, lambda = lambda, disp = disp)
  return(sims)
})
mat_sims <- do.call(rbind, cols)

sim_res <- createDHARMa(simulatedResponse = mat_sims, 
                            observedResponse = data2$Nbr_acci,
                            fittedPredictedResponse = modele2$fitted.values,
                            integerResponse = T)
plot(sim_res)
```

Pour affiner notre diagnostic, nous pouvons également comparer les résidus simulés et chaque variable indépendante. La figure \@ref(fig:simrespoiss2) n'indique aucune relation problématique entre nos variables indépendantes et les résidus.

```{r simrespoiss2, message=FALSE, warning=FALSE, fig.align ="center", fig.cap="Comparaison des résidus simulés et de chaque variable indépendante", out.width = "95%"}
par(mfrow=c(3,4))
vars <- c("Feux_auto", "Feux_piet", "Pass_piet", "Terreplein", "Apaisement",
                "LogEmploi", "Densite_pop", "Entropie", "DensiteInter",
                "Long_arterePS", "Artere", "NB_voies5")

for(v in  vars){
  plotResiduals(sim_res, data2[[v]], main = v)
}
```


Maintenant que l'ensemble des diagnostics a été effectué, nous pouvons passer à la vérification de la qualité d'ajustement.

**Vérifier la qualité d'ajustement**

Pour le calcul des pseudo R^2^, notez qu'il n'existe pas à proprement parler de *loglikelihood* pour les quasi-distributions. Pour contourner ce problème, il est possible d'utiliser le *loglikelihood* d'un simple modèle de Poisson (puisque les coefficients ne changent pas), mais il est important de garder à l'esprit que ces pseudo R^2^ seront d'autant plus faibles que la surdispersion originale était forte.

```{r message=FALSE, warning=FALSE}

modelnull <- glm(Nbr_acci ~ 1,
              family = poisson(link="log"),
              data = data2)

rsqs(loglike.full = logLik(modele),
     loglike.null = logLik(modelnull),
     full.deviance = deviance(modele),
     null.deviance = deviance(modelnull),
     nb.params = modele$rank,
     n = nrow(data2))

```

Le modèle parvient ainsi à expliquer 48% de la déviance totale, il obtient un R^2^ ajusté de McFadden de 0,33 et un R^2^ de Cox et Snell de 0,78.

```{r message=FALSE, warning=FALSE}
# calcul du RMSE
sqrt(mean((predict(modele2, type = "response") - data2$Nbr_acci)**2))

# nombre moyen d'accidents
mean(data2$Nbr_acci)
```

L’erreur quadratique moyenne du modèle est de 1,86, ce que signifie qu’en moyenne le modèle se trompe d’environ deux accidents pour chaque intersection. Cette valeur est relativement élevée si nous la comparons avec le nombre moyen d’accidents, soit 1,5. Ceci s’explique certainement pas le grand nombre de zéros dans la variable Y qui tendent à tirer les prédictions vers le bas.

**Interprétation des résultats**

L'ensemble des coefficients du modèle sont accessibles via la fonction `summary`. Puisque la fonction de lien du modèle est la fonction *log*, il est pertinent de convertir les coefficients avec la fonction `exp` afin de pouvoir les interpréter sur l'échelle originale (nombre d'accidents) plutôt que l'échelle logarithmique (`log(nombre d'accidents)`). N'oubliez pas que ces effets sont multiplicatifs une fois transformés avec la fonction `exp`. Nous pouvons également utiliser les erreurs standards pour calculer des intervalles de confiance à 95% des exponentiels des coefficients. Le tableau \@ref(tab:coeffpoiss) présente l'ensemble des informations pertinentes pour l'interprétation des résultats.

```{r message=FALSE, warning=FALSE}

# Calcul des coefficients en exponentiel et des intervalles de confiance
tableau <- summary(modele2)$coefficients

coeffs <- tableau[,1]
err.std <- tableau[,2]
expcoeff <- exp(coeffs)
exp2.5 <- exp(coeffs - 1.96*err.std)
exp975 <- exp(coeffs - 1.96*err.std)
pvals <- tableau[,4]

tableauComplet <- cbind(coeffs,err.std,expcoeff,exp2.5,exp975,pvals)
# print(tableauComplet)
```

```{r coeffpoiss, message=FALSE, warning=FALSE, echo=FALSE}
tableau <- build_table(modele2, confid = T, sign = T, coef_digits = 2, std_digits = 2)


show_table(tableau[,c(1,2,3,6,9,10,11)], 
      col.names = c("Variable","Coeff.", "exp(Coeff.)", "Val.p", "IC 2,5% exp(Coeff.)", "IC 97,5% exp(Coeff.)", "Sign."),
      caption = 'Résultats du modèle de quasi-Poisson')
```

Parmi les variables décrivant les aménagements de l’intersection, nous constatons que les présences d’un feu de circulation et d’un feu de traversée pour les piétons multiplient le nombre attendu d’accidents à une intersection par 3,0 et 1,39. Par contre, les présences d’un passage piéton, d’un terre-plein ou de mesures d’apaisement n’ont pas d’effets significatifs (valeurs de p > 0,05).
Concernant les variables décrivant l’environnement à proximité des intersections, nous observons que la concentration d’emplois et la densité de population contribuent toutes les deux à augmenter le nombre d’accidents à une intersection, bien que leurs effets soient limités. Enfin, la présence d’une rue à cinq voies à l’intersection augmente le nombre d’accidents attendu à l’intersection de 89%. Nous ne détaillons pas plus loin les résultats, car nous utilisons le même jeu de données dans les prochaines sections.

#### Le modèle négatif binomial {#sect06222}

Dans le cas où une variable de comptage serait marquée par une sur ou sous-dispersion, la distribution de Poisson n’est pas en mesure de capturer efficacement sa variance. Pour contourner ce problème, il est possible d’utiliser la distribution négative binomiale plutôt que la distribution de Poisson. Cette distribution peut être décrite comme une généralisation de la distribution de Poisson : elle inclut un second paramètre $\theta$ contrôlant la dispersion. L’intérêt premier de ce changement de distribution est que l’interprétation des paramètres est la même pour les deux modèles, tout en contrôlant directement l'effet d'une potentielle surdispersion.

```{r nbdentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim NB(\\mu,\\theta)$ \\newline $g(\\mu) = \\beta_0 + \\beta X$ \\newline $g(x) =log(x)$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable de comptage", "Négative binomiale", model_formula , "log", "$\\mu$", "$\\beta_0$, $\\beta$ et $\\theta$", "Absence d’excès de zéros, respect du lien variance-moyenne")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle négatif binomial",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

##### Conditions d'application

Les conditions d'application d'un modèle négatif binomial sont presque les mêmes que celle d'un modèle de Poisson. La seule différence est que la condition d'absence de sur ou sous-dispersion est remplacée par une condition de respect du lien moyenne-variance. En effet, dans un modèle binomial, le paramètre de dispersion $\theta$ est combiné avec $\mu$ pour exprimer la variance de la distribution. Dans le *package* **mgcv** que nous utilisons dans l'exemple, le lien entre $\mu$, $\theta$ et la variance est le suivant : 

\footnotesize
\begin{equation}
variance = \mu + \mu^{\frac{2}{\theta}}
(\#eq:glm17)
\end{equation}
\normalsize

Il s'agit donc d'un modèle hétéroscédastique, sa variance n'est pas fixe, mais fonction de sa propre moyenne. Si la moyenne augmente, la variance augmente (comme pour un modèle de Poisson), et l'intensité de cette augmentation est contrôlée par le paramètre $\theta$. Si cette condition n'est pas respectée, l'analyse des résidus simulés révélera un problème de dispersion.

##### Exemple appliqué dans R

Dans l’exemple précédent avec le modèle de Poisson, nous avions observé une certaine surdispersion que nous avions contournée en utilisant un modèle de quasi-poisson. Dans l’article original, les auteurs avaient opté pour un modèle négatif binomial, ce que proposons de faire ici. Les variables utilisées sont les mêmes que pour le modèle de Poisson. Nous utilisons le *package* **mgcv** et sa fonction `gam` pour ajuster le modèle.

**Vérification des conditions d'application**

Nous avions vu précédemment que nos variables indépendantes n'étaient pas marquées par une multicolinéarité forte. Il n'est pas nécessaire de recalculer les valeurs de VIF puisque nous utilisons les mêmes données. La première étape du diagnostic est donc de calculer les distances de Cook.

```{r cookdistnb, fig.cap = "Distances de Cook pour le modèle négatif binomial", fig.align = "center", message=FALSE, warning=FALSE, out.width='75%'}
library(mgcv)

# chargement des données
data_accidents <- read.csv("data/glm/accident_pietons.csv")

# ajustement d'une première version du modèle
modelnb <- gam(Nbr_acci ~ Feux_auto + Feux_piet + Pass_piet + 
                 Terreplein + Apaisement +
                LogEmploi + Densite_pop + Entropie + DensiteInter +
                Long_arterePS + Artere + NB_voies5,
              family = nb(link="log"),
              data = data_accidents)

# calcul et affichage des distances de Cook
cooksd <- cooks.distance(modelnb)

df <- data.frame(
  cook = cooksd,
  oid = 1:length(cooksd)
)

ggplot(data = df)+
  geom_point(aes(x = oid, y = cook), size = 0.5, color = rgb(0.4,0.4,0.4)) +
  labs(x = "", y = "distance de Cook")+
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank())
```

Nous observons dans la figure \@ref(fig:cookdistnb) que quatre observations se distinguent très nettement des autres. 

```{r message=FALSE, warning=FALSE}
cas_etrange <- subset(data_accidents, cooksd > 0.03)
print(cas_etrange)
```

Il s'agit à nouveau des quatre observations avec un grand nombre d'accidents. Nous décidons de les retirer du jeu de données pour ne pas fausser les résultats concernant l'ensemble des autres intersections. Dans une analyse plus détaillée, il serait judicieux de chercher à comprendre pourquoi ces quatre observations sont particulièrement accidentogènes.

```{r cookdistnb2, fig.cap = "Distances de Cook pour le modèle négatif binomial (après avoir retiré quatre observations fortement influentes)", fig.align = "center", message=FALSE, warning=FALSE, out.width='75%'}
data2 <- subset(data_accidents, cooksd < 0.03)

# Ajustement d'une première version du modèle
modelnb <- gam(Nbr_acci ~ Feux_auto + Feux_piet + Pass_piet + 
                 Terreplein + Apaisement +
                LogEmploi + Densite_pop + Entropie + DensiteInter +
                Long_arterePS + Artere + NB_voies5,
              family = nb(link="log"),
              data = data2)

# Calcul et affichage des distances de Cook
cooksd <- cooks.distance(modelnb)

df <- data.frame(
  cook = cooksd,
  oid = 1:length(cooksd)
)

ggplot(data = df)+
  geom_point(aes(x = oid, y = cook), size = 0.5, color = rgb(0.4,0.4,0.4)) +
  labs(x = "", y = "distance de Cook")+
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank())
```

Après avoir retiré ces quatre observations, les distances de Cook (figure \@ref(fig:cookdistnb)) ne révèlent plus d'observations fortement influentes dans le modèle. La prochaine étape du diagnostic est donc d'analyser les résidus simulés.

```{r diagresnb, fig.cap = "Diagnostic général des résidus simulés pour le modèle négatif binomial", fig.align = "center", message=FALSE, warning=FALSE, out.width='95%'}
# extraction de la valeur de theta
theta <- modelnb$family$getTheta(T)
nsim <- 1000
# extraction des valeurs prédites par le modèle
mus <- predict(modelnb, type = "response")

# calcul des simulations
cols <- lapply(1:length(mus),function(i){
  mu <- mus[[i]]
  sims <- rnbinom(n = nsim, mu = mu, size = theta)
  return(sims)
})
mat_sims <- do.call(rbind, cols)

# calcul des résidus simulés
sim_res <- createDHARMa(simulatedResponse = mat_sims, 
                            observedResponse = data2$Nbr_acci,
                            fittedPredictedResponse = mus,
                            integerResponse = T)
# affichage du diagnostic
plot(sim_res)
```

La figure \@ref(fig:diagresnb) présentant le diagnostic des résidus simulés montre que ces derniers suivent bien une distribution uniforme, aucun problème de dispersion ni de valeurs aberrantes. La figure \@ref(fig:compnbdistr2) permet de comparer la distribution originale de la variable *Y* et les simulations issues du modèle (intervalles de confiance représentés en bleu). On constate que le modèle parvient bien à reproduire la distribution originale, et ce même pour les valeurs les plus à droite de la distribution.

```{r compnbdistr2, message=FALSE, warning=FALSE, fig.align ="center", fig.cap="Comparaison de la distribution originale et des simulations pour le modèle négatif binomial", out.width = "65%"}

# Extraction des valeurs prédites par le modèle
mus <- predict(modelnb, type="response")

# Génération de 1000 simulations pour chaque prédiction
theta <- modelnb$family$getTheta(T)
nsim <- 1000
cols <- lapply(1:length(mus),function(i){
  mu <- mus[[i]]
  sims <- round(rnbinom(n = nsim, mu = mu, size = theta))
  return(sims)
})
mat_sims <- do.call(rbind, cols)

# Préparation des données pour le graphique (valeurs réelles)
counts <- data.frame(table(data2$Nbr_acci))
names(counts) <- c("nb_accident",'frequence')
counts$nb_accident <- as.numeric(as.character(counts$nb_accident))
counts$prop <- counts$frequence / sum(counts$frequence)

# Préparation des données pour le graphique (valeurs simulées)
df1 <- data.frame(count = 0:25)

count_sims <- lapply(1:nsim, function(i){
  sim <- mat_sims[,i]
  cnt <- data.frame(table(sim))
  df2 <- merge(df1,cnt, by.x="count", by.y = "sim", all.x = T, all.y=F)
  df2$Freq <- ifelse(is.na(df2$Freq),0,df2$Freq)
  return(df2$Freq)
})

count_sims <- do.call(cbind,count_sims)

df_sims <- data.frame(
  val = 0:25,
  med = apply(count_sims, MARGIN = 1, median),
  lower = apply(count_sims, MARGIN = 1, quantile, probs = 0.025),
  upper = apply(count_sims, MARGIN = 1, quantile, probs = 0.975)
)

# Affichage du graphique
ggplot() + 
  geom_bar(aes(x=nb_accident, weight = frequence), width = 0.6, data = counts)+
  geom_errorbar(aes(x = val, ymin = lower, ymax = upper),
                data = df_sims, color = "blue", width = 0.6)+
  geom_point(aes(x = val, y = med), color = "blue", size = 1.3, data = df_sims)+
  scale_x_continuous(limits = c(-0.5,7), breaks = c(0:7))+
  xlim(-1,12)+
  labs(subtitle = "",
       x = "nombre d'accidents",
       y = "nombre d'occurrences")
```

À titre de comparaison, nous pouvons à nouveau réaliser le graphique permettant de visualiser si la variance attendue par le modèle est proche de celle effectivement observée dans les données. Nous avions constaté avec ce graphique lorsque nous ajustions un modèle de Poisson que la variance des données était trop grande comparativement à celle attendue par le modèle (figure \@ref:surdisppoiss)).

```{r surdispnb, message=FALSE, warning=FALSE, fig.aling = "center", fig.cap="Représentation de la sur-dispersion des données dans le modèle de Poisson"}

# Extraction des prédictions du modèle
mus <- predict(modelnb, type = "response")

# Création d'un dataframe pour contenir la prédiction et les vraies valeurs
df1 <- data.frame(
  mus = mus,
  reals = data2$Nbr_acci
)

# Calcul de l'intervalle de confiance à 95% selon la distribution de Poisson
# et stockage dans un second dataframe
seqa <- seq(0,round(max(mus)),1)
df2 <- data.frame(
  mus = seqa,
  lower = qnbinom(p = 0.025, mu = seqa, size = theta),
  upper = qnbinom(p = 0.975, mu = seqa, size = theta)
)

# Affichage des valeurs réelles et prédites (en rouge)
# et de leur variance selon le modèle (en noir)
ggplot() + 
  geom_errorbar(data = df2,
                mapping = aes(x = mus, ymin = lower, ymax = upper),
                width = 0.2, color = rgb(0.4,0.4,0.4)) + 
  geom_point(data = df1, 
             mapping = aes(x = mus, y = reals),
             color ="red", size = 0.5) + 
  labs(x = 'valeurs prédites',
       y = "valeurs réelles")
```

Nous pouvons ainsi constater à la figure \@ref(fig::surdispnb) que le modèle négatif binomial autorise une variance bien plus large que le modèle de Poisson et est ainsi mieux ajusté aux données.

**Vérification de la qualité d'ajustement**

```{r message=FALSE, warning=FALSE}
# calcul des pseudo R2
rsqs(loglike.full = logLik(modelnb),
     loglike.null = logLik(modelnull),
     full.deviance = deviance(modelnb),
     null.deviance = modelnb$null.deviance,
     nb.params = modelnb$rank,
     n = nrow(data2))

# calcul du RMSE
sqrt(mean((predict(modelnb, type = "response") - data2$Nbr_acci)**2))
```

Le modèle parvient à expliquer 45% de la déviance, il obtient un R^2^ ajusté de McFadden de 0.14 et un R^2^ de Nagelkerke de 0.42. L’erreur moyenne quadratique de la prédiction est de 1,9, ce qui est identique au modèle de Poisson ajusté précédemment.

**Interprétation des résultats**

Il est possible d'accéder à l'ensemble des coefficients du modèle via la fonction `summary`. À nouveau, les coefficients doivent être convertis avec la fonction exponentielle (du fait de la fonction de lien log) et interprétés comme des effets multiplicatifs. Le tableau \@ref(tab:coeffsnb) présente les coefficients estimés par le modèle. Les résultats sont très similaires à ceux du modèle de quasi-Poisson original. Nous notons cependant que la variable présence d'un feu pour piéton n'est plus significative au seuil 0,05.

```{r coeffsnb, message=FALSE, warning=FALSE, echo=FALSE}
tableau <- build_table(modelnb, confid = T, sign = T, coef_digits = 2, std_digits = 2)

show_table(tableau[,c(1,2,3,6,9,10,11)], 
      col.names = c("Variable","Coeff.", "exp(Coeff.)", "Val.p", "IC 2,5% exp(Coeff.)", "IC 97,5% exp(Coeff.)", "Sign."),
      caption = 'Résultats du modèle négatif binomial')
```

#### Le modèle de Poisson avec excès fixe de zéros {#sect06223}

Dans le cas où la variable *Y* comprendrait significativement plus de zéros que ce que suppose une distribution de Poisson, il est possible d’utiliser la distribution de Poisson avec excès de zéros. Pour rappel, cette distribution ajoute un paramètre *p* contrôlant pour la proportion de zéros dans la distribution. Du point de vue conceptuel, cela revient à formuler l’hypothèse suivante : dans les données que l’on a observées, deux processus distincts sont à l’œuvre. Le premier est issu d’une distribution de Poisson et le second produit des zéros qui s’ajoutent aux données. Les zéros produits par la distribution de Poisson sont appelés les **vrais zéros**, alors que ceux produits par le second phénomène sont appelés les **faux zéros**.

```{r poisszidentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim ZIP(\\mu,\\theta)$ \\newline $g(\\lambda) = \\beta_0 + \\beta X$ \\newline $g(x) =log(x)$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable de comptage", "Poisson avec excès de zéros", model_formula , "log", "$\\lambda$", "$\\beta_0$, $\\beta$ et *p*", "Absence de surdispersion")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle de Poisson avec excès fixe de zéros",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

Dans cette formulation, *p* est fixé. Nous n’avons donc aucune information sur ce qui produit les zéros supplémentaires, mais seulement leur proportion totale dans le jeu de données.

##### Interprétation des paramètres

L’interprétation des paramètres est identique à celle d’un modèle de Poisson. Le paramètre *p* représente la proportion de zéros dans la variable *Y* une fois que sont contrôlées les variables indépendantes.

##### Exemple appliqué dans R

La variable de comptage des accidents des piétons que nous avons utilisée dans les deux exemples précédents semble être une bonne candidate pour une distribution de Poisson avec excès de zéros. En effet, nous avions pu constater une surdispersion dans le modèle de Poisson original, ainsi qu’un nombre important d’intersections sans accident. Tentons donc d’améliorer notre modèle en ajustant un excès fixe de zéros. Nous utilisons la fonction `gamlss` du *package* **gamlss**.


```{r message=FALSE, warning=FALSE}
library(gamlss)

modelzi <- gamlss(formula = Nbr_acci ~ Feux_auto + Feux_piet + Pass_piet +
                    Terreplein + Apaisement + LogEmploi + Densite_pop + 
                    Entropie + DensiteInter + Long_arterePS + Artere + NB_voies5,
                  sigma.formula = ~1,
               family = ZIP(mu.link = "log", sigma.link="logit"),
               data = data_accidents)

modelnull <- glm(formula = Nbr_acci ~ 1,
               family = poisson(link="log"),
               data = data_accidents)

# Constante pour p
coeff_p <- modelzi$sigma.coefficients
cat("Coefficient pour p =", round(coeff_p,4))

# Calcul de la déviance expliquée
1 - deviance(modelzi) / deviance(modelnull)

# Calcul de la probabilité de base p d'être un faux 0
# en appliquant l'inverse de la fonction logistique
exp(-coeff_p) / (1+exp(-coeff_p))
```

Nous constatons immédiatement que le modèle avec excès fixe de zéros est peu ajusté aux données. Cette version du modèle ne parvient à capter que 8% de la déviance, ce qui s’explique facilement, car nous n’avons donné aucune variable au modèle pour distinguer les vrais et les faux zéros. Pour cela, nous devons passer au prochain modèle : Poisson avec excès ajusté de zéros. Notons tout de même que d'après ce modèle, 19% des observations seraient des faux zéros.

#### Le modèle de poisson avec excès ajusté de zéros {#sect06224}

Nous avons vu dans le modèle précédent que l’excès de zéro était conceptualisé comme la combinaison de deux phénomènes, l’un issu d’une distribution de Poisson que l’on souhaite modéliser, et le second générant des zéros supplémentaires. Il est possible d’aller plus loin que de simplement contrôler la proportion de zéro supplémentaire en modélisant explicitement ce second processus en ajoutant une deuxième équation au modèle. Cette deuxième équation a pour enjeu de modéliser *p* (la proportion de 0) à l’aide de variables indépendantes, ces dernières pouvant se retrouver dans les deux parties du modèle. **Pour résumer, un modèle de Poisson avec excès ajusté de zéro revient à combiner un modèle logistique binomial avec un modèle de Poisson**. La partie binaire du modèle s’occupe des faux zéros, et la partie Poisson du reste des données. L’idée étant que pour chaque observation, le modèle évalue sa probabilité d’être un faux zéro (partie binomiale), avant d’évaluer son impact dans le modèle de Poisson.

```{r poisszadentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim ZIP(\\mu,\\theta)$ \\newline $g(\\lambda) = \\beta_0 + \\beta X$ \\newline $s(p) = \\alpha_0 + \\alpha_X$ \\newline $g(x) =log(x)$ \\newline $s(x) = log(\\frac{x}{1-x})$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable de comptage", "Poisson avec excès de zéros", model_formula , "log et logistique", "$\\lambda$ et *p*", "$\\beta_0$, $\\beta$,  $\\alpha_0$ et $\\alpha$", "Absence de surdispersion")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle de Poisson avec excès ajusté de zéros",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

##### Interprétation des paramètres

L’interprétation des paramètres $\beta_0$ et $\beta$ est identique à celle d’un modèle de Poisson. Les paramètres $\alpha_0$ et $\alpha$ sont identiques à ceux d’un modèle binomial. Plus spécifiquement, ces derniers paramètres modélisent la probabilité d’observer des valeurs supérieures à 0.

##### Exemple appliqué

Nous avions vu dans l’exemple précédent que l’utilisation du modèle avec excès fixe de zéros pour les données d’accident des piétons aux intersections ne donnait pas de résultats satisfaisants. Nous tentons ici d’améliorer le modèle en ajoutant les prédicteurs significatifs du modèle de Poisson dans la seconde équation de régression destinée à détecter les zéros.

**Vérification des conditions d'application**

Pour un modèle de Poisson avec excès de 0, il n'est pas possible de calculer de distances de Cook. Nous devons donc directment passer à l'analyse des résidus simulés

```{r diagzip, fig.cap="Diagnostic général des résidus simulés du modèle de Poisson avec excès de zéros ajusté", fig.align='center', out.width = '95%',message=FALSE, warning=FALSE}
# Ajuster une première version du modèle
modelza <- gamlss(formula = Nbr_acci ~ Feux_auto + Feux_piet + Pass_piet +
                    Terreplein + Apaisement + LogEmploi + Densite_pop + 
                    Entropie + DensiteInter + Long_arterePS + Artere + NB_voies5,
                  sigma.formula = ~1 + Feux_auto + Feux_piet + Densite_pop + NB_voies5,
               family = ZIP(mu.link = "log", sigma.link="logit"),
               data = data_accidents)

# Extraire la prédiction des valeurs lambda
lambdas <- predict(modelza, type = "response", what = "mu")

# Extraire la prédiction des valeurs p
ps <- predict(modelza, type = "response", what = "sigma")

# Calculer la combinaison de ces deux éléments

preds <- lambdas * ps

# Effectuer les 1000 simulations
nsim <- 1000
cols <- lapply(1:length(lambdas),function(i){
  lambda <- lambdas[[i]]
  p <- ps[[i]]
  sims <- rZIP(n = nsim, mu = lambda, sigma = p)
  return(sims)
})
mat_sims <- do.call(rbind, cols)

# Calculer les résidus simulés
sim_res <- createDHARMa(simulatedResponse = mat_sims, 
                            observedResponse = data_accidents$Nbr_acci,
                            fittedPredictedResponse = preds,
                            integerResponse = T)
plot(sim_res)
```

La figure \@ref(fig:diagzip) indique deux problèmes importants dans le modèle : la présence de valeurs aberrantes ainsi qu'un potentiel problème de dispersion. Nous commençons donc par identifier ces valeurs aberrantes.

```{r, warning=FALSE, message=FALSE}
# Identification des outliers
isOutlier <- outliers(sim_res,return = "logical",lowerQuantile = 0.001,
                      upperQuantile = 0.999)
cas_etrange <- subset(data_accidents, isOutlier)
print(cas_etrange)
```

Nous retirons des données six observations pouvant avoir une certaine influence sur le modèle. Après réajustement, la figure \@ref(fig:diagzip2) nous informe que nous n'avons plus de valeurs aberrantes restantes ni de fort problème de dispersion. En revanche, le premier quantile des résidus tant à être plus faible que ce que l'on pourrait attendre d'une distribution uniforme. Ce constat laisse penser que le modèle a du mal à bien identifier les faux zéros. Ce résultat n'est pas étonnant, car aucune variable n'avait été identifiée à cette fin dans l'article original [@cloutier2014carrefours] qui utilisait un modèle négatif binomial.

```{r diagzip2, fig.cap="Diagnostic général des résidus simulés du modèle de Poisson avec excès de zéros ajusté (sans valeurs aberrantes)", fig.align='center', out.width = '95%',message=FALSE, warning=FALSE}
data2 <- subset(data_accidents, isOutlier==FALSE)

# Ajuster une première version du modèle
modelza <- gamlss(formula = Nbr_acci ~ Feux_auto + Feux_piet + Pass_piet +
                    Terreplein + Apaisement + LogEmploi + Densite_pop + 
                    Entropie + DensiteInter + Long_arterePS + Artere + NB_voies5,
                  sigma.formula = ~1 + Feux_auto + Feux_piet + Densite_pop + NB_voies5,
               family = ZIP(mu.link = "log", sigma.link="logit"),
               data = data2)

# Extraire la prédiction des valeurs lambda
lambdas <- predict(modelza, type = "response", what = "mu")

# Extraire la prédiction des valeurs p
ps <- predict(modelza, type = "response", what = "sigma")

# Calculer la combinaison de ces deux éléments
preds <- lambdas * ps

# Effectuer les 1000 simulations
nsim <- 1000
cols <- lapply(1:length(lambdas),function(i){
  lambda <- lambdas[[i]]
  p <- ps[[i]]
  sims <- rZIP(n = nsim, mu = lambda, sigma = p)
  return(sims)
})
mat_sims <- do.call(rbind, cols)

# Calculer les résidus simulés
sim_res <- createDHARMa(simulatedResponse = mat_sims, 
                            observedResponse = data2$Nbr_acci,
                            fittedPredictedResponse = preds,
                            integerResponse = T)
plot(sim_res)
```

Nous pouvons une fois encore comparer des simulations issues du modèle et la distribution originale de la variable *Y*. La figure \@ref(fig:compzipdistr) montre clairement que les simulations du modèle (en bleu) sont très éloignées dans la distribution originale (en gris), ce qui remet directement en question la pertinence de ce modèle.

```{r compzipdistr, message=FALSE, warning=FALSE, fig.align ="center", fig.cap="Comparaison de la distribution originale et des simulations pour le modèle de Poisson avec excès de zéros ajusté", out.width = "65%"}

# extraire la prédiction des valeurs lambda
lambdas <- predict(modelza, type = "response", what = "mu")

# extraire la prédiction des valeurs p
ps <- predict(modelza, type = "response", what = "sigma")

# génération de 1000 simulations pour chaque prédiction
nsim <- 1000
cols <- lapply(1:length(lambdas),function(i){
  lambda <- lambdas[[i]]
  p <- ps[[1]]
  sims <- round(rZIP(nsim,mu=lambda, sigma = p))
  return(sims)
})
mat_sims <- do.call(rbind, cols)

# préparation des données pour le graphique (valeurs réelles)
counts <- data.frame(table(data2$Nbr_acci))
names(counts) <- c("nb_accident",'frequence')
counts$nb_accident <- as.numeric(as.character(counts$nb_accident))
counts$prop <- counts$frequence / sum(counts$frequence)

# préparation des données pour le graphique (valeurs simulées)
df1 <- data.frame(count = 0:25)

count_sims <- lapply(1:nsim, function(i){
  sim <- mat_sims[,i]
  cnt <- data.frame(table(sim))
  df2 <- merge(df1,cnt, by.x="count", by.y = "sim", all.x = T, all.y=F)
  df2$Freq <- ifelse(is.na(df2$Freq),0,df2$Freq)
  return(df2$Freq)
})

count_sims <- do.call(cbind,count_sims)

df_sims <- data.frame(
  val = 0:25,
  med = apply(count_sims, MARGIN = 1, median),
  lower = apply(count_sims, MARGIN = 1, quantile, probs = 0.025),
  upper = apply(count_sims, MARGIN = 1, quantile, probs = 0.975)
)

# affichage du graphique
ggplot() + 
  geom_bar(aes(x=nb_accident, weight = frequence), width = 0.6, data = counts)+
  geom_errorbar(aes(x = val, ymin = lower, ymax = upper),
                data = df_sims, color = "blue", width = 0.6)+
  geom_point(aes(x = val, y = med), color = "blue", size = 1.3, data = df_sims)+
  scale_x_continuous(limits = c(-0.5,7), breaks = c(0:7))+
  xlim(-1,12)+
  labs(subtitle = "",
       x = "nombre d'accidents",
       y = "nombre d'occurrences")
```

**Vérifier la qualité d'ajustement**

```{r message=FALSE, warning=FALSE}
modelenull <- glm(Nbr_acci ~ 1,
               family = poisson(link="log"),
               data = data2)

# calcul des R2
rsqs(loglike.full = logLik(modelza),
     loglike.null = logLik(modelenull),
     full.deviance = deviance(modelza),
     null.deviance = deviance(modelenull),
     nb.params = modelza$sigma.df + modelza$mu.df,
     n = nrow(data2)
     )

# calcul du RMSE
sqrt(mean((preds - data2$Nbr_acci)**2))
```

Le modèle avec excès de zéro ajusté ne parvient à expliquer que 11% de la déviance totale. Il obtient cependant des valeurs de R^2^ assez hautes (McFadden ajusté : 0,36, Nagerlkerke : 0,82). Son RMSE est cependant très élevé (2,6), comparativement à celui que nous avions obtenu avec le modèle négatif binomial (1,9). Considérant ces éléments, ce modèle est nettement moins informatif que le modèle négatif binomial et ne devrait pas être retenu. Nous allons cependant montrer ici comment interpréter ces résultats.

**Interprétation des résultats**

L'ensemble des coefficients du modèle sont accessibles avec la fonction `summary`. Les coefficients dédiés à la partie Poisson (appelée **Mu** dans le résumé) doivent être analysés et interprétés de la même manière que s'ils provenaient d'un modèle de Poisson. Les coefficients appartenant à la partie logistique (appelé **Sigma** dans le résumé) doivent être analysés et interprétés de la même manière que s'ils provenaient d'un modèle logistique.

```{r message=FALSE, warning=FALSE}
# Extraction des résultats
base_table <- summary(modelza)

# Multiplication par 1000 des coefficients de population
# (effet pour 1000 habitants)
base_table[8,1] <- 1000 * base_table[8,1]
base_table[8,2] <- 1000 * base_table[8,2]
base_table[17,1] <- 1000 * base_table[17,1]
base_table[17,2] <- 1000 * base_table[17,2]

# Multiplication par 1000 des coefficients de longueur artère
# (effet pour 1 km)
base_table[11,1] <- 1000 * base_table[11,1]
base_table[11,2] <- 1000 * base_table[11,2]

# Calcul des exponentiels des prédicteurs
# et des intervalles de confiance
expcoeff <- exp(base_table[,1])
expcoeff2.5 <- exp(base_table[,1] - 1.96 * base_table[,2])
expcoeff97.5 <- exp(base_table[,1] + 1.96 * base_table[,2])

base_table <- cbind(base_table, expcoeff, expcoeff2.5,expcoeff97.5)

# Calculer une colonne indiquant le niveau de significativité
sign <- case_when(
  base_table[,4] < 0.001 ~ "***",
  base_table[,4]  >= 0.001 &  base_table[,4]<0.01 ~ "**",
  base_table[,4]  >= 0.01 &  base_table[,4]<0.05 ~ "*",
  base_table[,4]  >= 0.05 &  base_table[,4]<0.1 ~ ".",
  TRUE ~ ""
)

# Arrondir à trois décimales
base_table <- round(base_table,3)

# Enlever les colonnes de valeurs de t et d'erreur standard
base_table <- base_table[,c(1,4,5,6,7)]
base_table <- cbind(base_table, sign)

# Remplacer les 0 dans la colonne pval
base_table[,2] <- ifelse(base_table[,2]=="0","<0.001",base_table[,2])

# Séparer le tout en deux tableaux
part_poiss <- base_table[1:13,]
part_logit <- base_table[14:18,]

# Mettre les bons noms de colonnes
colnames(part_poiss) <- c("Coeff.","Val.p","Exp(Coeff.)",
                          "IC 2,5% exp(Coeff.)","IC 97,5% exp(Coeff.)", "Sign.")

colnames(part_logit) <- c("Coeff.","Val.p","RC","IC 2,5% RC","IC 97,5% RC", "Sign.")

```

Nous rapportons les résultats de ce modèle de Poisson avec excès de zéro ajusté dans les tableaux \@ref(tab:zapoisstab1) et \@ref(tab:zapoisstab2).

```{r zapoisstab1, message=FALSE, warning=FALSE, echo = FALSE}
part_poiss <- cbind(rownames(part_poiss),part_poiss)
rownames(part_poiss) <- NULL
show_table(part_poiss, 
    caption = "Résultats de la partie Poisson du modèle de Poisson avec excès de zéros ajusté",
    col.names = c('Variable',"Coeff.","Val.p","Exp(Coeff.)","IC 2,5% exp(Coeff.)","IC 97,5% exp(Coeff.)", "Sign.")
)

```

```{r zapoisstab2, message=FALSE, warning=FALSE, echo = FALSE}
part_logit <- cbind(rownames(part_logit),part_logit)
rownames(part_logit) <- NULL
show_table(part_logit, 
    caption = "Résultats de la partie logistique du modèle de Poisson avec excès de zéros ajusté",
    col.names = c('Variable',"Coeff.","Val.p","RC","IC 2,5% RC","IC 97,5% RC", "Sign.")
)

```

Nous observons ainsi que la présence d’un feu de circulation divise les chances de ne pas observer d'accident à une intersection par 5. La densité de population a également un effet positif sur la probabilité d’observer des accidents à une intersection. Pour chaque tranche de 1000 habitants supplémentaire à proximité de l'intersection, les chances de ne pas observer d'accident sont réduites de 11%.

Concernant les coefficients pour la partie Poisson du modèle, nous observons qu’à nouveau, les présences d’un feu de circulation et d’un feu pour piéton contribuent à multiplier respectivement par 2 et 1,5 le nombre attendu d’accidents à une intersection. De même, la présence d’un axe de circulation à cinq voies augmente de 57% le nombre d’accidents. Enfin, la densité de population est aussi associée à une augmentation du nombre d’accidents : pour 1000 habitants supplémentaires autour de l'intersection, on augmente le nombre d'accidents attendu de 9%.

#### Conclusion sur les modèles destinés à des variables de comptage {#sect06225}

Dans cette section, nous avons vu que modéliser une variable de comptage ne doit pas toujours être réalisé avec une simple distribution de Poisson. Il est nécessaire de tenir compte de la sur ou sous-dispersion potentielle ainsi que de l’excès de zéros. Nous n’avons cependant pas couvert tous les cas, il est en effet possible d’ajuster des modèles avec une distribution négative binomiale avec excès de zéros (avec le *package* **gamlss**), ainsi que des modèles de **Hurdle**. Ces derniers ont une approche différente de celle proposée par les distributions ajustées pour tenir compte de l’excès de zéro que nous détaillons dans l’encadré pour aller plus loin ci-dessous. Le processus de sélection du modèle peut être résumé avec la figure \@ref(fig:figpoisswork). Notez que même en suivant cette procédure, rien ne garantit que votre modèle final reflète bien les données que vous étudiez. L’analyse approfondie des résidus et des prédictions du modèle est la seule façon de déterminer si oui ou non le modèle est fiable.

```{r figpoisswork, fig.align='center', echo=FALSE, auto_pdf = TRUE, fig.cap="Processus de sélection d'un modèle pour une variable de comptage",  out.width='90%'}
knitr::include_graphics('images/glm/poisson_workflow.png', dpi = NA)
```

::: {.bloc_aller_loin data-latex=""}
**Modèle de Hurdle *versus* modèle avec excès de zéro**

Les modèle de Hurdle sont une autre catégorie de modèle GLM. Ils peuvent être décrit avec la formulation suivante : 

\footnotesize
\begin{equation}\left\{\begin{array}{c}
Y \sim \text {Binomial}(p) \text { si } y=0 \\
\text { logit }(p)=\beta_{a} X_{a} \\
Y \sim \text { TrPoisson}(\lambda) \text { si } y>0 \\
\log (\lambda)=\beta_{b} X_{b}
\end{array}\right.
(\#eq:glm17)
\end{equation}
\normalsize

On constate qu’un modèle de Hurdle utilise deux distributions, la première est une distribution binomiale dont l’objectif est de prédire si les observations sont à 0 ou au-dessus de 0. La seconde est une distribution strictement positive (supérieure à 0), il peut s’agir d’une distribution tronquée de Poisson, tronquée négative binomiale, Gamma, log-normale ou autre, dépendamment du phénomène modélisé. Puisque le modèle fait appel à deux distributions, deux équations de régressions sont utilisées, l’une pour prédire *p* (la probabilité d’observer une valeur au-dessus de 0) et l'autre l’espérance (moyenne) de la seconde distribution. 

En d’autres termes, un modèle de Hurdle modélise les données à zéro et les données au-delà de 0 comme deux processus différents (chacun avec sa propre distribution). Cette approche se distingue des modèles avec excès de zéros qui utilisent une seule distribution pour décrire l’ensemble des données. D’après un modèle avec excès de zéro, il existe des vrais et des faux zéros que l’on tente de distinguer. Dans un modèle de Hurdle, l’idée est que les zéros constituent une limite. On modélise la probabilité de dépasser cette limite, et ensuite la magnitude du dépassement de cette limite.

Prenons un exemple pour rendre la distinction plus concrète. Admettons que nous utilisions un capteur capable de mesurer la concentration de particules fines dans l’air. D’après les spécifications du fabricant, le capteur est capable de mesurer des taux de concentration à partir de 0,001 µg/m^3^. Dans une ville avec des niveaux de concentration très faibles, il sera très fréquent que le capteur enregistre des valeurs à zéro. Considérant ce phénomène, il serait judicieux de modéliser le processus avec un modèle de Hurdle Gamma puisque les 0 représentent une limite qui n’a pas été franchie : le seuil de détection du capteur. On va donc traiter différemment les secteurs au-dessous et au-dessus de ce seuil. Si l’on reprend notre exemple sur les accidents des piétons à des intersections, il est plus judicieux dans ce cas de modéliser le phénomène avec un modèle avec excès de zéro puisque l’on peut observer zéro accident à une intersection dangereuse (vrai zéro), et zéro accident à une intersection sur laquelle aucun piéton ne traverse jamais (faux zéro).
:::

### Les modèles GLM pour des variables continues {#sect0623}

Comme nous avons pu le voir dans la section \@ref(sect024), il existe un grand nombre de distributions permettant de décrire une grande diversité de variables continues. Il serait fastidieux de tous les présenter, nous allons seulement revenir sur les plus fréquents.

#### Le modèle GLM gaussien {#sect06231}

Comme nous l'avons vu en introduction, il s'agit du GLM le plus simple puisqu'il correspond à la transposition de la régression linéaire classique (des moindres carrés) dans la forme des modèles généralisés.

```{r gaussiandentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim Normal(\\mu,\\sigma)$ \\newline $g(\\mu) = \\beta_0 + \\beta X$ \\newline $g(x) = x$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable continue dans l'intervalle $]-\\infty ; + \\infty[$", "Normale", model_formula , "identitaire", "$\\mu$", "$\\beta_0$, $\\beta$ et $\\sigma$", "Homoscédasticité")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle gaussien",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

##### Conditions d'application

Les conditions d'application sont les mêmes que celles pour une régression linéaire classique. La condition de l'homoscédasticité (homogénéité de la variance), est due au fait que la variance du modèle est contrôlée par un seul paramètre fixe $var(y) = \sigma$ (l'écart type de la distribution normale). À titre de comparaison, rappelons que dans un modèle de Poisson, la variance est égale à la moyenne ($var(y) = E(y)$) alors que dans un modèle négatif binomial, la variance est fonction de la moyenne et d'un paramètre $\theta$ ($var(y) = E(y) + E(y)^{\frac{2}{\theta}}$). Pour ces deux exemples, la variance augmente au fur et à mesure que la moyenne augmente.

##### Interprétation des paramètres

L'interprétation des paramètres est la même que pour une régression linéaire classique : 

* $\beta_0$ : la constante, donc de la moyenne attendue de la variable *Y* lorsque les valeurs de toutes les variables *X* sont 0.
* $\beta$ : les coefficients de régressions qui quantifient l’impact d’une augmentation d’une unité des variables *X* sur la moyenne de la variable *Y*.
* $\sigma$ : l’écart type de *Y* après avoir contrôlé les variables *X*. Il peut s’interpréter comme l’incertitude restante après modélisation de la moyenne de *Y*. Concrètement, si vous utilisez votre équation de régression pour prédire une nouvelle valeur de *Y* : $\hat{Y}$, l’intervalle de confiance à 95% de cette prédiction est ($\hat{Y} - 3\sigma\text{ ; }\hat{Y} + 3\sigma$). Vous noterez donc que plus $\sigma$ est grand, plus grande est l'incertitude de la prédiction.

##### Exemple appliqué dans R

Pour cet exemple, nous reprenons le modèle LM que nous avions présenté dans la section \@ref(sect0583). À titre de rappel, l'objectif est de modéliser la densité végétale dans les secteurs de recensement de Montréal. Pour cela, nous utilisons des variables relatives aux populations vulnérables physiologiquement ou socioéconomiquement, tout en contrôlant l'impact de la forme urbaine. Parmi ces dernières, l'âge médian des bâtiments est ajouté au modèle avec une polynomiale d'ordre deux, et la densité d'habitants est transformée avec la fonction logarithmique.

**Vérification des conditions d'application**

La première étape de la vérification des conditions d'application est bien sûr la vérification de l'absence de multicolinéarité excessive. 

```{r message=FALSE, warning=FALSE}
# Chargement des données
load("data/lm/DataVegetation.RData")

# Calcul du VIF
library(car)
vif(glm(VegPct ~ log(HABHA)+poly(AgeMedian,2)+
                       Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal))
```

Puisque l'ensemble des valeurs de VIF sont inférieures à deux, nos données ne sont pas marquées par une multicolinéarité problématique. La seconde étape du diagnostic consiste à  calculer et afficher les distances de Cook.

```{r gausscook, message=FALSE, warning=FALSE, fig.align = "center", fig.cap = "Distances de Cook pour le modèle gaussien", out.width = "50%"}
# Ajustement du modèle
modele <- glm(VegPct ~ log(HABHA)+poly(AgeMedian,2)+
                       Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal)

# Calcul des distances de Cook
cooksd <- cooks.distance(modele)

# Affichage des valeurs
df <- data.frame(
  cook = cooksd,
  oid = 1:length(cooksd)
)

ggplot(data = df) + 
  geom_point(aes(x = oid, y = cook), color = rgb(0.4,0.4,0.4,0.7), size = 1) + 
  labs(x="", y = "distance de Cook")

```

La figure \@ref(fig:gausscook) indique clairement que quatre observations sont très influentes dans le modèle. 

```{r message=FALSE, warning=FALSE}
# Sélection des cas étranges
cas_etranges <- subset(DataFinal, cooksd > 0.03)
print(cas_etranges)
```

Il s'agit de quatre îlots dans Montréal avec des logements très anciens : plus de 200 ans, alors que la moyenne est de 52 ans pour le reste de la zone d'étude. Le fait que nous ayons dans le modèle une polynomiale d'ordre 2 pour cette variable intensifie l'impact de ces valeurs extrêmes. Par conséquent, nous décidons de simplement les supprimer. Nous verrons plus tard qu'une alternative envisageable est de changer la distribution du modèle pour une distribution de Student (plus robuste aux valeurs extrêmes).

```{r gausscook2, message=FALSE, warning=FALSE, fig.align = "center", fig.cap = "Distances de Cook pour le modèle gaussien après suppression des observations influentes", out.width = "50%"}

# Retirer les cas étranges
DataFinal2 <- subset(DataFinal, cooksd < 0.03)

# Ajustement du modèle
modele <- glm(VegPct ~ log(HABHA)+poly(AgeMedian,2)+
                       Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal2, 
              family = gaussian())

# Calcul des distances de Cook
cooksd <- cooks.distance(modele)

# Affichage des valeurs de Cook
df <- data.frame(
  cook = cooksd,
  oid = 1:length(cooksd)
)

ggplot(data = df) + 
  geom_point(aes(x = oid, y = cook), color = rgb(0.4,0.4,0.4,0.7), size = 1) + 
  labs(x="", y = "distance de Cook")

```

Une fois ces observations retirées, les nouvelles distances de Cook (figure \@ref(fig:gausscook2)) ne révèlent plus d'observations fortement influentes. Nous pouvons passer à l'analyse des résidus simulés. La figure \@ref(fig:gaussresid) démontre que la ditribution des résidus est significativement différente d'une distribution uniforme, que des valeurs aberrantes sont encore présentes et qu'il existe un lien entre résidus et prédiction dans le modèle.

```{r gaussresid, message=FALSE, warning=FALSE, fig.align = "center", fig.cap = "Diagnostic général des résidus simulés pour le modèle gaussien", out.width = "95%"}

# Extraction des prédictions du modèle
mus <- predict(modele, type = 'response')
modsigma <- sigma(modele)

# Extraction de l'écart type du modèle

# Génération de 1000 simulations pour chaque prédiction
nsim <- 1000
cols <- lapply(1:length(mus),function(i){
  mu <- mus[[i]]
  sims <- rnorm(nsim, mean=mu, sd = modsigma)
  return(sims)
})
mat_sims <- do.call(rbind, cols)

# Calculer les résidus simulés
sim_res <- createDHARMa(simulatedResponse = mat_sims, 
                            observedResponse = DataFinal2$VegPct,
                            fittedPredictedResponse = mus,
                            integerResponse = F)
plot(sim_res)

```

Pour mieux cerner ce problème, nous pouvons dans un premier temps comparer la distribution originale des données et les simulations issues du modèle. La figure \@ref(fig:gausssim) montre clairement que la distribution normale est mal ajustée aux données. Ces dernières sont légèrement asymétriques et ne peuvent pas être inférieures à zéro, ce que la distribution normale ne parvient pas à reproduire.

```{r gausssim, message=FALSE, warning=FALSE, fig.align = "center", fig.cap = "Comparaison de la distribution originale de la variable et des simulations issues du modèle", out.width = "75%"}

df <- reshape2::melt(mat_sims[,1:30])

ggplot() + 
  geom_histogram(data = DataFinal2, mapping = aes(x = VegPct, y = ..density..),
                 color = "black", fill = "white", bins = 50)+
  geom_density(data = df, aes(x = value, group = Var2), 
               color = rgb(0.4,0.4,0.4,0.4), fill = rgb(0,0,0,0))+
  labs(x = "Pourcentage de végétation dans l'îlot (%)",
  y = "Densité")
```

Il est également possible de vérifier si la condition d'homogénéité de la variance s'applique bien aux données.

```{r gausssim2, message=FALSE, warning=FALSE, fig.align = "center", fig.cap = "Comparaison de la distribution originale de la variable et des simulations issues du modèle", out.width = "75%"}

# Extraction des prédictions du modèle
mus <- predict(modele, type = "response")
sigma_model <- sigma(modele)

# Création d'un dataframe pour contenir les prédictions et les vraies valeurs
df1 <- data.frame(
  mus = mus,
  reals = DataFinal2$VegPct
)

# Calcul de l'intervalle de confiance à 95% selon la distribution normale
# et stockage dans un second dataframe
seqa <- seq(0,100,10)
df2 <- data.frame(
  mus = seqa,
  lower = qnorm(p = 0.025, mean = seqa, sd = sigma_model),
  upper = qnorm(p = 0.975, mean = seqa, sd = sigma_model)
)

# Affichage des valeurs réelles et prédites (en rouge)
# et de leur variance selon le modèle (en noir)
ggplot() + 
  geom_point(data = df1, 
             mapping = aes(x = mus, y = reals),
             color ="red", size = 0.5) + 
  geom_errorbar(data = df2,
                mapping = aes(x = mus, ymin = lower, ymax = upper),
                width = 0.2, color = rgb(0.4,0.4,0.4)) + 
  labs(x = 'valeurs prédites',
       y = "valeurs réelles")

```

À nouveau, on constate que le modèle s'attendrait à trouver des valeurs négatives pour la concentration de végétation, ce qui n'est pas possible dans notre cas. En revanche, il semblerait que la variance soit bien homogène puisque la dispersion des observations semble à peu près suivre la dispersion attendue par le modèle (en noir).

Malgré ces différents constats indiquant clairement qu'un modèle gaussien est un choix sous-optimal pour ces données, nous allons poursuivre l'analyse de ce modèle.

**Vérification de la qualité d'ajustement**

```{r warning = FALSE, message=FALSE}
# Ajustement d'un modèle nul
modelenull <- glm(VegPct ~ 1,
                  data = DataFinal2, 
                  family = gaussian())

# Calcul des pseudo R2
rsqs(loglike.full = logLik(modele),
     loglike.null = logLik(modelenull),
     full.deviance = deviance(modele),
     null.deviance = deviance(modelenull), 
     nb.params = modele$rank,
     n = nrow(DataFinal2)
     )
```

Le modèle parvient à expliquer 47% de la déviance totale, mais obtient un R^2^ ajusté de McFadden de seulement 0,07.

```{r warning = FALSE, message=FALSE}
# calcul du RMSE
sqrt(mean((predict(modele, type = "response") - DataFinal2$VegPct)**2))

```

L'erreur quadratique moyenne et de 13,5 points de pourcentage, ce qui indique que le modèle a une assez faible capacité prédictive.

**Interprétation des résultats**

L'ensemble des coefficients du modèle sont accessibles via la fonction `summary`; le tableau \@ref(tab:tableaugauss) présente les résultats pour les coefficients du modèle.

```{r tableaugauss, warning = FALSE, message=FALSE, echo = FALSE}
# Afficher le tableau
tab <- build_table(modele, coef_digits = 3, p_digits = 3)

show_table(tab, 
    caption = "Résultats du modèle gaussien",
    col.names = c('Variable',"Coeff.","Err.std","Val.z", "val.p","IC coeff 2,5%", "IC coeff 97,5%", "Sign.")
)

```

Les résultats de la régression linéaire multiples avaient déjà été interprétés dans la section \@ref(sect05512), nous ne commenterons pas ici les résultats du modèle GLM gaussien qui sont très semblables.


#### Le modèle GLM avec une distribution de Student {#sect06232}

Pour rappel, la distribution de Student ressemble à une distribution normale (section \@ref(sect024311)). Elle est symétrique autour de sa moyenne et a également une forme de cloche. Cependant, elle dispose de queues lourdes, ce qui signifie qu’elle permet de représenter des phénomènes présentant davantage de valeurs extrêmes qu’une distribution normale. Pour contrôler le poids des queues, la distribution de Student intègre un troisième paramètre : $\nu$ (nu). Lorsque $\nu$ tends vers l’infini, la distribution de Student tend vers une distribution normale (figure : \@ref(fig:studentdistrib)).

```{r studentdistrib, echo=FALSE, message=FALSE, warning=FALSE, out.width='95%', fig.cap="Effet du paramètre nu sur une distribution de Stundent"}

library(LaplacesDemon)
library(ggplot2)

colors <- c("#03071e","#6a040f","#f48c06", "#0096c7")
labels_col <- c("nu=2","nu=5","nu=30","normal")

ggplot() + 
  stat_function(aes(color = colors[[1]]), size = 1,
        fun = dst, args = list("mu"=0,
                               "sigma"=10,
                                "nu"=2)) +
  stat_function(aes(color = colors[[2]]), size = 1,
        fun = dst, args = list("mu"=0,
                               "sigma"=10,
                                "nu"=5)) +
  stat_function(aes(color = colors[[3]]), size = 1,
        fun = dst, args = list("mu"=0,
                               "sigma"=10,
                                "nu"=30)) +
  stat_function(aes(color = colors[[4]]), size = 1,
        fun = dnorm, args = list("mean"=0,
                               "sd"=10)) +
  scale_color_identity(name = "Paramètres",
        breaks = colors,
        labels = labels_col,
        guide = "legend") + 
  theme(axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        panel.background = element_blank(),
        panel.grid = element_blank()) +
  xlim(-50,50) + 
  ylim(0,0.05)

```

Comme vous pouvez le constater dans la carte d'identité au tableau \@ref(tab:studentdentity), le modèle GLM Student est très proche du modèle GLM gaussien. On modélise explicitement la moyenne de la distribution et son paramètre de dispersion (variance) est laissé fixe. Ce GLM est même souvent utilisé comme une version « robuste » du modèle gaussien du fait de sa capacité à intégrer explicitement l’effet des observations extrêmes. En effet, dans un modèle gaussien, les observations extrêmes (aussi appelées observations aberrantes) vont davantage influencer les paramètres du modèle que si une distribution de Student était utilisée.

```{r studentdentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim Student(\\mu,\\sigma,\\nu)$ \\newline $g(\\mu) = \\beta_0 + \\beta X$ \\newline $g(x) = x$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable continue dans l'intervalle $]-\\infty ; + \\infty[$", "Student", model_formula , "identitaire", "$\\mu$", "$\\beta_0$, $\\beta$,  $\\sigma$ et $\\nu$", "Homoscédasticité")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle de Student",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

##### Conditions d'application

Les conditions d’application sont les mêmes que pour un modèle GLM gaussien, à ceci prêt que ce modèle utilisant la distribution de Student est moins sensible aux observations extrêmes.

##### Interprétation des paramètres

L’interprétation des paramètres est la même que pour un modèle gaussien puisque l’on modélise la moyenne de la distribution et que la fonction de lien est la fonction identitaire. Le seul paramètre supplémentaire est $\nu$ qui n’a en soit aucune interprétation pratique. Notez simplement que si $\nu$ est supérieur à 30, un simple modèle GLM gaussien serait sûrement suffisant.

##### Exemple appliqué dans R

Nous proposons ici de simplement réajuster le modèle gaussien présenté dans la section précédente en utilisant une distribution de Student. Nous utilisons pour cela la fonction `gam` du *package* **mgcv** avec le paramètre `family=scat` pour utiliser une distribution de Student. Les valeurs de VIF ont déjà été calculées dans l'exemple précédent, nous pouvons donc passer directement au calcul des distances de Cook.


```{r stucookdist, fig.align = "center", out.width = "50%", fig.cap = "Distances de Cook pour un modèle GLM avec une distribution de Student", message=FALSE, warning=FALSE}
# Chargement des données
load("data/lm/DataVegetation.RData")

# Ajustement du modèle
modele <- gam(VegPct ~ log(HABHA)+poly(AgeMedian,2)+
                       Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal,
              family = scat)

# Calcul des distances de Cook
cooksd <- cooks.distance(modele)

# Affichage des valeurs
df <- data.frame(
  cook = cooksd,
  oid = 1:length(cooksd)
)

ggplot(data = df) + 
  geom_point(aes(x = oid, y = cook), color = rgb(0.4,0.4,0.4,0.7), size = 1) + 
  labs(x="", y = "distance de Cook")

```

Nous retrouvons les quatre observations avec des distances de Cook très fortes que nous avions identifiées dans le modèle gaussien. Nous décidons donc de les enlever pour les mêmes raisons que précédemment.

```{r stucookdist2, fig.align = "center", out.width = "50%", fig.cap = "Distances de Cook pour un modèle GLM avec une distribution de Student, après suppression des valeurs fortement influentes", message=FALSE, warning=FALSE}
# Chargement des données
DataFinal2 <- subset(DataFinal, cooksd<0.1)

# Ajustement du modèle
modele <- gam(VegPct ~ log(HABHA)+poly(AgeMedian,2)+
                       Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal2,
              family = scat)

# Calcul des distances de Cook
cooksd <- cooks.distance(modele)

# Affichage des valeurs
df <- data.frame(
  cook = cooksd,
  oid = 1:length(cooksd)
)

ggplot(data = df) + 
  geom_point(aes(x = oid, y = cook), color = rgb(0.4,0.4,0.4,0.7), size = 1) + 
  labs(x="", y = "distance de Cook")

```

Nous pouvons à présent vérifier si les résidus simulés se comportent tel qu'attendu.

```{r studentresid, message=FALSE, warning=FALSE, fig.align = "center", fig.cap = "Diagnostic général des résidus simulés pour le GLM avec distribution de Student", out.width = "95%"}

# Extraction des prédictions du modèle
mus <- predict(modele, type = 'response')

# Affichage des paramètres nu et sigma
modele$family$family
sigma_model <- 11.281
nu_model <- 6.333

library(LaplacesDemon) # pour simuler des données d'une distribution de Student

# Génération de 1000 simulations pour chaque prédiction
nsim <- 1000
cols <- lapply(1:length(mus),function(i){
  mu <- mus[[i]]
  sims <- rst(nsim, mu=mu, sigma = sigma_model, nu = nu_model)
  return(sims)
})
mat_sims <- do.call(rbind, cols)

# Calculer les résidus simulés
sim_res <- createDHARMa(simulatedResponse = mat_sims, 
                            observedResponse = DataFinal2$VegPct,
                            fittedPredictedResponse = mus,
                            integerResponse = F)
plot(sim_res)
```
Il semble que nous obtenions des résultats similaires à ceux du modèle gaussien : les résidus ne suivent pas une distribution uniforme (figure \@ref(fig:studentresid)) et nous avons potentiellement des valeurs aberrantes dans les données. Le graphique de quantile-quantile n'est parfois pas très adapté pour discerner une déviation de la distribution uniforme, nous pouvons dans ce cas afficher un histogramme des résidus pour en avoir le coeur net (figure \@ref(fig:studentresid2)).

```{r studentresid2, message=FALSE, warning=FALSE, fig.align = "center", fig.cap = "Distribution des résidus simulés du modèle GLM avec distribution de Student", out.width = "95%"}
ggplot()+
  geom_histogram(aes(x = residuals(sim_res)), bins = 50, color = "white")
```

Pour cet exercice, il est intéressant de comparer les formes des simulations issues du modèle gaussien et du modèle de Student pour bien distinguer la différence entre les deux.

```{r studentresid3, message=FALSE, warning=FALSE, fig.align = "center", fig.cap = "Simulations issues des modèles gaussien et Student, comparées aux données originales", echo = F, out.width = "95%"}

# formattage des simulations Students
df1 <- reshape2::melt(mat_sims[,1:20])

# simulation pour un modèle gaussien
modeleGau <- glm(VegPct ~ log(HABHA)+poly(AgeMedian,2)+
                       Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal2)

sigma2 <- sigma(modeleGau)
mus <- predict(modeleGau, type = 'response')

nsim <- 1000
cols <- lapply(1:length(mus),function(i){
  mu <- mus[[i]]
  sims <- rnorm(nsim, mean=mu, sd = sigma2)
  return(sims)
})
mat_sims2 <- do.call(rbind, cols)

df2 <- reshape2::melt(mat_sims2[,1:20])

P1 <- ggplot()+
  geom_histogram(data = DataFinal2, mapping = aes(x = VegPct, y = ..density..), color = 'white', bins = 50) + 
  geom_density(data = df1, aes(x = value, group = Var2), color = rgb(0.9,0.22,0.27,0.4)) + 
  labs(x="Densité de végétation",
       y = "",
       subtitle = "Simulations du modèle Student")+
  xlim(-10,110) + 
  ylim(0,0.023) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())


P2 <- ggplot()+
  geom_histogram(data = DataFinal2, mapping = aes(x = VegPct, y = ..density..), color = 'white', bins = 50) + 
  geom_density(data = df2, aes(x = value, group = Var2), color = rgb(0.11,0.21,0.34,0.4)) + 
  labs(x="Densité de végétation",
       y = "",
       subtitle = "Simulations du modèle gaussien")+
  xlim(-10,110) + 
  ylim(0,0.023) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
  
ggarrange(P1,P2, ncol = 2, nrow = 1)

```

Nous constatons ainsi que la différence entre les deux modèles est ici très mince, voire inexistante. Le seul élément que l'on peut noter est que le modèle de Student à une courbe (une queue de distribution) moins aplatie vers la droite. Cela lui permettrait de mieux tenir de compte de cas extrêmes avec de fortes densités de végétation (ce qui concerne donc très peu d'observations puisque cette variable a un maximum de 100).

Pour déterminer si le modèle de Student est plus pertinent à retenir que le modèle gaussien, nous pouvons ajuster un second modèle de Student pour lequel nous forçons artificiellement $\nu$ à être très élevé. Pour rappel, quand $\nu$ tend vers l'infini, la distribution de Student tend vers une distribution normale. Nous forçons ici $\nu$ à être supérieur à 100 pour créer un second modèle de Student se comportant quasiment comme un modèle gaussien et calculons les AIC des deux modèles.

```{r message=FALSE, warning=FALSE}

# Calcul d'un modèle de Student identitique à un modèle gaussien
modele2 <- gam(VegPct ~ log(HABHA)+poly(AgeMedian,2)+
                   Pct_014+Pct_65P+Pct_MV+Pct_FR, data = DataFinal2,
               family = scat(min.df = 100))

# Calcul des deux AIC
AIC(modele)
AIC(modele2)
```

Le second AIC (modèle de Student) est plus élevé, indiquant que le modèle est moins bien ajusté aux données. Dans le cas présent, il est plus pertinent de s'en tenir au modèle gaussien. Ce résultat n'est pas surprenant puisque la variable *Y* (pourcentage de végétation dans les îlots de l'île de Montréal) est relativement compacte et comporte peu / pas de valeurs pouvant être qualifiées de valeurs extrêmes.

Nous ne détaillons pas ici l'interprétation des coefficients du modèle (accessible avec la fonction `summary`) puisqu'ils s'interprètent de la même façon qu'un modèle GLM et qu'un modèle de régression linéaire multiple.

#### Le modèle GLM avec distribution Gamma {#sect06233}

Pour rappel, la distribution Gamma est strictement positive ($[0;+\infty[$), asymétrique et a une variance proportionnelle à sa moyenne (hétéroscedastique). Dans la section sur les distributions, nous avions vu que la distribution Gamma (section \@ref(sect024315)) est formulée avec deux paramètres : sa forme ($\alpha$ ou *shape*) et son échelle ($b$ ou *scale*). Ces deux paramètres n’ont pas une interprétation très intuitive, mais il est possible avec un peu de jonglage mathématique d’arriver à une reparamétrisation intéressante. Cela est détaillé dans l'encadré ci-dessous; notez toutefois qu'il n'est pas nécessaire de maîtriser parfaitement le contenu de cet encadré pour lire la suite du contenu les modèles GLM avec une distribution Gamma.


::: {.bloc_aller_loin data-latex=""}
**Reparamétrisation d'une distribution Gamma pour un GLM**

Si nous disposons d’une variable *Y*, suivant une distribution Gamma telle que $Y \sim Gamma(\alpha,b)$ avec $\alpha$ le paramètre de forme et $b$ le paramètre d’échelle, alors, l'espérance et la variance de *Y* peuvent être définies comme suit : 

\footnotesize
\begin{equation}
\begin{aligned}
&E(Y) = \alpha * b \\
&Var(Y) = \alpha * b^2\\
\end{aligned}
(\#eq:glm18)
\end{equation}
\normalsize

En d’autres termes, l’espérance (l’équivalent de la moyenne pour une distribution normale) de notre variable *Y* est égale au produit des paramètres de forme et d’échelle.

Avec ces propriétés, il est possible de redéfinir la fonction de densité de la distribution Gamma et d’arriver à une nouvelle formulation : $Y \sim Gamma(\mu,\alpha)$. $\mu$ est donc l'espérance de *Y* (interprétable comme sa moyenne, soit sa valeur attendue) et $\alpha$ permet de capturer la dispersion de la distribution Gamma. Par extension des relations présentées ci-dessus, il est alors possible de reformuler la variance en fonction de $\mu$ et $\alpha$.

\footnotesize
\begin{equation}
\begin{aligned}
&Var(Y) = \alpha \times b^2\\
&\mu = \alpha \times b \text{ soit }b = \frac{\mu}{a}\\
&Var(Y) = \alpha \times (\frac{\mu}{\alpha})^2 \text{ soit } Var(Y) = \frac{\mu^2}{\alpha}\\
\end{aligned}
(\#eq:glm19)
\end{equation}
\normalsize

On observe donc que la variance dans un modèle Gamma augmente de façon quadratique avec la moyenne, mais est tempérée par le paramètre de forme. On en conclut qu'un paramètre de forme plus grand produira une distribution moins étalée.

Dans ce contexte, $\mu$ doit être strictement positif : la valeur attendue moyenne d'une distribution Gamma doit être positive par définition puisque qu'une distribution Gamma ne peut pas produire de valeurs négatives. Il est donc logique d’utiliser la fonction logarithmique comme fonction de lien, puisque sa contrepartie (la fonction exponentielle) ne produit que des résultats positifs.
:::

Pour résumer, nous nous retrouvons donc avec un modèle qui prédit sur une échelle logarithmique la moyenne d’une distribution Gamma. Notez qu’il existe d’autres façons de spécifier un modèle GLM avec une distribution Gamma, mais celle-ci est la plus intuitive.

```{r gammadentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim Gamma(\\mu,\\alpha)$ \\newline $g(\\mu) = \\beta_0 + \\beta X$ \\newline $g(x) = log(x)$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable continue dans l'intervalle $]0 ; + \\infty[$", "Gamma", model_formula , "log", "$\\mu$", "$\\beta_0$, $\\beta$, et  $\\alpha$", "$Variance = \\frac{\\mu^2}{\\alpha}$")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle Gamma",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

##### Interprétation des paramètres

Puisque le modèle utilise la fonction de lien *log*, alors les coefficients $\beta$ expriment l’augmentation de l’espérance (la valeur attendue de *Y*, ce qui est proche de l'idée de moyenne) de la variable Y sur une échelle logarithmique (comme dans un modèle de Poisson). Il est possible de convertir les coefficients dans l’échelle originale de la variable *Y* en utilisant la fonction exponentielle (l’inverse de la fonction *log*), mais ces coefficients représentent alors des effets multiplicatifs et non des effets additifs.
Prenons un exemple, admettons que le coefficient $\beta_1$, associé à la variable $X_1$ soit de 1,5. Cela signifie qu’une augmentation d’une unité de $X_1$, augmente le log de *Y* de 1,5 unité. L’exponentielle du coefficient est 4,48, ce qui signifie qu’une augmentation d’une unité entraîne une multiplication par 4,48 de la valeur attendue de *Y* (l’espérance de *Y*). Le paramètre de forme ($\alpha$) n’a pas d’interprétation pratique, bien qu’il soit utilisé dans les différents tests des conditions d’application du modèle et dans le calcul de sa déviance.

##### Conditions d'application

Dans un modèle GLM gaussien, la variance est capturée par un paramètre $\sigma$ et est constante, produisant la condition d'homoscésadicité des résidus. Dans un modèle Gamma, la variance varie en fonction de l'espérance et du paramètre de forme selon la relation : $Var(Y) = \frac{E(Y)^2}{\alpha}$. Les résidus sont donc par nature hétéroscédastiques dans un modèle Gamma et doivent suivre cette relation.

##### Exemple appliqué dans R

Pour cet exemple, nous nous intéressons à la durée de déplacements en milieu urbain. Ce type d’analyse permet notamment de mieux comprendre les habitudes de déplacement de la population et d’orienter les politiques de transport. Plusieurs travaux concluent que les durées de déplacement en milieu urbain varient en fonction du motif du déplacement, du mode de transport utilisé, des caractéristiques socio-économiques de l’individu, et des caractéristiques du trajet en lui-même [@anastasopoulos2012analysis; @frank2008urban]. Nous modélisons ici la durée en minute d’un ensemble de déplacements effectués par des Montréalais en 2017 et enregistré avec l’application MTL Trajet proposée par la ville de Montréal. Ces données sont disponibles sur le site web des [données ouvertes de Montréal](http://donnees.ville.montreal.qc.ca/dataset/mtl-trajet) et son anonymisées. Nous ne disposons donc d’aucune information au niveau individuel.  Compte tenu du très grand nombre d’observations (plus de 185 000), nous n’avons dû effectuer quelques opérations de tri et nous avons ainsi supprimé :

* Les trajets utilisant de multiples modes de transport (sauf en combinaison avec à pied, par exemple, un trajet effectué à pied et en transport en commun a été recatégorisé comme un trajet en transport en commun uniquement). Les déplacements multimodaux se distinguent largement des déplacements unimodaux dans la littérature scientifique.
* Les trajets de nuit (seuls les trajets démarrant entre 07h et 21h ont été conservés).
* Les trajets dont le point de départ est un arrondissement / municipalité pour lequel moins de 150 trajets ont été enregistrés (trop peu d’observations)
* Les trajets de plus de deux heures (cas rares, considérés comme des données aberrantes).
* Les trajets dont le point de départ est à moins de 100 mètres du point d’arrivée (formant des boucles plutôt que des déplacements).

Nous arrivons ainsi à un total de 24969 observations. Pour modéliser ces durées de déplacement, nous utilisons les prédicteurs présentés dans le tableau

```{r variablegamma, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}
df <- data.frame(
  C1 = c("Mode", "Motif", "HeureDep", "ArrondDep", "LogDist", "MemeArrond", "Semaine"),
  C2 = c("Mode de déplacement", "Motif du déplacement", "Heure de départ", "Arrondissement de départ", "Logarithme de la distance à vol d’oiseau en km", "L’arrivée du trajet du trajet se situe-t-elle dans le même arrondissement que le départ?", "Le trajet a-t-il été effectué en semaine ou en fin de semaine?"),
  C3 = c("Variable catégorielle", "Variable catégorielle", "Variable catégorielle", "Variable catégorielle", "Variable continue", "Variable binaire", "Variable binaire"),
  C4 = c("Transport collectif ; piéton ; vélo et véhicule individuel", "Travail ; loisir ; magasinage et éducation", "De 07h à 21h", "Nom de l’arrondissement dont part le trajet", "Logarithme de la distance à vol d’oiseau en km entre le point de départ et d’arrivée", "Oui ou non", "Semaine ou fin de semaine")
)

show_table(df, 
    caption = "Variables indépendantes utilisées dans le modèle Gamma",
    col.names = c("Nom de la variable","Signification","Type de variable","Mesure"), 
    col.to.resize = c(2,4),
    col.width = "5cm"
)

```

Les temps de trajet forment une variable strictement positive et très vraisemblablement asymétrique. En effet, nous nous attendons à observer une certaine concentration de valeurs autour d'une moyenne, et davantage de trajets avec de courtes durées que de trajets avec de longues durées. Pour nous en assurer, réalisons un histogramme de la distribution de notre variable *Y* et comparons la avec des distributions normale et Gamma.

```{r gammadatadistrib, fig.cap = "Distribution des temps de trajets diurnes à Montréal", fig.align = "center",  message=FALSE, warning=FALSE, out.width='50%'}

# Chargement des données
dataset <- read.csv("data/glm/DureeTrajets.csv", stringsAsFactors = F)

arrondMTL <- c("Mercier-Hochelaga-Maisonneuve", "Villeray-Saint-Michel-Parc-Extension", 
               "Ville-Marie", "Verdun", "Saint-Leonard", "Saint-Laurent", 
               "Rosemont-La Petite-Patrie", "Riviere-des-Prairies-Pointe-aux-Trembles", 
               "Pierrefonds-Roxboro", "Outremont", "Montreal-Nord", "Le Sud-Ouest", 
               "Le Plateau-Mont-Royal", "Lachine" , "Ahuntsic-Cartierville",
               "Anjou" ,"Cote-des-Neiges-Notre-Dame-de-Grace", "LaSalle"  
)

dataset <- subset(dataset,dataset$ArrondDep %in% arrondMTL)

# Définissons 7H du matin comme la référence pour la variable Heure de départ

dataset$HeureDep <- relevel(
  factor(dataset$HeureDep, levels = as.character(7:21)),
  ref = "7")

# Comparaison de la distribution originale avec une distribution 
# normale et une distribution Gamma
library(fitdistrplus)
model_gamma <- fitdist(dataset$Duree, distr = "gamma")

ggplot(data = dataset) + 
  geom_histogram(aes(x=Duree, y = ..density..), bins = 40, color = "white")+
  stat_function(fun = dgamma, color = 'red', size = 0.8, 
                args = as.list(model_gamma$estimate))+
  stat_function(fun = dnorm, color = 'blue', size = 0.8, 
                args = list(mean = mean(dataset$Duree), 
                            sd = sd(dataset$Duree)))+
  labs(x = 'Temps de déplacement (minutes)',
       y = '',
       subtitle = "modèles Gamma et gaussien")
```

La figure \@ref(fig:gammadatadistrib) permet de constater l'asymétrie de la distribution des temps de trajets et qu'un modèle Gamma (ligne rouge) a plus de chance d'être adapté aux données qu'un modèle gaussien (ligne bleue).

**Vérification des conditions d'application**

Comme pour les modèles précédents, nous commençons par la vérification de l'absence de multicolinéarité.

```{r  message=FALSE, warning=FALSE, out.width='50%'}
## Calcul du VIF
vif(glm(Duree ~ Mode + Motif + HeureDep + LogDist +
          ArrondDep + MemeArrond + Jour,
        data = dataset,
        family = Gamma(link="log")))
```

L'ensemble des valeurs de VIF sont inférieures à trois, indiquant donc l'absence de multicolinéarité excessive. Nous pouvons donc ajuster une première version du modèle (ici avec le *package* **VGAM** et la fonction `vglm`) et calculer les distances de Cook.

```{r cookdistgamma, message=FALSE, warning=FALSE, out.width='50%', fig.cap = "Distances de Cook pour le modèle Gamma", fig.align = "center"}
# Calcul du modèle avec VGAM
modele <- vglm(Duree ~ Mode + Motif + HeureDep + LogDist +
                 ArrondDep+ MemeArrond + Semaine,
               data = dataset,
               family=gamma2(lmu = "loglink"))

# Calcul des distances de Cook
hats <- hatvaluesvlm(modele)[,1]
res <- residuals(modele,type = "pearson")[,1]
disp <- modele@coefficients[[2]]**-1
nbparams <- modele@rank

cooksd <- (res/(1 - hats))^2 * hats/(disp * nbparams)

df <- data.frame(
  cook = cooksd,
  oid = 1:length(cooksd)
)

# Représentation des distances de Cook
ggplot(data = df)+
  geom_point(aes(x = oid, y = cook), size = 0.5, color = rgb(0.4,0.4,0.4,0.4)) + 
  geom_hline(yintercept = 0.003, color = "red") +
  labs(x = "", y = "distance de Cook")
```
Puisque nous disposons d'un (très) grand nombre d'observations, nous pouvons nous permettre de retirer les quelques observations fortement influentes (distance de Cook > 0,003 dans notre cas) qui apparaissent dans la figure \@ref(fig:cookdistgamma). Nous retirons ainsi 28 observations et réajustons le modèle.

```{r message=FALSE, warning=FALSE}
# Retirer les valeurs influentes
dataset2 <- subset(dataset, cooksd<0.003)

# Calcul du modèle avec VGAM
modele <- vglm(Duree ~ Mode + Motif + HeureDep + LogDist +
                 ArrondDep+ MemeArrond + Semaine,
               data = dataset2,
               family=gamma2(lmu = "loglink"))
```

Nous constatons ainsi à la figure \@ref(fig:cookdistgamma2) que dans la nouvelle version du modèle, aucune valeur particulièrement influente ne semble être présente.

```{r cookdistgamma2, message=FALSE, warning=FALSE, out.width='50%', fig.cap = "Distances de Cook pour le modèle Gamma (sans les observations fortement influentes)", fig.align = "center", echo = FALSE}

# Calcul des distances de Cook
hats <- hatvaluesvlm(modele)[,1]
res <- residuals(modele,type = "pearson")[,1]
disp <- modele@coefficients[[2]]**-1
nbparams <- modele@rank

cooksd <- (res/(1 - hats))^2 * hats/(disp * nbparams)

df <- data.frame(
  cook = cooksd,
  oid = 1:length(cooksd)
)

# Représentation des distances de Cook
ggplot(data = df)+
  geom_point(aes(x = oid, y = cook), size = 0.5, color = rgb(0.4,0.4,0.4,0.4)) +
  labs(x = "", y = "distance de Cook")
```

```{r simdistribgamma, message=FALSE, warning=FALSE, out.width='60%', fig.cap = "Comparaison de la distribution originale et de simulations issues du modèle Gamma", fig.align = "center", echo = TRUE}

# Extraction des prédictions du modèle (mu)
mus <- modele@fitted.values
# Extration du paramètre de forme
shape <- exp(modele@coefficients[[2]])

# Calcul des simulations
nsim <- 1000

cols <- lapply(1:length(mus),function(i){
  mu <- mus[[i]]
  sims <- rgamma(n = nsim,shape = shape, scale = mu/shape)
  return(sims)
})
mat_sims <- do.call(rbind, cols)


# Représentation graphique de 20 simulations
df2 <- reshape2::melt(mat_sims[,0:20])

ggplot() + 
  geom_histogram(aes(x = Duree, y = ..density..),
                 data = dataset, bins = 100, color = "black", fill = "white") + 
  geom_density(aes(x = value, y=..density.., group = Var2), data = df2, 
               fill = rgb(0,0,0,0), color = rgb(0.9,0.22,0.27,0.4), size = 1)+
  xlim(0,200)+
  labs(X="durée (minutes)", y="densité")

```
Avant de calculer les résidus simulés, nous comparons la distribution originale des données et des simulations issues du modèle. La figure \@ref(fig:simdistribgamma) permet de constater que le modèle semble bien capturer l’essentiel de la forme de la variable *Y* originale. Nous notons un léger décalage entre la pointe des deux distributions, laissant penser que les valeurs prédites par le modèle tendent à être légèrement plus grandes que les valeurs réelles. Pour mieux appréhender ce constat, nous passons à l'analyse des résidus simulés.

```{r gammaresids, message=FALSE, warning=FALSE, out.width='60%', fig.cap = "Distribution des résidus simulés du modèle Gamma", fig.align = "center", echo = TRUE}

# DHarma tests
sim_res <- createDHARMa(simulatedResponse = mat_sims, 
                            observedResponse = dataset2$Duree,
                            fittedPredictedResponse = modele@fitted.values[,1],
                            integerResponse = F)

ggplot() + 
  geom_histogram(aes(x = residuals(sim_res)), bins = 100, color = "white") + 
  labs(x = "résidus simulés",
        y = "")
```

Nul besoin d'un test statistique pour constater que ces résidus (figure \@ref(fig:gammaresids)) ne suivent pas une distribution uniforme. Nous observons une nette surreprésentation de résidus à 1 et une nette sous-représentation de résidus à 0. Il y a donc de nombreuses observations dans notre modèle pour lesquelles les simulations sont systématiquement trop fortes, et pas assez pour lesquelles les simulations seraient systématiquement trop faibles.

```{r gammaresids2, message=FALSE, warning=FALSE, out.width='60%', fig.cap = "Diagnostic général des résidus simulés du modèle Gamma", fig.align = "center", echo = TRUE}
plot(sim_res)
```

La figure \@ref(fig:gammaresids2) indique que le modèle souffre à la fois d'un problème de dispersion (la relation espérance-variance n'est donc pas respectée) et est affecté par des valeurs aberrantes. Considérant que nous avons encore un très grand nombre d'observations, nous faisons le choix de retirer celles pour lesquelles la méthode des résidus simulés estime qu'elles sont des valeurs aberrantes dans au moins 1% des simulations, soit environ 620 observations.

```{r gammaresids3, message=FALSE, warning=FALSE, out.width='60%', fig.cap = "Diagnostic général des résidus simulés du modèle Gamma (après suppression d'environ 620 valeurs aberrantes)", fig.align = "center", echo = TRUE}

# Sélection des valeurs aberrantes au seuil 0.01
sim_outliers <- outliers(sim_res, 
                         lowerQuantile = 0.01,
                         upperQuantile = 0.99, 
                         return = "logical")
table(sim_outliers)

# Retirer ces observations des données
dataset3 <- subset(dataset2, sim_outliers==FALSE)

# Réajuster le modèle
modele <- vglm(Duree ~ Mode + Motif + HeureDep + 
                 LogDist + ArrondDep + MemeArrond + Semaine,
               data = dataset3,
               model = T,
               family=gamma2)

modele2 <- vglm(Duree ~ Mode + Motif + HeureDep + ArrondDep + MemeArrond + Semaine,
               data = dataset3,
               model = T,
               family=gamma2)

# Extraction des prédictions du modèle (mu)
mus <- modele@fitted.values
# extration du paramètre de forme
shape <- exp(modele@coefficients[[2]])

# Calcul des simulations
nsim <- 1000

cols <- lapply(1:length(mus),function(i){
  mu <- mus[[i]]
  sims <- rgamma(n = nsim,shape = shape, scale = mu/shape)
  return(sims)
})
mat_sims <- do.call(rbind, cols)

# Calcul des résidus simulés
sim_res <- createDHARMa(simulatedResponse = mat_sims, 
                            observedResponse = dataset3$Duree,
                            fittedPredictedResponse = modele@fitted.values[,1],
                            integerResponse = F)

plot(sim_res)
```

La figure \@ref(fig:gammaresids3) indique que les résidus simulés ne suivent toujours pas une distribution uniforme, et qu'il existe une relation prononcée (panneau de droite) entre les résidus et les valeurs prédites.  Cette dernière laisse penser que des prédicteurs importants ont été omis dans le modèle, ce qui n’est pas surprenant compte tenu du fait que nous ne disposons d’aucune donnée socioéconomique sur les individus ayant réalisé les trajets en question. Nos données sont également potentiellement affectées par la présence de dépendance spatiale.

D'un point de vue purement pédagogique, nous pouvons comparer graphiquement la variance observée dans les données et la variance attendue par le modèle. La figure \@ref(fig:dispgamma) montre clairement que la variance des données tend à être plus grande qu'attendue quand les temps de trajet sont courts, mais diminue trop vite quand les temps de trajet augmentent. D'autres distributions pourraient être envisagées pour ajuster notre modèle : Lognormal, Weibull, etc.

```{r dispgamma, fig.align="center", warning = FALSE, message=FALSE, fig.cap="Comparaison de la variance attendue par le modèle et la variance observée dans les données pour le modèle Gamma", out.width = "50%"}

# Extraction des prédictions du modèle
mus <- predict(modele, type = "response")[,1]

# Création d'un dataframe pour contenir la prédiction et les vraies valeurs
df1 <- data.frame(
  mus = mus,
  reals = dataset3$Duree
)

# Calcul de l'intervalle de confiance à 95% selon la distribution de Poisson
# et stockage dans un second dataframe
seqa <- seq(10,120,10)

shape <- exp(modele@coefficients[[2]])

df2 <- data.frame(
  mus = seqa,
  lower = qgamma(p = 0.025, shape = shape, scale = seqa/shape),
  upper = qgamma(p = 0.975, shape = shape, scale = seqa/shape)
)

# Affichage des valeurs réelles et prédites (en rouge)
# et de leur variance selon le modèle (en noir)
ggplot() + 
  geom_point(data = df1, 
             mapping = aes(x = mus, y = reals),
             color =rgb(0.9,0.22,0.27,0.4), size = 0.5) + 
  geom_errorbar(data = df2,
                mapping = aes(x = mus, ymin = lower, ymax = upper),
                width = 0.2, color = rgb(0.4,0.4,0.4)) + 
  labs(x = 'valeurs prédites',
       y = "valeurs réelles")
```
À ce stade, nous disposons de suffisamment d’éléments pour douter des résultats du modèle. Nous allons tout de même poursuivre notre analyse afin d'illustrer l'estimation de la qualité d'ajustement d'un tel modèle et son interprétation.

**Analyse de la qualité d'ajustement**
```{r warning = FALSE, message=FALSE}
#ajustement d'un modele nul
modele.null <- vglm(Duree ~1,
               data = dataset3,
               model = T,
               family=gamma2)

# calcul des pseudos R2
rsqs(loglike.full = logLik(modele),
     loglike.null = logLik(modele.null),
     full.deviance = logLik(modele) * -2,
     null.deviance = logLik(modele.null) * -2,
     nb.params = modele2@rank,
     n = nrow(dataset3)
     )

# calcul du RMSE
preds <- predict(modele, type="response")[,1]
sqrt(mean((preds - dataset3$Duree)**2))
```
Le modèle n’explique que 5% de la déviance et obtient des valeurs de R^2^ ajusté de McFadden, de Cox et Snell et de Nagelkerke de respectivement 0,05, 0,33 et 0,33. La moyenne de l’erreur quadratique est de seulement 13,5 indiquant que le modèle se trompe en moyenne de seulement 13,3 minutes. La capacité de prédiction du modèle est donc limitée sans être catastrophique.

**Interprétation des résultats**

Pour rappel, la fonction de lien dans notre modèle est la fonction *log*. Chaque coefficient représente donc l'impact de l'augmentation d'une unité des variables indépendantes sur le logarithme de l'espérance de notre variable dépendante. Si nous transformons nos coefficients avec la fonction exponentielle (*exp*) nous obtenons, pour chaque augmentation d'une unité des variables indépendantes, la multiplication de l'espérance de notre variable dépendante.

Puisque nos trajets peuvent provenir de nombreux arrondissements, nous proposons de représenter l'exponentiel de leurs coefficients avec un graphique. Nous pouvons d'ailleurs comparer les exponentiels des coefficients et les effets marginaux pour simplifier l'interprétation.


```{r arrondgamma, warning = FALSE, message=FALSE, fig.cap = "Effet de l'arrondissement de départ sur les temps de trajet à Montréal", fig.align = "center", out.width = "100%"}

# Extraction des coefficient du modèle
coeffs <- modele@coefficients
# Calcul des interval de confiance des coefficients
conf <- confint(modele)
# Passage en exponentiel
df <- exp(cbind(coeffs, conf))

# Extraction des coefficients pour les arrondissements
dfArrond <- data.frame(df[grepl("ArrondDep",row.names(df), fixed = T),])
names(dfArrond) <- c("coeff", "lower","upper")
dfArrond$Arrondissement <- gsub("ArrondDep","",rownames(dfArrond),fixed = T)

# Graphique des exponentiels des coefficients
P1 <- ggplot(data = dfArrond) + 
  geom_vline(xintercept = 1, color = "red")+
  geom_errorbarh(aes(y = reorder(Arrondissement, coeff), xmin = upper, xmax = lower),
                 height = 0)+
  geom_point(aes(y = reorder(Arrondissement, coeff), x = coeff)) + 
  geom_text(aes(x = upper, y = reorder(Arrondissement, coeff),
                label = paste("coeff. : ",round(coeff,2),sep="")),
                size = 3, nudge_x = 0.07)+
  labs(x = "Coefficient multiplicateur (ref : Ahuntsic-Cartierville)",
       y = "",
       subtitle = "Exponentiels des coefficients du modèle")+
  xlim(c(0.75,1.46))

# Création d'un dataframe fictif pour les effets marginaux
dfpred <- expand.grid(
  LogDist = mean(dataset3$LogDist),
  Motif = 'education',
  HeureDep = '7',
  MemeArrond = 'Different',
  ArrondDep = unique(dataset3$ArrondDep),
  Mode = 'pieton','velo','transport collectif',
  Semaine = 'lundi au vendredi'
)

# Utiliser le modèle pour effectuer des prédictions (échelle log)
lin_pred <- predict(modele,dfpred, se = T)
mu_lin_pred <- lin_pred$fitted.values[,1]
se_lin_pred <- lin_pred$se.fit[,1]

dfpred2 <- data.frame(
  pred = exp(mu_lin_pred),
  lower = exp(mu_lin_pred- 1.96*se_lin_pred),
  upper = exp(mu_lin_pred+ 1.96*se_lin_pred)
)

dfpred2 <- cbind(dfpred2, dfpred)

# Réaliser le graphique des effets marginaux
P2 <- ggplot(data = dfpred2) + 
  geom_col(aes(x = pred, y = ArrondDep)) + 
  geom_errorbarh(aes(xmin = lower, xmax = upper, y = ArrondDep)) + 
  labs(x = "Temps de déplacement prédit", y="",
       subtitle = "Prédiction du modèle")

ggarrange(P1,P2, ncol = 1, nrow = 2)
  

```

La figure \@ref(fig:arrondgamma) permet de constater que les arrondissements Ville-Marie et Plateau-Mont-Royal se distinguent avec des trajets plus courts (environ 20% plus courts en moyenne que les trajets partant d'Ahuntsic-Cartierville). À l'inverse, Lachine est de loin l'arrondissement avec les trajets les plus longs (25% plus long en moyenne que les trajets partant d'Ahuntsic-Cartierville).

Nous appliquons la même méthode de visualisation à la variable Heure de départ des trajets.

```{r heuresgamma, warning = FALSE, message=FALSE, fig.cap = "Effet l'heure de départ sur les temps de trajet à Montréal", fig.align = "center", out.width = "85%"}

# Extraction des valeurs pour les heures de départ
dfHeures <- data.frame(df[grepl("HeureDep",row.names(df), fixed = T),])
names(dfHeures) <- c("coeff", "lower","upper")
dfHeures$Heure <- gsub("HeureDep","",rownames(dfHeures),fixed = T)

# rajouter des 0 et des h pour de jolies légendes
dfHeures$Heure <- ifelse(str_length(dfHeures$Heure)==1,
                         paste("0",dfHeures$Heure,"h",sep=""),paste(dfHeures$Heure,"h",sep=""))

# afficher le graphique
ggplot(data = dfHeures) + 
  geom_hline(yintercept = 1, color = "red")+
  geom_errorbar(aes(x = Heure, ymin = upper, ymax = lower), width = 0)+
  geom_point(aes(x = Heure, y = coeff)) + 
  geom_text(aes(y = upper, x = Heure, label = round(coeff,2)), size = 3, nudge_y = 0.07)+
  labs(x = "Coefficient multiplicateur (ref : 07:00)",
       y = "")
```

Nous pouvons ainsi observer à la figure \@ref(fig:heuresgamma) que les trajets effectués entre 10h et 12h sont les plus longs de la journée, entre 30 et 40% plus longs que ceux effectués à 07h et 08h qui constituent les trajets les plus courts.

Le reste des coefficients (ainsi que le paramètre de forme) sont affichés dans le tableau \@ref(tab:coeffsgamma). Comparativement à un trajet effectué à pied, un trajet en transport en commun durera en moyenne 52% plus longtemps (1,53 fois plus long), alors que des déplacements en véhicule individuel et en vélo sont respectivement 28% et 23% moins longs. Aucune différence n’est observable entre les déplacements effectués en semaine ou pendant la fin de semaine.

Les déplacements ayant comme motif le magasinage et le travail ont tendance à être en moyenne plus courts de 11% et 6% respectivement, comparativement aux déplacements effectués pour l’éducation ou le loisir (différence non significative entre loisir et éducation). Sans surprise, la distance entre le point de départ et d’arrivée du trajet (`LogDist`) affecte sa durée de façon positive. Considérant qu’il est difficile d’interpréter des log de kilomètre (dû à une transformation de la variable originale), nous représentons l’effet de cette variable avec la prédiction du modèle à la figure. Nous utilisons pour cela le cas suivant : déplacement à pied à 07h en semaine, ayant pour motif éducation, dont le point de départ se situe dans l’arrondissement Ahuntsic et donc le point d’arrivée est dans un autre arrondissement. Seule la distance du trajet varie de 1 à 40km. À titre de comparaison, nous représentons aussi pour les mêmes conditions le cas d’un cycliste (en vert), et d’un utilisateur du transport en commun (en bleu). Les lignes en pointillés représentent les intervalles de confiance à 95% des prédictions.

```{r coeffsgamma, warning = FALSE, message=FALSE, echo = F}
tableau <- build_table(modele, coef_digits = 3, OR_digits = 3)

tableau_ok <- tableau[c(1:11,28,48:nrow(tableau)),]
tableau_ok <- tableau_ok[,c(1,2,3,6,7,8,9)]

show_table(tableau_ok, 
    caption = "Résultats pour le modèle GLM Gamma",
    col.names = c('Variable',"Coeff.","Exp(Coeff.)","Val.p","IC 2,5% exp(Coeff.)","IC 97,5% exp(Coeff.)", "Sign.")
)

```

```{r distancegamma, warning = FALSE, message=FALSE, fig.cap = "Effet de la distance à vol d'oiseau sur les temps de trajet à Montréal", fig.align = "center", out.width = "85%"}

# Création d'un dataframe fictif pour la prédiction
dfpred <- expand.grid(
  Dist = seq(1,40, 0.5),
  Motif = 'education',
  HeureDep = '7',
  MemeArrond = 'Different',
  ArrondDep = 'Ahuntsic-Cartierville',
  Mode = c('pieton','velo','transport collectif'),
  Semaine = 'lundi au vendredi'
)

# Mise en log de la variable de distance
dfpred$LogDist <- log(dfpred$Dist)

# Calcul des prédictions et de leur erreur standard (échelle log)
lin_pred <- predict(modele,dfpred, se = T)

# Calcul des intervales de confiance et mise en exponentielle des prédictions
dfpred$pred <- exp(lin_pred$fitted.values[,1])
dfpred$lower <- exp(lin_pred$fitted.values[,1] -1.96*lin_pred$se.fit[,1])
dfpred$upper <- exp(lin_pred$fitted.values[,1] +1.96*lin_pred$se.fit[,1])

# Ajoutons les accents pour le graphiques
dfpred$Mode <- as.character(dfpred$Mode)
dfpred$Mode2 <- case_when(dfpred$Mode == "pieton" ~ "piéton",
                         dfpred$Mode == "velo" ~ "vélo",
                         TRUE ~ dfpred$Mode)

# Affichage des résultats
ggplot(data = dfpred) + 
  geom_path(aes(x = Dist, y = lower, color = Mode2), linetype = "dashed")+
  geom_path(aes(x = Dist, y = upper, color = Mode2), linetype = "dashed")+
  geom_path(aes(x = Dist, y = pred, color = Mode2), size = 1) + 
  labs(y = "temps de trajet prédit (min)",
       x = "distance à vol d'oiseau (km)")
```

#### Le modèle GLM avec une distribution de Beta {#sect06234}

Pour rappel, la distribution de Beta est une distribution définie sur l’intervalle $]0,1[$, elle est donc particulièrement utile pour décrire des proportions, des pourcentages ou des probabilités. Dans la section \@ref(sect024316) sur les distributions, nous avions présenté la paramétrisation classique de la distribution avec les paramètres $a$ et $b$ étant tous les deux des paramètres de forme. Ces deux paramètres n’ont pas d’interprétation pratique, mais il est possible (comme pour la distribution Gamma) de reparamétrer la distribution Beta avec un paramètre de centralité (espérance) et de dispersion.

::: {.bloc_aller_loin data-latex=""}
**Reparamétrer la distribution Beta**

Pour une distribution Beta telle que définie par $Y \sim Beta(a,b)$, l'espérance de cette distribution et sa variance sont données par : 

\footnotesize
\begin{equation}
\begin{aligned}
&E(Y) = \frac{a}{a+b} \\
&Var(Y) = \frac{a \times b}{(a+b)^2(a+b+1)}\\
\end{aligned}
(\#eq:glm20)
\end{equation}
\normalsize

Pour reparamétrer cette distribution, on définit un nouveau paramètre $\phi$ (phi) tel que :

\footnotesize
\begin{equation}
\begin{aligned}
&a = \phi * E(Y) \\
&b = \phi - a \\
&Var(Y) = \frac{E(Y) \times (1-E(Y))}{1+\phi}
\end{aligned}
(\#eq:glm21)
\end{equation}
\normalsize

De cette manière, il est possible d'exprimer la distribution Beta en fonction de son espérance (sa valeur attendue, ce qui s'interprète approximativement comme une moyenne) et un paramètre $\phi$ intervenant dans le calcul de sa variance. Vous noterez d'ailleurs que la variance de cette distribution dépend de sa moyenne, impliquant à nouveau une hétéroscédasticité intrinsèque.
:::

Puisqu'une distribution Beta est définie dans l'intervalle $]0,1[$, son espérance doit également être comprise dans $]0,1[$. La fonction de lien d'un modèle GLM avec distribution Beta doit donc contraindre son équation de régression dans cet intervalle. La candidate toute désignée est donc la fonction logistique.

Pour résumer, nous nous retrouvons donc avec un modèle qui prédit la moyenne d’une distribution de Beta avec une fonction de lien logistique. La variance de cette distribution est fonction de cette moyenne et d'un second paramètre $\phi$. Ces informations sont résumées dans la fiche d'identité du modèle (tableau \@ref(tab:betaentity)).

```{r betaentity, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}

model_formula <- "$Y \\sim Beta(\\mu,\\phi)$ \\newline $g(\\mu) = \\beta_0 + \\beta X$ \\newline $g(x) = log(\\frac{x}{1-x})$"

tableau <- data.frame(
 C1 = c("Type de variable dépendante","Distribution utilisée", "Formulation", "Fonction de lien", "Paramètre modélisé", "Paramètres à estimer", "Conditions d'application"),
 C2 = c("Variable continue dans l'intervalle $]0,1[$", "Student", model_formula , "log", "$\\mu$", "$\\beta_0$, $\\beta$, et  $\\phi$", "$Variance = \\frac{\\mu \\times (1-\\mu)}{1+\\phi}$")
)

show_table(tableau, 
    caption = "Carte d'identité du modèle Beta",
    col.names = NULL, 
    col.to.resize = c(2),
    col.width = "8cm"
)
```

##### Conditions d'application

Comme pour un modèle Gamma, la seule condition d'application spécifique à un modèle avec distribution Beta est que la variance des résidus suit la forme attendue par la distribution Beta.

##### Interprétation des coefficients

Puisque le modèle utilise la fonction de lien logistique, les exponentiels des coefficients $\beta$ du modèle peuvent être interprétés comme des rapports de cotes (voir la section \@ref(sect06211) sur le modèle GLM binomial). Admettons ainsi que nous ayons obtenu pour une variable indépendante $X_1$ le coefficient $\beta_1$ de 0,12. Puisque le coefficient est positif, cela signifie qu’une augmentation de $X_1$ conduit à une augmentation en moyenne de *Y* (plus exactement de son espérance). L’exponentiel de 0,12 étant 1,13 indique qu’une augmentation d’une unité de $X_1$ multiplie par 1,13 (augmente de 13%) les chances d'une augmentation de *Y*. Pour ce type de modèle, il est particulièrement important de calculer ses prédictions afin d'en faciliter l'interprétation.

##### Exemple appliqué dans R

Afin de présenter le modèle GLM avec une distribution Beta, nous utilisons un jeu de données que nous avons construit pour l’île de Montréal. Nous nous intéressons à la question des îlots de chaleur urbains au niveau des aires de diffusion (AD – entités spatiales du recensement canadien comprenant de 400 à 700 habitants). Pour cela, nous avons calculé dans chaque AD le pourcentage de sa surface classifiée comme îlot de chaleur dans la carte des îlots de [chaleur/fraicheur](https://www.donneesquebec.ca/recherche/fr/dataset/ilots-de-chaleur-fraicheur-urbains-et-temperature-de-surface) réalisé par l’INSPQ et le CERFO.

La question que nous nous posons est la suivante : les populations vulnérables socio-économiquement et/ou physiologiquement sont-elles systématiquement plus exposées à la nuisance que représentent les îlots de chaleur ? Cette question se rattache donc au champ de la recherche sur la justice environnementale et plus spécifiquement sur sa dimension spatiale (à savoir l'équité environnementale, à distinguer des dimensions procédurale et de reconnaissance). Plusieurs études se sont d'ailleurs déjà penchées sur la question des îlots de chaleur abordée sous l'angle de l'équité environnementale [@harlan2007shade; @sanchez2019cooling; @huang2011everyone]. Nous modélisons donc pour chaque AD (n = 3158) de l’île de Montréal la proportion de sa surface couverte par des îlots de chaleur. Nos variables indépendantes sont divisées en deux catégories : variables environnementales et variables socio-économiques. Les premières sont des variables de contrôle, il s’agit de la densité de végétation dans l’AD (ajouté avec une polynomiale d'ordre deux) et de l'arrondissement dans lequel il se situe. Ces deux paramètres affectent directement les chances d’observer des îlots de chaleurs, mais nous souhaitons isoler leurs effets (toutes choses étant égales par ailleurs) de ceux des variables socio-économiques. Ces dernières ont pour objectif de cibler les populations vulnérables sur le plan physiologique (personnes âgées et enfants de moins de 14 ans) ou socio-économique (minorités visibles et faible revenu). L'ensemble de ces variables sont présentées dans le tableau \@ref(tab:variablebeta). Notez que puisque le modèle avec distribution de Beta ne peut pas prendre en compte des valeurs exactes de 1 ou 0, nous les avons remplacées respectivement par 0,99 et 0,01. Cette légère modification n'altère que marginalement les données, surtout si l'on considère qu'elles sont agrégées au niveau des AD et proviennent originalement d'imagerie satellitaire.

```{r variablebeta, echo=FALSE, message=FALSE, warning=FALSE, out.width='50%'}
df <- data.frame(
  C1 = c("A65PlusPct","A014Pct","PopFRPct","PopMVPct","VegPct","Arrond"),
  C2 = c("Population de 65 ans et plus", "Population de 14 ans et moins", "Population à faible revenu", "Minorités visibles", "Végétation", "Arrondissements"),
  C3 = c("Variable continue", "Variable continue", "Variable continue", "Variable continue", "Variable continue", "Variable continue"),
  C4 = c("Pourcentage de la population ayant 65 ans et plus", "Pourcentage de la population ayant 14 ans et moins", "Pourcentage de la population à faible revenu", "Pourcentage de la population faisant partie des minorités visibles", "Pourcentage de la surface de l’AD couverte par de la végétation", "Arrondissement de l'Île de Montréal"))

show_table(df, 
    caption = "Variables indépendantes utilisées dans le modèle Beta",
    col.names = c("Nom de la variable","Signification","Type de variable","Mesure"), 
    col.to.resize = c(2,4),
    col.width = "5cm"
)
```

**Vérification des conditions d'application**

Sans surprise, nous commençons par charger nos données et nous assurer de l'absence de multicolinéarité excessive entre nos variables indépendantes.

```{r message=FALSE, warning=FALSE}
## Chargement des données
dataset <- read.csv("data/glm/data_chaleur.csv",fileEncoding = "utf8")

## Calcul des valeurs de vif
vif(glm(hot ~
          A65Pct + A014Pct + PopFRPct + PopMVPct +
          poly(prt_veg, degree = 2)  + Arrond,
        data = dataset))
```

La seule variable semblant poser un problème de multicolinéarité est la variable `Arrond`. Cependant, du fait de sa nature multinomiale, elle regroupe en réalité 32 coefficients (voir la colonne `Df`). Il faut donc utiliser la règle habituelle de 5 sur le carré de la troisième colonne (`GVIF^(1/(2*Df))`) du tableau [@fox1992generalized], soit `1,032820^2 = 1.066717`, ce qui est bien inférieur à la limite de 5. Nous n'avons donc pas de problème de multicolinéarité excessive. Nous pouvons passer au calcul des distances de Cook. Pour ajuster notre modèle, nous utilisons le *package* **MGCV** et la fonction `gam` avec le paramètre `family = betar(link = "logit")`.

```{r cookdistbeta, message=FALSE, warning=FALSE, fig.cap = "Distances de Cook pour le modèle GLM Beta", fig.align = "center", out.width = "50%"}
# Ajustement d'une première version du modèle
modele <- gam(hot ~
                A65Pct + A014Pct + PopFRPct + PopMVPct +
                poly(prt_veg, degree=2) + Arrond,
              data = dataset, family = betar(link = "logit"))

# calcul des distances de Cook
df <- data.frame(
  cooksd = cooks.distance(modele),
  oid = 1:nrow(dataset)
)

# Affichage des distances de Cook
ggplot(data = df)+
  geom_point(aes(x = oid, y = cooksd),
             color = rgb(0.4,0.4,0.4,0.4), size = 0.5)+
  labs(x = "", y = "Distance de Cook")
```

Nous pouvons observer à la figure \@ref(fig:cookdistbeta) que seulement deux observations se distinguent très nettement des autres. Nous les isolons donc dans un premier temps.

```{r message=FALSE, warning=FALSE}
cas_etranges <- subset(dataset, df$cooksd >= 0.01)
print(cas_etranges[,23:ncol(cas_etranges)])
```

Ces deux observations n'ont pas de points communs marqués, et ne semblent pas avoir de valeurs particulièrement fortes sur les différentes variables indépendantes ou la variable dépendante. Nous décidons donc de les supprimer et de recalculer les distances de Cook.

```{r cookdistbeta2, message=FALSE, warning=FALSE, fig.cap = "Distances de Cook pour le modèle GLM Beta (suppression de deux observations influentes)", fig.align = "center", out.width = "50%"}
# Suppression des deux observations très influentes
dataset2 <- subset(dataset, df$cooksd < 0.01)

modele2 <- gam(hot ~
                A65Pct + A014Pct + PopFRPct + PopMVPct +
                I(prt_veg**2) + prt_veg + Arrond,
              data = dataset2, family = betar(link = "logit"), methode = "REML")

# calcul des distances de Cook
df2 <- data.frame(
  cooksd = cooks.distance(modele2),
  oid = 1:nrow(dataset2)
)

# Affichage des distances de Cook
ggplot(data = df2)+
  geom_point(aes(x = oid, y = cooksd),
             color = rgb(0.4,0.4,0.4,0.4), size = 0.5)+
  labs(x = "", y = "Distance de Cook")
```

Après réajustement (figure \@ref(fig:cookdistbeta2)) nous constatons à nouveau qu'une observation est **extrêmement** éloignée des autres. Nous la retirons donc également car cette différence est si forte qu'elle risque de polluer le modèle.

```{r cookdistbeta3, message=FALSE, warning=FALSE, fig.cap = "Distances de Cook pour le modèle GLM Beta (suppression de trois observations influentes)", fig.align = "center", out.width = "50%"}
# Suppression de l'observation très étonnante
dataset3 <- subset(dataset2, df2$cooksd<max(df2$cooksd))

# Réajustement du modèle
modele3 <- gam(hot ~
                A65Pct + A014Pct + PopFRPct + PopMVPct +
                I(prt_veg**2) + prt_veg + Arrond,
              data = dataset3, family = betar(link = "logit"), methode = "REML")

# Calcul des distances de Cook
df3 <- data.frame(
  cooksd = cooks.distance(modele3),
  oid = 1:nrow(dataset3)
)

# Affichage des distances de Cook
ggplot(data = df3)+
  geom_point(aes(x = oid, y = cooksd),
             color = rgb(0.4,0.4,0.4,0.4), size = 0.5)+
  labs(x = "", y = "Distance de Cook")
```

Tout semble aller pour le mieux après ce second passage (figure \@ref(fig:cookdistbeta3)). Si nous avions continué à observer des valeurs aussi influentes, nous aurions dû commencer à sérieusement questionner nos données ou notre modèle. La prochaine étape du diagnostic est donc l'analyse des résidus simulés.

```{r residsimbeta, message=FALSE, warning=FALSE, fig.cap = "Diagnostic général des résidus simulés du modèle Beta", fig.align = "center", out.width = "95%"}

# Extraction de phi
modele3$family$family
phi <- 14.612

# Réalisation des simulations
nsim <- 1000
mus <- modele3$fitted.values

cols <- lapply(1:length(mus),function(i){
  mu <- mus[[i]]
  p <- mu * phi
  q <- (1-mu)*phi
  sims <- rbeta(n = nsim,shape1 = p, shape2 = q)
  return(sims)
})

mat_sims <- do.call(rbind, cols)

# Calcul des résidus simulés
sim_res <- createDHARMa(simulatedResponse = mat_sims,
                           observedResponse = dataset3$hot,
                           fittedPredictedResponse = modele3$fitted.values,
                           integerResponse = F)

plot(sim_res)
```

La figure \@ref(fig:residsimbeta) indique que les résidus suivent bien une distribution uniforme. Le test des valeurs aberrantes n'est pas significatif au seuil 0,01 (nous retenons ce seuil considérant le grand nombre de simulations et d'observation de notre jeu de données), nous décidons de ne pas supprimer davantage d'observations. Le panneau de droite indique une relation non linéaire instable, mais essentiellement centrée sur la ligne droite attendue. Pour plus de détail, nous calculons ces résidus simulés avec chacune des variables indépendantes.

```{r residsimbeta2, message=FALSE, warning=FALSE, fig.cap = "Relation entre chaque variable indépendante et les résidus simulés du modèle Beta", fig.align = "center", out.width = "95%"}

# Préparons un plot multiple
par(mfrow=c(3,2))
vars <- c("A65Pct", "A014Pct", "PopFRPct", "PopMVPct", "prt_veg")

for(v in vars){
  plotResiduals(sim_res, dataset3[[v]], main = v)
}
plotResiduals(sim_res, dataset3[["prt_veg"]]**2, main = "prt_veg^2")
```

La figure \@ref(fig:residsimbeta2) indique des relations marginales et négligeables entre nos variables indépendantes et nos résidus simulés. Concernant la variable `Arrond` (figure \@ref(fig:residsimbeta3)), nous observons une situation plus particulière. Pour quelques arrondissements, les résidus simulés sont nettement plus forts ou plus faibles. Notre hypothèse est que cet effet est provoqué par l'introduction de cette variable dans notre modèle comme un effet fixe alors que sa nature devrait nous inciter à l'introduire comme un effet aléatoire. Nous n'avons pas encore présenté ces concepts ici, mais nous le ferons dans le chapitre \@ref(chap06). En attendant, nous conservons le modèle tel quel et passons à l'analyse de sa qualité d'ajustement.

```{r residsimbeta3, message=FALSE, warning=FALSE, fig.cap = "Relation entre la variable Arrondissement et les résidus simulés du modèle Beta", fig.align = "center", out.width = "95%"}
df <- data.frame(
  resid = residuals(sim_res),
  Arrond = dataset3$Arrond
)

ggplot(data = df) + 
  geom_boxplot(aes(x = Arrond, y = resid))+
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()
        )+
  labs(x = "Arrondissements", y = "Résidus simulés")
```

**Analyse de la qualité d'ajustement**

Dans un premier temps, nous comparons la distribution originale des données à des simulations issues du modèle.

```{r residsimbeta4, message=FALSE, warning=FALSE, fig.cap = "Comparaison entre la distribution originale et les simulations issue du modèle", fig.align = "center", out.width = "60%"}

# Extraction de 20 simulations
df2 <- data.frame(mat_sims[,1:20])
df3 <- reshape2::melt(df2)

ggplot() +
  geom_histogram(aes(x = hot, y = ..density..),
                 data = dataset3, bins = 100, color = "black", fill = "white") +
  geom_density(aes(x = value, y=..density.., group = variable), data = df3,
               fill = rgb(0,0,0,0), color = rgb(0.2,0.2,0.2,0.3), size = 1)
```

Nous constatons à la figure \@ref(fig:residsimbeta4) que le modèle est parvenu à reproduire la forme générale de la distribution originale : un plus grand nombre de valeurs proches de zéro, suivies d’une répartition presque homogène dans les valeurs comprises entre 0,15 et 0,8, suivies par un plus faible nombre de valeurs quand *Y* est supérieur à 0,8. Il semble en revanche manquer un certain nombre de valeurs extrêmes proches de 0 (absence d'îlot de chaleur) et proches de 1 (couverture à 100% par des îlots de chaleur).

```{r, message=FALSE, warning=FALSE}
# Calcul des pseudo R2
rsqs(loglike.full = modele3$deviance/-2,
     loglike.null = modele3$null.deviance/-2,
     full.deviance = modele3$deviance,
     null.deviance = modele3$null.deviance,
     nb.params = modele3$rank,
     n = nrow(dataset3))

# Calcul du RMSE
sqrt(mean((modele3$fitted.values - modele3$y)**2))
```
Le modèle parvient à expliquer 90% de la déviance totale et obtient des pseudo R^2^ très élevés. Il obtient cependant un RMSE de 0,10 soit une erreur quadratique moyenne de 10% dans la prédiction, ce qui est tout de même important. Le modèle ne semble pas souffrir particulièrement de surajustement comme les pseudo R^2^ auraient pu nous le laisser penser. 

L'ensemble des coefficients du modèle sont accessibles via la fonction `summary`. Pour rappel, il est nécessaire de les convertir avec la fonction exponentielle pour pouvoir les interpréter en termes de rapport de cote. À nouveau, nous proposons de construire dans un premier temps une figure pour observer l'effet des arrondissements.

```{r betaArrond, message=FALSE, warning=FALSE, fig.cap = "Rapports de cote pour les arrondissements dans le modèle Beta", fig.align = "center", out.width = "95%"}

# Identifier les coefficients pour les arrondissements
test <- grepl("Arrond",names(modele3$coefficients), fixed = T)

# Extraire les coefficients et les erreurs standards
coeffs <- modele3$coefficients[test]
err.std <- summary(modele3)$se[test]

# Créer un dataframe avec les rapports de cote et les intervalles de confiance
df <- data.frame(
  Arrond = gsub("Arrond","",names(coeffs), fixed = T),
  coeffs = coeffs,
  err.std = err.std,
  RC = exp(coeffs),
  lowerRC = exp(coeffs-1.96*err.std),
  upperRC = exp(coeffs+1.96*err.std)
)

# Retrouver l'arrondissement de référence
allArrond <- unique(dataset3$Arrond)
refArrond <- setdiff(allArrond, df$Arrond)

# Créer le graphique
ggplot(data = df) + 
  geom_errorbarh(aes(xmin = lowerRC, xmax = upperRC, y = reorder(Arrond,RC)))+
  geom_point(aes(x = RC, y = reorder(Arrond,RC)))+
  geom_vline(xintercept = 1, color = "red")+
  geom_text(aes(x = upperRC, y = reorder(Arrond, RC), 
                label = paste("RC : ",round(RC,2),sep="")), size = 3, nudge_x = 0.3)+
  labs(x = paste("Rapport de cote (Red : ",refArrond,')',sep=''),
       y = 'Arrondissement')
```

Nous constatons ainsi que seuls quelques arrondissements ont une différence d'exposition aux îlots de chaleurs significative au seuil 0,05 comparativement à Ahuntsic-Cartierville. Pour l'essentiel il s'agit d'arrondissements pour lesquels on observe des rapports de cote supérieurs à 1. Verdun, Lasalle et le Plateau Mont-Royal sont les arrondissements les plus touchés avec des chances d'observer des niveaux supérieurs de densité d'îlots de chaleur multipliés par 3,19, 2,89 et 2,74. Le reste des coefficients sont affichés dans le tableau \@ref(tab:coeffsbeta). 

Nous notons ainsi que le seul groupe associé avec une augmentation significative des chances d'observer une augmentation de la densité d'îlot de chaleur est le groupe des personnes à faible revenu (1,4% de chance supplémentaire à chaque augmentation d'un point de pourcentage de la variable indépendante). Pour mieux cerner la taille de cet effet, nous pouvons représenter l'effet marginal de ce coefficient en maintenant toutes les autres variables à leur moyenne. Nous allons également calculer ces effets marginaux pour trois arrondissements différents : Verdun (RC le plus fort), Ahuntsic-Cartierville (la référence) et Dollard-des-Ormeaux (RC le plus faible). Nous réalisons également un second graphique pour visualiser l'effet non linéaire de la variable *pourcentage de végétation*. La figure \@ref(fig:betaFR) nous indique ainsi que le rôle de l'arrondissement est plus important que celui du pourcentage de personnes à faible revenu. Cependant, on constate que passer de 0% de personnes à faible revenu dans un AD à 75% est associé avec une multiplication de la surface couverte par des îlots de chaleur par environ 1,5 (toutes choses égales par ailleurs). Le rôle de la végétation dans la réduction de la surface des îlots de chaleur est très net et non linéaire. L'essentiel de la réduction est observé entre 0 et 50% de végétation dans un AD, au-delà de ce seuil, la réduction des îlots de chaleur par la végétation est moins flagrante. Il semblerait donc exister à Montréal une forme d’iniquité systématique pour les populations à faible revenu, qui serait davantage exposées aux îlots de chaleur. Cependant, compte tenu de la dépendance spatiale et de l’hétéroscésadicité observée plus haut, des ajustements devraient être apportés au modèle pour confirmer ou infirmer ce résultat.

```{r coeffsbeta, warning = FALSE, message=FALSE, echo = F}
tableau <- build_table(modele3, coef_digits = 3, OR_digits = 3)

tableau_ok <- tableau[c(1:6),]
tableau_ok <- tableau_ok[,c(1,2,3,6,9,10,11)]

show_table(tableau_ok, 
    caption = "Résultats pour le modèle GLM Beta",
    col.names = c('Variable',"Coeff.","RC","Val.p","IC 2,5% RC","IC 97,5% RC", "Sign.")
)
```

```{r betaFR, message=FALSE, warning=FALSE, fig.cap = "Effets marginaux des variable pourcentage de personnes à faible revenu et densité de végétation", fig.align = "center", out.width = "95%"}

# Créer un dataframe pour la prédiction
df <- expand.grid(
  A65Pct = mean(dataset3$A65Pct),
  A014Pct = mean(dataset3$A014Pct),
  PopFRPct = seq(0,75, 1),
  PopMVPct = mean(dataset3$PopMVPct),
  prt_veg = mean(dataset3$prt_veg),
  Arrond = c("Verdun",'Ahuntsic-Cartierville','Dollard-des-Ormeaux')
)
# Effectuer les prédiction sur l'échelle log
pred <- predict(modele3, df, se=T, type = "link")

# Calculer les prédictions et leurs intervales de confiance
ilink <- modele3$family$linkinv
df$pred <- ilink(pred$fit)
df$lower <- ilink(pred$fit - 1.96* pred$se.fit)
df$upper <- ilink(pred$fit + 1.96* pred$se.fit)

# Afficher le résultat
P1 <- ggplot(data = df)+
  geom_path(aes(x = PopFRPct, y = pred, color = Arrond), size =1) + 
  geom_path(aes(x = PopFRPct, y = lower, color = Arrond), linetype="dashed") +
  geom_path(aes(x = PopFRPct, y = upper, color = Arrond), linetype="dashed")+
  labs(x = "Pourcentage de personnes à faible revenu", 
       y = "Surface de l'AD couverte par des îlots de chaleur (%)",
       color = 'Arrondissement')+
  ylim(0,1)

# Pour la végétation
df2 <- expand.grid(
  A65Pct = mean(dataset3$A65Pct),
  A014Pct = mean(dataset3$A014Pct),
  PopFRPct = mean(dataset3$PopFRPct),
  PopMVPct = mean(dataset3$PopMVPct),
  prt_veg = seq(0,95,1),
  Arrond = c("Verdun",'Ahuntsic-Cartierville','Dollard-des-Ormeaux')
)

# Effectuer les prédiction sur l'échelle log
pred2 <- predict(modele3, df2, se=T, type = "link")

# Calculer les prédictions et leurs intervales de confiance
df2$pred <- ilink(pred2$fit)
df2$lower <- ilink(pred2$fit - 1.96* pred2$se.fit)
df2$upper <- ilink(pred2$fit + 1.96* pred2$se.fit)

# Afficher le résultat
P2 <- ggplot(data = df2)+
  geom_path(aes(x = prt_veg, y = pred, color = Arrond), size =1) + 
  geom_path(aes(x = prt_veg, y = lower, color = Arrond), linetype="dashed") +
  geom_path(aes(x = prt_veg, y = upper, color = Arrond), linetype="dashed")+
  labs(x = "Pourcentage de couverture végétale", y = '', color = 'Arrondissement')+
  ylim(0,1)

ggarrange(P1,P2, common.legend = T)
```

## 	Conclusion sur les modèles linéaires généralisés

Comme vous avez dû le remarquer, les modèles linéaires généralisés constituent un monde à part entière et tout un livre pourrait être rédigé à leur sujet. Leur grande flexibilité les rend extrêmement utiles dans de nombreux contextes, mais complique leur mise en œuvre, chaque modèle ayant ses propres spécificités théoriques. Ils partagent cependant tous une base commune : le choix d’une distribution et d’une fonction de lien, l’ensemble de leurs spécificités découle directement de ces deux choix.

