{"title":"Méthodes de classification non supervisée","markdown":{"headingText":"Méthodes de classification non supervisée","headingAttr":{"id":"sec-chap13","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\nDans le cadre de ce chapitre, nous présentons les méthodes les plus utilisées en sciences sociales pour explorer la présence de groupes homogènes au sein d'un jeu de données, soit les méthodes de classification non supervisée. Le qualificatif *non supervisé* signifie que ces classes/groupes ne sont pas connus a priori et doivent être identifiés à partir des données. Autrement dit, nous cherchons à regrouper les observations partageant des caractéristiques similaires sur la base de plusieurs variables. Ces méthodes descriptives et exploratoires multivariées peuvent être vues comme une façon de réduire le nombre d'observations d'un jeu de données à un ensemble d'observations synthétiques, représentant le mieux possible la population à l’étude.\n\n::: bloc_package\n::: bloc_package-header\n::: bloc_package-icon\n:::\n**Liste des *packages* utilisés dans ce chapitre**\n:::\n::: bloc_package-body\n\n* Pour créer des graphiques :\n  - `ggplot2` le seul, l'unique!\n  - `ggpubr` pour combiner des graphiques et réaliser des diagrammes.\n* Outils généraux pour faciliter les classifications :\n  - `clusterCrit` pour calculer des indicateurs de qualité de classification.\n  - `NbClust` pour trouver le bon nombre de groupe dans une classification.\n  - `cluster` pour appliquer la méthode GAP.\n  - `proxy` pour calculer plusieurs types de distances.\n  - `Gmedian` pour calculer le k-médianes.\n  - `geocmeans` pour explorer les résultats de classifications floues.\n:::\n:::\n\n::: bloc_objectif\n::: bloc_objectif-header\n::: bloc_objectif-icon\n:::\n**Pourquoi recourir à des méthodes de classification non supervisée en sciences sociales?**\n:::\n::: bloc_objectif-body\nLes méthodes de classification sont très utilisées en sciences sociales. Elles visent à identifier des groupes cohérents au sein d'un ensemble d'observations sur la base de plusieurs variables (@fig-ClassifNonNSuperv). Ces groupes peuvent ensuite être analysés et nous renseigner sur les caractéristiques communes partagées par les individus qui les composent. \n\n![Principe de base des méthodes de classification non supervisée](images/Chap13/ClassifNonNSuperv.png){#fig-ClassifNonNSuperv width=\"40%\" fig-align=\"center\"}\n\nUn exemple classique est l'identification de profils d'individus ayant répondu à un sondage, en fonction de plusieurs caractéristiques (par exemple, l'âge, le sexe, la situation de famille, le revenu, etc.). En identifiant ces groupes homogènes, il est ensuite possible d'explorer les associations entre ces profils et d'autres variables.\n\nUn second exemple serait de regrouper les secteurs d'une ville selon leurs caractéristiques environnementales (végétation, niveau de bruit, pollution atmosphérique, etc.) et socioéconomiques (revenu médian des ménages, pourcentage d’immigrants, pourcentage de personnes à faible scolarité, taux de chômage, etc.).\n:::\n:::\n\n\n## Méthodes de classification : un aperçu {#sec-131}\n\nIl existe une multitude de méthodes de classification généralement regroupées dans plusieurs familles imbriquées à partir de deux distinctions importantes.\n\nLa première distinction vise à séparer les méthodes **supervisées** des **non supervisées**. Pour les premières, les catégories/groupes/classes des observations sont connues à l'avance. L'enjeu n'est pas de trouver les catégories puisqu’elles sont connues, mais **de déterminer des règles ou un modèle permettant d'attribuer des observations à ces catégories**. Parmi les méthodes de classification supervisée, les plus connues sont les forêts d’arbres décisionnels, les réseaux de neurones artificiels ou encore l’analyse factorielle discriminante. Nous n'abordons pas ces méthodes dans ce chapitre dédié uniquement aux méthodes de classification non supervisée. Pour ces dernières, les catégories ne sont pas connues à l'avance et l'enjeu est de **faire ressortir les structures des groupes propres aux données**. Ainsi, les méthodes de classification non supervisée « relèvent de la statistique exploratoire multidimensionnelle et permettent de classifier automatiquement les observations sans connaissance a priori sur la nature des classes présentes dans le jeu de données; les plus connues sont sans conteste les algorithmes de classification ascendante hiérarchique (CAH) et du *k-means* (k-moyennes) » [@2021_4, p.1]. Notez également qu'à la frontière entre ces deux familles, se situent les méthodes de classification semi-supervisée. Il s'agit de cas spécifiques où des informations partielles sont connues sur les groupes à détecter : seulement le groupe final de certaines observations est connu, certaines observations sont supposées appartenir à un même groupe même s'il est indéfini en lui-même [@bair2013semi].\n\nLa seconde distinction vise à séparer les méthodes **strictes** des **floues**. Les premières ont pour objectif d'assigner chaque observation à une et une seule catégorie, alors que les secondes décrivent le degré d’appartenance de chaque observation à chaque catégorie. Autrement dit, « dans une classification stricte, chaque observation appartient à une seule classe. Mathématiquement parlant, l’appartenance à une classe donnée est binaire (0 ou 1) tandis que dans une classification floue, chaque observation a une probabilité d’appartenance variant de 0 à 1 à chacune des classes » [@2021_4, p.1]. Bien entendu, pour chaque observation, la somme des degrés d’appartenance à chacune des classes est égale à 1 (@fig-floueVSstrict). En termes de données, cela signifie que pour les méthodes strictes, le groupe d'appartenance d'une observation est contenu dans une seule variable nominale (une colonne d'un *DataFrame*). Pour les méthodes floues, il est nécessaire de disposer d’autant de variables continues (plusieurs colonnes numériques d'un *DataFrame*), soit une par groupe, dans lesquelles sont enregistrées le degré d'appartenance de chaque observation à chacun des groupes. Parmi les méthodes de classification supervisée floue, notez que nous avons déjà abordé la régression logistique multinomiale dans le chapitre sur les GLM ([section @sec-0824]).\n\n![Classifications stricte et floue](images/Chap13/floueVSstricte.png){#fig-floueVSstrict width=\"35%\" fig-align=\"center\"}\n\nEn résumé, le croisement de ces deux distinctions permet ainsi de différencier les méthodes **supervisées strictes**, **supervisées floues**, **non supervisées strictes** et **non supervisées floues** (@fig-methoClassif), auxquelles s'ajoutent les méthodes semi-supervisées discutées brièvement.\n\n![Synthèse des principales méthodes de classification (Gelb et Apparicio 2021)](images/Chap13/syntheseClassif.png){#fig-methoClassif width=\"60%\" fig-align=\"center\"}\n\nDans ce chapitre, nous décrivons les trois méthodes de classification non supervisée les plus utilisées et faciles à mettre en œuvre : la classification ascendante hiérarchique, les nuées dynamiques strictes (*k-means* et *k-medians*) et nuées dynamiques floues (*c-means* et *c-medians*). \n\n## Notions essentielles en classification {#sec-132}\n\nAvant de décrire différentes méthodes de classification non supervisées, il convient de définir deux notions centrales, soit la **distance** et l'**inertie**.\n\n### Distance {#sec-1321}\n\nLa distance en analyse de données est définie comme une fonction (*d*) permettant de déterminer à quel point deux observations sont semblables ou différentes l'une de l'autre. Elle doit respecter les conditions suivantes :\n\n* **la non-négativité** : la distance minimale entre deux objets est égale à 0; $d(x,y) \\geq 0$.\n\n* **le principe d'identité des indiscernables** : la distance entre deux objets $x$ et $y$ est égale à 0, si $x = y$; $d(x,y)=0\\text{ si et seulement si }x=y$.\n\n* **la symétrie** : la distance entre $x$ et $y$ est la même qu'entre $y$ et $x$; $d(x,y) = d(y,x)$.\n\n* **le triangle d'inégalité** : passer d'un point $x$ à un point $z$ est toujours plus court ou égal que de passer par $y$ entre $x$ et $z$; $d(x,z)\\leq d(x,y)+d(y,z)$.\n\nIl existe un grand nombre de types de distance qui peuvent être utilisés pour déterminer le degré de similarité entre les observations. Nous présentons ici les six types les plus fréquemment utilisés en sciences sociales, mais retenez qu’il en existe bien d'autres.\n\n#### Distance euclidienne {#sec-13211}\n\nIl s'agit vraisemblablement de la distance la plus couramment utilisée, soit la longueur de la ligne droite la plus courte entre les deux objets considérés. Pour la représenter, admettons que nous nous intéressons à trois classes d'étudiants et d'étudiantes A, B et C pour lesquelles nous avons calculé la moyenne de leurs notes dans les cours de méthodes quantitatives et qualitatives. Ces deux variables sont mesurées dans la même unité et varient de 0 à 100. Le nuage de points à la @fig-dist0 illustre cette situation avec des données fictives.\n\n```{r}\n#| label: fig-dist0\n#| fig-cap: Situation de base pour le calcul de distance\n#| fig-align: center\n#| out-width: \"60%\"\n#| echo: false\n#| message: false\n#| warning: false\nlibrary(ggplot2)\nlibrary(dplyr)\ndf <- data.frame(\n  \"classe\" = c(\"A\" , \"B\" , \"C\"),\n  \"quantitative\" = c(85,80,83),\n  \"qualitative\" = c(80,79,77)\n)\n\nggplot(df) + \n  geom_point(aes(x = quantitative, y = qualitative, color = classe), size = 3) + \n  scale_color_manual(values = c(\"A\" = \"#E73D3D\", \"B\" = \"#3CE73C\", \"C\" = \"#3C3CE7\")) + \n  labs(x = \"Moyenne des notes en méthodes quantitatives\",\n       y = \"Moyenne des notes en méthodes qualitatives\")\n\n```\n\nLes distances euclidiennes entre les classes B et C et les classes C et A sont représentées par les lignes noires à la @fig-dist1. Nous pouvons constater que la distance entre les classes C et B est plus petite que celle entre les classes A et C, ce qui signale que les deux premières se ressemblent davantage.\n\n\n```{r}\n#| label: fig-dist1\n#| fig-align: center\n#| fig-cap: Représentation de la distance euclidienne\n#| out-width: \"60%\"\n#| message: false\n#| warning: false\n#| echo: false\n\ndf2 <- data.frame(\n  xstart = c(80,83),\n  xend = c(83,85),\n  ystart = c(79,77),\n  yend = c(77,80)\n)\n\nggplot(df) + \n  geom_segment(data = df2, mapping = aes(x = xstart, y = ystart, \n                                         xend = xend, yend = yend))+\n  geom_point(aes(x = quantitative, y = qualitative, color = classe), size = 3) + \n  scale_color_manual(values = c(\"A\" = \"#E73D3D\", \"B\" = \"#3CE73C\", \"C\" = \"#3C3CE7\")) + \n  labs(x = \"Moyenne des notes en méthodes quantitatives\",\n       y = \"Moyenne des notes en méthode qualitatives\")\n\n```\n\n\nLa formule de la distance euclidienne (@eq-euclideandist) est simplement la racine carrée de la somme des écarts au carré pour chacune des variables décrivant les observations *a* et *b*.\n\n$$ \nd(a,b) = \\sqrt{\\sum{}^v_{i=1}(a_i-b_i)^2}\n$$ {#eq-euclideandist}\n\navec *v* le nombre de variables décrivant les observations *a* et *b*.\n\nNous pouvons facilement calculer la distance euclidienne pour notre jeu de données : \n\n* $d(A,B)=\\sqrt{(\\mbox{85}-\\mbox{80})^2+(\\mbox{80}-\\mbox{77})^2} = \\mbox{5,83}$\n* $d(B, c)=\\sqrt{(\\mbox{80}-\\mbox{83})^2+(\\mbox{79}-\\mbox{77})^2} = \\mbox{3,60}$\n\n\n::: bloc_attention\n::: bloc_attention-header\n::: bloc_attention-icon\n:::\n**Distance et unité de mesure **\n:::\n::: bloc_attention-body\nIl est très important de garder à l'esprit que la distance entre deux observations dépend directement des unités de mesure utilisées. Cela est très souvent problématique, car il est rare que toutes les variables utilisées pour décrire des observations soient mesurées dans la même unité. Ainsi, une variable dont les valeurs numériques sont plus grandes risque de déséquilibrer les calculs de distance. À titre d'exemple, une variable mesurée en mètres plutôt qu'en kilomètres produit des distances euclidiennes 1000 fois plus grandes.\n\nIl est donc nécessaire de standardiser les variables utilisées avant de calculer des distances. Cette opération permet de transformer les variables originales vers une échelle commune. Plusieurs types de transformations peuvent être utilisés tels que décrits à la [section @sec-02552] :\n\n* **Le centrage et la réduction** qui consistent à soustraire de chaque valeur sa moyenne, puis à la diviser par son écart-type. La nouvelle variable obtenue s'exprime alors en écart-type (appelé aussi score-z). La formule de la transformation est $f(x) = \\frac{x - \\bar{x}}{\\sigma_x}$, avec $\\bar{x}$ la moyenne de $x$ et $\\sigma_x$ l'écart-type de $x$.\n\n* **La transformation sur une mise à l'échelle de 0 à 1** qui permet de modifier l'étendue d'une variable afin que sa valeur maximale soit de 1 et sa valeur minimale soit de 0. La formule de cette transformation est $f(x) = \\frac{x-min(x)}{max(x)-min(x)}$.\n\n* **La transformation en rang** qui consiste à remplacer les valeurs d'une variable par leur rang. La valeur la plus faible est remplacée par 1, et la plus forte par *n* (nombre d'observations). Notez que cette transformation modifie la distribution de la variable originale contrairement aux deux transformations précédentes. Cette propriété peut être désirable si les écarts absolus entre les valeurs ont peu d'importance, si la variable n'a pas été mesurée avec précision ou encore si des valeurs extrêmes sont présentes.\n\n* **La transformation en percentile** qui consiste à remplacer les valeurs d'une variable par leur percentile correspondant. Elle peut être vue comme une standardisation de la transformation en rang, car elle ne dépend pas du nombre d'observations.\n\nLa @fig-impactTransform montre l'effet de ces transformations sur l'histogramme d'une variable.\n\n```{r}\n#| label: fig-impactTransform\n#| echo: false\n#| fig-align: center\n#| fig-cap: Effets de différentes transformations sur la distribution d'une variable\n#| message: false\n#| warning: false\n#| out-width: \"80%\"\n\nx <- rgamma(10000, 0.95,0.1)\ndfx <- data.frame(\n  x = x,\n  xstd = (x - mean(x)) / sd(x),\n  x_01 = (x-min(x)) / (max(x) - min(x)),\n  x_rang = rank(x, ties.method = \"min\"),\n  x_prt = trunc(rank(x, ties.method = \"average\"))/length(x)\n)\n\nx2 <- reshape2::melt(dfx)\n\nx2$variable <- case_when(x2$variable == \"x\" ~ \"1-originale\",\n                         x2$variable == \"xstd\" ~ \"2-centrée-réduite\",\n                         x2$variable == \"x_01\" ~ \"3-mise à l'échelle 0-1\",\n                         x2$variable == \"x_rang\" ~ \"4-rang\",\n                         x2$variable == \"x_prt\" ~ \"5-percentile\",\n                         )\n\nggplot(x2) + \n  geom_histogram(aes(x = value), bins = 50, color = \"white\") + \n  facet_wrap(vars(variable), ncol = 2, scales = \"free\")\n\n```\n:::\n:::\n\n#### Distance de Manhattan {#sec-13212}\n\nCette seconde distance est également couramment utilisée. Elle doit son nom au réseau de rue de l'île de Manhattan qui suit un plan quadrillé. La distance de Manhattan correspond à la somme des écarts absolus entre les valeurs des différentes variables décrivant les observations (@eq-manhattandist). La @fig-dist2 illustre que la distance Manhattan (lignes noires) représente les deux côtés opposés de l'hypoténuse d’un triangle rectangle; l'hypoténuse représentant quant à elle la distance euclidienne.\n\n$$\nd(a,b) = \\sum{}^v_{i=1}(|a_i-b_i|)\n$$ {#eq-manhattandist}\n\n\n```{r}\n#| label: fig-dist2\n#| echo: false\n#| fig-align: center\n#| fig-cap: Représentation de la distance de Manhattan\n#| message: false\n#| warning: false\n#| out-width: \"60%\"\n\ndf2 <- data.frame(\n  xstart = c(85,83,80,80),\n  xend = c(83,83,80,83),\n  ystart = c(80,80,79,77),\n  yend = c(80,77,77,77)\n)\n\nggplot(df) + \n  geom_segment(data = df2, mapping = aes(x = xstart, y = ystart, \n                                         xend = xend, yend = yend))+\n  geom_point(aes(x = quantitative, y = qualitative, color = classe), size = 3) + \n  scale_color_manual(values = c(\"A\" = \"#E73D3D\", \"B\" = \"#3CE73C\", \"C\" = \"#3C3CE7\")) + \n  labs(x = \"Moyenne des notes en méthodes quantitatives\",\n       y = \"Moyenne des notes en méthode qualitatives\")\n\n```\n\nLa distance de Manhattan doit être privilégiée à la distance euclidienne lorsque les données considérées ont un très grand nombre de dimensions (variables). En effet, lorsque le nombre de variables est important (supérieur à 30), la distance euclidienne tend à être grande pour toutes les paires d’observations et à moins bien discriminer les observations proches et lointaines les unes des autres. Du fait de sa nature additive, la distance de Manhattan est moins sujette à ce problème [@aggarwal2001surprising].\n\nCalculons la distance de Manhattan pour nos deux paires d'observations :\n\n* $d(A,B)=|85-80|+|80-77| = 8$\n* $d(B, c)=|80-83|+|79-77| = 5$\n\n#### Distance du khi-deux {#sec-13213}\n\nLa distance du khi-deux est basée sur le test du khi-deux ([chapitre @sec-chap05]) et est généralement utilisée pour calculer la distance entre deux histogrammes, deux images ou deux ensembles de mots. Plus précisément, elle permet de mesure la distance entre deux observations A et B, pour lesquelles nous disposons d'un ensemble de variables étant toutes des variables de comptage.\n\nPrenons un exemple concret en générant trois histogrammes A, B et C sur l'intervalle [0,50] à partir des distributions normale, log-normale et Gamma, puis comptons le nombre de valeurs de chaque unité (1, 2, 3, 4, etc.). Ces histogrammes sont représentés à la @fig-dist3.\n\n```{r}\n#| label: fig-dist3\n#| echo: false\n#| fig-align: center\n#| fig-cap: Trois histogrammes pour illustrer le calcul de la distance du khi-deux\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\nset.seed(123)\nx1 <- rnorm(500, mean = 25, sd = 5)\nx2 <- rlnorm(500, meanlog = 2.75, sdlog = 0.25)\nx3 <- rgamma(500, 2,0.2)\n\ndf <- data.frame(\n  normal = x1,\n  lognormal = x2,\n  gamma = x3\n)\n\ndf2 <- reshape2::melt(df)\ndf2$distribution <- case_when(\n  df2$variable == \"normal\" ~ \"Normale\",\n  df2$variable == \"lognormal\" ~ \"Log-normale\",\n  df2$variable == \"gamma\" ~ \"Gamma\",\n)\ndf2$distribution <- as.factor(df2$distribution)\n\nggplot(df2) + \n  geom_histogram(aes(x = value), breaks = 0:50) +\n  facet_wrap(vars(distribution), ncol = 2) + \n  labs(x = \"\", y = \"\")\n```\n\nNous pouvons calculer les distances du khi-deux entre les paires d'histogrammes (@tbl-tabdist3). Nous constatons ainsi que les histogrammes B et C sont les plus semblables.\n\n```{r}\n#| label: tbl-tabdist3\n#| tbl-cap: Distance du khi-deux entre trois histogrammes\n#| echo: false\n#| message: false\n#| warning: false\n\nchi2dist <- function(x,y){\n  vec1 <- (x-y)**2\n  vec2 <- x+y\n  sum(vec1[vec2!=0] / vec2[vec2!=0])/2\n}\n\nA <- hist(df$normal, breaks = 0:50, plot = FALSE)$counts\nB <- hist(df$lognormal, breaks = 0:50, plot = FALSE)$counts\nC <- hist(df$gamma, breaks = 0:50, plot = FALSE)$counts\n\ntableau <- data.frame(\n  pair = c(\"A-B\", \"A-C\", \"B-C\"),\n  distances = c(chi2dist(A,B), chi2dist(A,C), chi2dist(B,C))\n)\n\nknitr::kable(tableau,\n            format.args = list(decimal.mark = ',', big.mark = \" \"),\n\t\t\t      col.names = c(\"Histogrammes\" , \"Distance du khi-deux\"),\n            align=c(\"l\", \"r\")\n           )\n```\n\nLa formule de cette distance est la suivante : \n\n$$ \nd_{\\chi^2}(a,b) = \\frac{1}{2}\\sum^n_{i=1}\\frac{(a_i-b_i)^2}{(a_i+b_i)}\n$$ {#eq-chi2dist}\n\navec $a_i$ et $b_i$ les comptages pour les histogrammes. Notez que si $a_i$ et $b_i$ valent tous les deux 0, il faut retirer ces valeurs avant le calcul, car cela provoquerait une division par 0.\n\nÀ première vue, cette distance peut paraître moins utile que les deux précédentes. Pourtant, de nombreuses données sont collectées comme des histogrammes. Un premier exemple serait des images que nous pouvons représenter sous forme de trois histogrammes, un pour chaque canal de couleur (rouge, vert et bleu). Un second exemple serait des données sonores, souvent synthétisées sous forme d'histogrammes des fréquences sonores enregistrées (octaves ou tiers d'octaves). Un dernier exemple pourrait être le nombre d'accidents de la route enregistré à diverses intersections d'une ville chaque heure. Dans ce contexte, un histogramme serait formé par l'intersection avec les heures de la journée comme limites des bandes et le nombre d'accidents comme hauteur des bandes.\n\n#### Distance de Mahalanobis {#sec-13214}\n\nProposée dans les années 1930 par le statisticien indien Prasanta Chandra Mahalanobis [-@chandra1936generalised], cette distance se base sur la matrice de covariance des variables analysées. Plus spécifiquement, elle est utilisée pour calculer la distance entre un point et une distribution normale multivariée. Elle permet notamment de tenir compte du fait que certaines variables sont corrélées et ainsi d'éviter de surestimer les distances entre des observations dans des jeux de données comprenant des variables corrélées entre elles.\n\nLa formule permettant de calculer cette distance est la suivante : \n\n\n$$ \nd(a,b) = \\sqrt{(a-b)^TS^{-1}(a-b)}\n$$ {#eq-mahalanobis}\n\navec *S* étant la matrice de covariance.\n\n#### Distance de Hamming {#sec-13215}\n\nCette distance est utilisée quand les écarts entre les variables de deux observations sont uniquement binaires. Un bon exemple serait un jeu de données ne comprenant que des variables qualitatives pouvant avoir une valeur identique pour deux observations (distance = 0) ou différente (distance = 1). La distance de Hamming est la simple addition de ces écarts.\n\nPrenons un exemple très simple en prenant trois maisons pour lesquelles nous connaissons cinq caractéristiques (@tbl-dist4).\n\n```{r}\n#| label: tbl-dist4\n#| echo: false\n#| tbl-cap: Exemple de données pour la distance de Hamming\n#| message: false\n#| warning: false\n\ntableau <- data.frame(\n  couleur = c(\"blanc\", \"blanc\", \"rouge\"),\n  jardin = c(\"non\", \"non\", \"oui\"),\n  garage = c(\"oui\" , \"non\" , \"oui\"),\n  cheminee = c(\"oui\" , \"oui\" , \"non\"),\n  cave = c(\"non\", \"non\", \"oui\")\n)\n\nknitr::kable(tableau,\n            format.args = list(decimal.mark = ',', big.mark = \" \"),\n\t\t\t      col.names = c(\"couleur\", \"jardin\", \"garage\", \"cheminée\", \"sous-sol\"),\n            align = c(\"l\", \"l\", \"l\", \"l\", \"l\")\n           )\n```\n\nNous pouvons utiliser la distance de Hamming pour estimer le niveau de dissimilarité entre ces différentes maisons et l'organiser dans une matrice de distances. À la lecture du @tbl-dist5), les maisons 2 et 3 sont les plus dissimilaires (distance de Hamming = 5), et les maisons 1 et 2 les plus similaires (distance de Hamming = 1).\n\n```{r}\n#| label: tbl-dist5\n#| tbl-cap: Distance de Hamming entre les maisons\n#| echo: false\n#| message: false\n#| warning: false\n\nhouse_dists <- apply(tableau,1, function(x){\n  dists <- apply(tableau, 1, function(y){\n    sum(x != y)\n  })\n return(dists) \n})\n\nrow.names(house_dists) <- c(\"maison 1\" , \"maison 2\" , \"maison 3\")\n\nknitr::kable(house_dists,\n          format.args = list(decimal.mark = ',', big.mark = \" \"),\n\t\t      row.names = TRUE,\n          col.names = c(\"maison 1\" , \"maison 2\" , \"maison 3\"),\n          align = c(\"l\", \"c\", \"c\", \"c\")\n          )\n\n```\n\n#### Distance de Gower {#sec-13216}\n\nLa distance de Gower [-@gower1971general] peut être utilisée pour mesurer la distance entre deux observations lorsque les données sont à la fois qualitatives et quantitatives. Cette distance est comprise dans un intervalle de 0 à 1, 0 signifiant que les deux observations sont identiques et 1, que les observations sont radicalement différentes.\n\nElle se calcule de la façon suivante : \n\n$$\n\\begin{aligned}\n&d(a,b) = 1-\\frac{1}{p}\\sum^p_{j=1}s_{12j}\\\\\n&\\left\\{\\begin{array}{c}\ns_{xyj} = 1 \\text{ si } x_j = y_j \\text{, 0 autrement pour une variable qualitative} \\\\\ns_{xyj} =  1 - \\frac{|x_j-y_j|}{max(j)-min(j)} \\text{ pour une variable quantitative}\n\\end{array}\\right.\n\\end{aligned}\n$$ {#eq-gower}\n\n\navec *p* le nombre de variables, *x* et *y* deux observations et *j* une variable.\n\nAutrement dit, si la valeur d'une variable qualitative diffère entre deux observations, la distance entre ces deux observations augmente de $1/p$. Pour une variable quantitative, la distance augmente selon la différence absolue entre les valeurs de la variable divisée par l'étendue totale de la variable, le tout à nouveau divisé par *p*.\n\nSi cette mesure semble intéressante puisqu'elle permet de combiner des variables quantitatives et qualitatives, elle souffre de deux limites importantes :\n\n* Elle ne prend pas en compte le fait que certaines modalités des variables qualitatives sont moins fréquentes ni que certaines combinaisons sont également moins fréquentes.\n\n* Les variables qualitatives tendent à affecter bien plus la distance que les variables quantitatives. En effet, pour obtenir un écart de 1 sur une variable quantitative, il faut que les deux valeurs soient respectivement le maximum et le minimum de cette variable.\n\n::: bloc_aller_loin\n::: bloc_aller_loin-header\n::: bloc_aller_loin-icon\n:::\n**D'autres distances pour des données mixtes**\n:::\n::: bloc_aller_loin-body\nIl existe bien d'autres distances qui peuvent être utilisées dans le cas de données mixtes. Le *package* `kmed` en implémente cinq (auxquelles s'ajoute la distance de Gower) dans sa fonction `distmix` : les distances de Wishart, de Podani, d'Huang, d'Harikumar et d'Ahmad. Ces différentes distances ont toutes leurs avantages et leurs défauts respectifs; pour plus d'information, référez-vous à la documentation de la fonction `distmix`.\n:::\n:::\n\n#### Distance du Phi^2^ {#sec-13217}\n\nLa distance du $\\Phi^2$ (Phi^2^) est une variante de la distance du $\\chi^2$. Il s'agit donc d'une distance à utiliser lorsque les données à analyser sont uniquement qualitatives. Elle calcule la distance entre deux observations en additionnant les différences entre les valeurs de chaque variable (1 si différentes, 0 si identiques, pour chaque variable), divisées respectivement par la fréquence totale d'occurrences de chaque modalité dans le jeu de données. En d'autres termes, cette distance tient compte du fait que certaines valeurs pour des variables qualitatives peuvent être observées plus fréquemment que d'autres et qu'une distance plus grande devrait être obtenue entre deux observations si l'une des deux présente des modalités rares comparativement au reste du jeu de données.\n\nElle peut être calculée de la façon suivante : \n\n$$\nd_{\\Phi^2}(i,j) = \\frac{1}{Q}\\sum_k\\frac{(\\delta_{ik} - \\delta_{jk})^2}{f_k}\n$$ {#eq-phidist}\n\navec *i* et *j* deux observations, *k* une modalité d'une variable qualitative, *Q* le nombre total de modalités des variables qualitatives, $\\delta_{ik} = 1$ si l'observation i a la modalité *k*, 0 sinon et $f_k$ la fréquence de la modalité *k* dans le jeu de données.\n\nLa distance du $\\Phi^2$ est très utile pour analyser les résultats de questionnaires.\n\n### Inertie {#sec-1322}\n\nUne notion importante à saisir dans le cadre des méthodes de classification non supervisée est celui celle l'**inertie** d'un jeu de données. Elle est proche de la notion de variance qui a été présentée dans le chapitre sur la statistique univariée ([section @sec-0253]).\n\nL'inertie est une quantité permettant de décrire la dispersion des observations d'un jeu de données. Cette mesure dépend à la fois des données (nombres d'observations et de variables, échelle des variables) et de la mesure de distance retenue entre deux observations. Plus spécifiquement, l'inertie correspond à la somme des distances entre chaque observation et le centre du jeu de données. \n\n$$\ninertie= \\sum{}^n_{i=1} d(c,x_i)\n$$ {#eq-inertia}\n\navec *c* le centre du jeu de données, *n* le nombre d'observations, *x* une observation et *d* la fonction calculant la distance entre deux observations.\n\nL'enjeu est de définir *c* dans un contexte où la distance euclidienne est utilisée. Il s'agit simplement d'une observation fictive dont les coordonnées sont les moyennes des différentes variables du jeu de données. Dans le cas d'autres distances, il peut s'agir de l'observation minimisant la distance à toutes les autres observations.\n\nPour bien visualiser la notion d'inertie, prenons une fois encore le jeu de données `IRIS` comme exemple. Admettons que nous ne nous intéressons qu'à deux variables de ce jeu de données : `sepal.Length` et `sepal.Width`. Nous pouvons représenter l'inertie totale du jeu de données à la @fig-dist6.\n\n\n```{r}\n#| label: fig-dist6\n#| echo: false\n#| fig-align: center\n#| fig-cap: Représentation de l'inertie du jeu de données IRIS\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\ndata(iris)\nggplot(iris) + \n  geom_segment(aes(x = Sepal.Length, y = Sepal.Width,\n                   xend = mean(Sepal.Length),\n                   yend = mean(Sepal.Width)),\n               color = \"blue\") +\n  geom_point(aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(x = mean(Sepal.Length), y = mean(Sepal.Width)), color = \"red\", size = 3)\n\n```\n\nChaque ligne bleue représente la contribution de chaque point à l'inertie totale du jeu de données. Pour chaque iris, nous connaissons son espèce (Setosa, Versicolor ou Virginica). Nous pouvons donc attribuer chaque point de ce jeu de données à un groupe (une espèce dans notre cas). Il devient alors possible de calculer l'inertie de chacun des sous-groupes de notre jeu de données. Pour cela, nous devons calculer le centre de chaque groupe (généralement les moyennes des variables des observations au sein d'un groupe) et ensuite calculer l'inertie entre chaque observation et le centre de son groupe. Nous représentons cette situation à la @fig-dist7.\n\n```{r}\n#| label: fig-dist7\n#| echo: false\n#| fig-align: center\n#| fig-cap: Représentation de l'inertie par groupe pour le jeu de données IRIS\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\nlibrary(gganimate)\n\nspec_means <- iris %>% \n  group_by(Species) %>% \n  summarise_all(mean)\n\nnames(spec_means)[2:ncol(spec_means)] <- paste(names(spec_means)[2:ncol(spec_means)],\".end\", sep = \"\")\n\niris2 <- merge(iris, spec_means, by = \"Species\")\n\nggplot(iris2) + \n  geom_segment(aes(x = Sepal.Length, y = Sepal.Width,\n                   xend = Sepal.Length.end,\n                   yend = Sepal.Width.end,\n                   color = Species)) +\n  geom_point(aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(data = spec_means,\n             mapping = aes(x = Sepal.Length.end, y = Sepal.Width.end, fill = Species),\n             size = 3, colour = \"black\", pch = 21)\n\n```\n\nCette inertie propre aux groupes est toujours inférieure ou égale à l'inertie totale du jeu de données. Il s'agit en réalité de l'inertie que la structure de groupe n'est pas en mesure d'expliquer. En utilisant ces concepts, il est possible de calculer la part de l'inertie totale expliquée par les groupes (@eq-explainedinertia) : \n\n$$ \n\\text{inertie expliquée} = 1-\\frac{\\text{inertie totale}}{\\text{inertie restante}}\n$$ {\\#eq-explainedinertia}\n\nCette valeur nous renseigne sur la capacité d'une classification à bien réduire l'inertie totale d'un jeu de données. Elle est comprise entre 0 et 1. Si l'inertie expliquée est à 0, c'est que la classification n'explique absolument aucune part de l'inertie totale. Si l'inertie expliquée est à 1, la classification utilisée explique l'intégralité de l'inertie, ce qui en pratique n'est atteignable que si le nombre de groupes de la classification est égal au nombre d'observations. En d'autres termes, chaque observation est attribuée à un groupe dont elle est la seule représentante. Un telle situation n'a aucun intérêt puisque l'objectif d'une classification est bien de réduire la complexité d'un jeu de données en regroupant les observations.\n\n\n## Classification ascendante hiérarchique {#sec-133}\n\nLa classification ascendante hiérarchique (CAH) est un algorithme de classification non supervisée dont l'objectif est de créer un arbre de classification des observations. Cet arbre est ensuite utilisé pour déterminer le nombre de groupes à former et à quel groupe appartient chaque observation.\n\n### Fonctionnement de l'algorithme {#sec-1331} \n\nLa classification ascendante hiérarchique est un algorithme permettant de regrouper les observations d'un jeu de données de façon itérative. À chaque itération, deux observations similaires sont agrégées en un groupe représenté par le point central entre les deux observations. Le processus est ensuite répété en considérant le nouveau point comme une observation jusqu'à ce que toutes les observations soient fusionnées en un seul groupe.\n\nCes regroupements successifs créent un arbre de classification appelé dendrogramme. La racine de cet arbre est le groupe unique fusionnant toutes les observations, et ses branches correspondent aux différentes agrégations effectuées jusqu'aux observations individuelles. Cet arbre peut être vu comme une hiérarchie de classification. Chaque niveau de l'arbre est un regroupement de plus en plus généraliste au fur et à mesure que nous nous approchons de sa racine.\n\nPour appliquer cette méthode, il est nécessaire de sélectionner une **fonction de distance** pour mesurer la dissimilarité ou la ressemblance entre deux observations. L'algorithme fonctionne avec n'importe quelle fonction de distance, ce qui permet de l'appliquer aussi bien à des données qualitatives que quantitatives. En effet, l'opération de regroupement des observations se base sur une matrice de distance, soit un tableau de taille *n x n* indiquant pour chaque paire d'observations leur degré de dissimilarité. La @fig-tablvsmat illustre cette transformation en appliquant la distance du $\\Phi^2$ à un jeu de données comprenant cinq observations et 5 variables qualitatives.\n\n```{r}\n#| label: fig-tablvsmat\n#| echo: false\n#| fig-align: center\n#| fig-cap: Du tableau de données à la matrice de distance\n#| message: false\n#| warning: false\n#| out-width: \"80%\"\n\nlibrary(gridExtra)\nlibrary(grid)\nsource(\"code_complementaire/classif_helper.R\")\n\n\ndf1 <- data.frame(\n  couleur = c(\"blanc\", \"blanc\", \"rouge\", \"bleu\", \"rouge\"),\n  jardin = c(\"non\", \"non\", \"non\", \"oui\", \"non\"),\n  garage = c(\"oui\" , \"non\" , \"oui\", \"oui\", \"oui\"),\n  cheminee = c(\"oui\" , \"oui\" , \"non\", \"non\", \"non\"),\n  cave = c(\"non\", \"non\", \"non\", \"oui\", \"non\")\n)\n\ndf2 <- df1\n\nfor(col in names(df2)){\n  df2[[col]] <- paste(col, df2[[col]], sep = \"_\")\n}\n\ndist_mat <- round(Phi2dist(df2),2)\ndist_mat <- ifelse(is.na(dist_mat),0, dist_mat)\nrownames(df1) <- paste(\"maison \", 1:nrow(df1), sep = \"\")\nrownames(dist_mat) <- paste(\"maison \", 1:nrow(df1), sep = \"\")\ncolnames(dist_mat) <- paste(\"maison \", 1:nrow(df1), sep = \"\")\n\ntheme_table <- ttheme_default()\ntheme_table$colhead$fg_params$fontface <- 3\ntheme_table$colhead$bg_params$fill <- \"white\"\n\ng1 <- tableGrob(df1)\ng2 <- tableGrob(dist_mat, theme = theme_table)\n\ngrid.arrange(rectGrob(), rectGrob(), nrow = 2, ncol = 1)\ngrid.arrange(g1, g2, nrow = 2, newpage = FALSE)\n```\n\n\nEn plus de la fonction de distance, il est également nécessaire de sélectionner un **critère d'agrégation**, soit la règle permettant de décider à chaque itération quelles observations doivent être regroupées. Les méthodes les plus courantes sont : \n\n* Le critère de Ward [-@ward1963hierarchical] : cette méthode consiste à agréger à chaque itération les deux observations permettant de minimiser la variance (ou l'inertie) intra-groupe, ce qui revient à maximiser l'inertie inter-groupe (autrement dit, à rendre les groupes les plus homogènes possibles et les plus dissemblables entre eux). Ainsi, l'enjeu est de fusionner les deux observations permettant d'avoir les groupes les plus dissimilaires possible après fusion.\n\n* Le lien complet : à chaque itération, les deux groupes d'observations associés sont ceux pour lesquels la distance maximale entre les observations les composant est la plus petite parmi tous les groupes.\n\n* Le lien simple : à chaque itération, les deux groupes d'observations associés sont ceux pour lesquels la distance minimum entre les observations les composant est la plus petite parmi tous les groupes.\n\nLa plus utilisée est de loin la méthode de Ward. La méthode du lien complet produit généralement des résultats similaires. En revanche, la méthode du lien simple peut produire des groupes non sphériques (non centrés sur leur moyenne) plus difficile à interpréter.\n\nPrenons un instant pour visualiser cet algorithme (@fig-animhclust). Cette animation a été réalisée par David Sheehan et est également accessible sur son [blog](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/). Elle présente bien le processus d'agglomération de la classification ascendante hiérarchique et la construction progressive du dendrogramme.\n\n![Principe de fonctionnement de la classification ascendante hiérarchique (auteur : David Sheehan)](images/Chap13/CAH.gif){#fig-animhclust width=\"75%\" fig-align=\"center\"}\n\n### Choisir le bon nombre de groupes {#sec-1332}\n\nUne fois que l'algorithme a été appliqué aux données et le dendrogramme obtenu, il faut encore choisir le nombre optimal de groupes pour la classification finale. Chaque embranchement du dendrogramme constitue une classification possible, allant de la plus complexe (chaque observation appartient à un groupe formé d'elle seule) à la plus simple (toutes les observations appartiennent au même groupe). Si le nombre de groupes n'est pas connu à l'avance et qu'aucune forte justification théorique n'existe, il est possible d'utiliser plusieurs techniques pour déterminer un nombre de groupes judicieux à partir des données. Nous en présentons ici trois, mais il convient de ne pas s'en tenir uniquement à ses critères arbitraires. Il est important d'explorer les résultats de la classification obtenue pour plusieurs valeurs de *k* candidates et de tenir compte de la qualité des informations qu'elles fournissent. Au final, il est pertinent de retenir la classification dont les résultats offrent l'interprétation la plus claire avec un nombre de groupes réduit (principe de parcimonie).\n\n#### Méthode du coude {#sec-13321}\n\nCette première approche est la plus simple à mettre en œuvre. Il s'agit simplement de produire plusieurs classifications à partir du dendrogramme avec différentes valeurs de *k* (nombre de groupes) et de calculer à chaque fois la part de l'inertie expliquée. Chaque groupe supplémentaire ne peut qu'améliorer l'inertie expliquée, car pour rappel, si $k=n$, alors nous expliquons 100 % de l'inertie totale. L'objectif est de déterminer à quel moment l'ajout d'un groupe supplémentaire ne contribue que de façon marginale à améliorer l'inertie expliquée. Si nous représentons les valeurs d'inertie expliquée pour les différentes valeurs de *k* dans un graphique, une rupture (un coude) indiquerait le point au-delà duquel les groupes supplémentaires ne captent finalement que du bruit et non plus de l'information.\n\nSi nous reprenons l'exemple du jeu de données `IRIS`, nous pouvons créer ce graphique avec *k* allant de 2 à 8 (@fig-kmeans2). Un premier coude très net est observable pour $k = 3$ et un second plus faible, mais tout de même marqué pour $k = 4$. \n\n```{r}\n#| label: fig-kmeans2\n#| echo: false\n#| fig-align: center\n#| fig-cap: Méthode du coude\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\nks <- 2:8\ndata(iris)\nX <- as.matrix(iris[,1:4])\nD <- dist(X)\n\nclust <- hclust(D)\n\n## calcul de l'inertie totale\ncenter <- apply(X, MARGIN = 2, mean)\n\ninert_tot <- sum((t(X) - as.numeric(center))**2)\n\nrows <- t(sapply(ks, function(k){\n  classif <- cutree(clust, k)\n  \n  ## calcul du centre des groupes\n  X2 <- as.data.frame(X)\n  X2$gp <- as.character(classif)\n  centers <- X2 %>% \n    group_by(gp) %>%\n    summarize_all(mean)\n  \n  ## calcul de l'inertie intra-classe\n  inert_intra <- sapply(unique(X2$gp), function(g){\n    elems <- subset(X2,X2$gp == g)\n    elems$gp <- NULL\n    x <- as.matrix(elems)\n    center <- subset(centers, centers$gp == g)\n    center$gp <- NULL\n    y <- as.numeric(center)\n    sum((t(x) - y)**2)\n  })\n  \n    \n  exp_inertia <- 1-(sum(inert_intra)/inert_tot)\n  return(c(k, exp_inertia))\n}))\n\nrows <- as.data.frame(rows)\nnames(rows) <- c(\"k\" , \"exp_inertia\")\n\nggplot(rows) + \n  geom_path(aes(x = k, y = exp_inertia)) + \n  geom_point(aes(x = k, y = exp_inertia), color = \"red\") + \n  scale_x_continuous(breaks = 2:8)+\n  labs(x = \"nombre de groupes\", \n       y = \"inertie expliquée\")\n\n```\n\n\n::: bloc_notes\n::: bloc_notes-header\n::: bloc_notes-icon\n:::\n**Inertie expliquée et centre de groupe**\n:::\n::: bloc_notes-body\nPour calculer l'inertie expliquée, il est nécessaire de pouvoir déterminer pour le centre de gravité (ou centroïde) chaque groupe. Lorsque la distance euclidienne est utilisée, il s'agit simplement de calculer pour chaque groupe la valeur moyenne des différentes colonnes des observations. Cependant, lorsque d'autres distances sont utilisées, il peut être plus difficile de déterminer le centre d'un groupe. Avec la distance de Manhattan, il est par exemple recommandé d'utiliser la médiane des colonnes plutôt que la moyenne. Pour la distance de Hamming, la moyenne peut aussi être utilisée, car elle représente pour cette distance la fréquence d'occurrence des différentes modalités des variables qualitatives. Pour d'autres distances plus complexes, il est préférable de définir le centre d'un groupe comme le point de ce groupe minimisant les distances à tous les autres points du groupe. Il s'agit du médoïde du groupe.\n:::\n:::\n\n#### Indicateur de silhouette {#sec-13322}\n\nSi un coude net ne s'observe pas pour la méthode précédente, il est possible d'utiliser l'indicateur de silhouette. Il permet de mesurer pour une classification à quel point une observation est similaire à celles dans son propre groupe (cohésion) comparativement aux observations des autres groupes. Elle se calcule de la façon suivante : \n\n$$\n\\begin{aligned}\ns(i) &= \\frac{b(i)-a(i)}{\\max \\{a(i), b(i)\\}} \\\\\na(i) &= \\frac{1}{|C_i|-1}\\sum_{j \\in C_i,i \\neq j}d(i,j) \\\\\nb(i) &= min_{i \\neq j}\\frac{1}{|C_j|}\\sum_{j \\in C_j}d(i,j)\n\\end{aligned}\n$$ {#eq-silhouetteidx}\n\navec $s(i)$ la valeur de l'indice de silhouette pour l'observation *i*, $a(i)$ la distance moyenne entre l'observation *i* et son groupe $C_i$ et $b(i)$ la distance minimale entre l'observation *i* et le centre de chaque autre groupe $C_j$.\n\n\nLa valeur totale de l'indice est simplement la moyenne des valeurs moyennes des indices de silhouette au sein de chaque groupe. Une valeur plus élevée indique une meilleure classification. Il est nécessaire de déterminer le centre des groupes pour calculer cet indicateur, ce qui peut être un exercice difficile quand une distance autre que la distance euclidienne est utilisée. Référez-vous à la note de la section précédente pour plus d'informations. L'indice de silhouette semble indiquer que seulement trois groupes serait un choix optimal, soit la valeur la plus haute (@fig-kmeans3).\n\n```{r}\n#| label: fig-kmeans3\n#| echo: false\n#| fig-align: center\n#| fig-cap: Méthode de l'indice de silhouette\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\nlibrary(clusterCrit)\nks <- 2:8\n\nrows <- t(sapply(ks, function(k){\n  classif <- cutree(clust, k)\n  sil <- intCriteria(X, classif, crit = \"Silhouette\")$silhouette\n  return(c(k, sil))\n}))\n\nrows <- as.data.frame(rows)\nnames(rows) <- c(\"k\" , \"silhouette\")\n\nggplot(rows) + \n  geom_path(aes(x = k, y = silhouette)) + \n  geom_point(aes(x = k, y = silhouette), color = \"red\") + \n  labs(x = \"nombre de groupes\", \n       y = \"Indice de Silhouette\")\n```\n\n\n#### Méthode GAP {#sec-13323}\n\nCette méthode, proposée par @tibshirani2001estimating, consiste à comparer l'inertie intra-groupe (inexpliquée) avec l'inertie observée pour un jeu de données généré aléatoirement (distribution uniforme des valeurs entre le minimum et le maximum de chaque variable) pour différentes valeurs successives de *k*. Une fois ces calculs effectués, l'objectif est de trouver la valeur de *k* telle que la valeur de GAP à *k + 1* n'est pas plus grande qu'un écart type pour GAP à *k + 1*.\n\nLa statistique GAP est calculée ainsi : \n\n$$\n\\begin{aligned}\nGAP(k) = \\frac{1}{\\text{nsim}} \\sum^{\\text{nsim}}_{\\text{sim} = 1} log(W_{ksim}) - log(W_k)\n\\end{aligned}\n$$ {#eq-gapidx}\n\navec $W_k$ l'inertie non expliquée (intra-groupe), $W_{ksim}$ l'inertie non expliquée (intra-groupe) obtenue pour un jeu de données simulé et *k* le nombre de groupes.\n\nL'idée est qu'une bonne classification doit produire des résultats plus structurés que ce que nous pourrions attendre du hasard. Chaque groupe supplémentaire permet de réduire l'inertie, mais lorsque l'ajout d'un groupe ne permet pas un gain significatif comparativement au hasard, alors l'ajout de ce groupe ne se justifie pas. À nouveau, il est possible de visualiser la situation avec un simple graphique (@fig-kmeans4). Selon cette méthode, il faudrait sélectionner quatre groupes, car il s'agit de la première valeur de *k* validant le critère de cette méthode. La seconde valeur retenue par cette méthode est 6.\n \n\n```{r}\n#| label: fig-kmeans4\n#| echo: false\n#| fig-align: center\n#| fig-cap: Méthode GAP\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\nlibrary(NbClust)\nlibrary(cluster)\n\nhclustfun <- function(x, k){\n  D <- dist(x)\n  clust <- hclust(D)\n  classif <- cutree(clust, k)\n  return(list(\n    \"cluster\" = classif\n  ))\n}\n\nvals <- clusGap(X, hclustfun, K.max = 8)\ntab <- data.frame(vals$Tab)\ntab$k <- 1:nrow(tab)\n\nis_valid <- sapply(2:nrow(tab), function(i){\n  tab[i-1,\"gap\"] >= (tab[i,\"gap\"] - tab[i,\"SE.sim\"])\n})\n\nvalids <- subset(tab, is_valid)[1,]\nvalids2 <- subset(tab, is_valid)[2,]\n\nggplot(tab[2:8,]) + \n  geom_line(aes(x = k, y = gap)) + \n  geom_segment(x = valids$k, xend = valids$k, y = min(tab$gap), yend = valids$gap, \n               linetype = \"dashed\") +\n  geom_segment(x = valids2$k, xend = valids2$k, y = min(tab$gap), yend = valids2$gap, \n               linetype = \"dashed\") +\n  geom_point(aes(x = k, y = gap), color = \"red\") + \n  scale_x_continuous(breaks = 1:10) + \n  labs(x = \"nombre de groupes\", y = \"GAP\")\n\n```\n\n### Limites de la classification ascendante hiérarchique {#sec-1333}\n\nBien que très flexible (choix de la fonction de distance et du critère d'agrégation), la CAH fait face à un enjeu majeur : la vitesse d'exécution et la consommation de mémoire lorsque de grands jeux de données sont utilisés. En effet, il est nécessaire de calculer à chaque étape une matrice de distance entre les groupes. Si un jeu de données comprend 1000 observations, cette matrice comprend donc 1000 x 1000 cases, soit un million de distances. Même en divisant ce nombre par deux (les éléments de la matrice sont symétriques, donc $d(ij) = d(ji)$), ce nombre augmente avec le carré du nombre d'observations. Pour de grands jeux de données, la CAH peut donc échouer à cause des limites de l'ordinateur utilisé. Il existe des versions plus performantes de l'algorithme réduisant cette limite, mais il convient de la garder en mémoire. Quand un très grand jeu de données doit être analysé, les méthodes des nuées dynamiques sont une solution à considérer.\n\n### Mise en œuvre dans R {#sec-1334}\n\nNous proposons ici un exemple issu d'un article portant sur les parcs urbains de Montréal [@apparicio2010accessibilite], dont l'objectif était notamment de classifier ces parcs en fonction de leur superficie et des équipements qu'ils comprennent, et ce, en utilisant la classification ascendante hiérarchique. Nous proposons ici de reproduire l'étape de classification effectuée dans cet article. La base de données comporte 653 parcs pour lesquels la présence de 18 équipements est codée comme un ensemble de variables binaires (0 signifiant absence et 1 présence). Nous disposons également de la taille de ces parcs, recodée en cinq catégories : moins d'un hectare, de 1 à 5 hectares, de 5 à 10 hectares, de 10 à 20 hectares et 20 hectares et plus. Le @tbl-exampleHclust1DF indique le nombre d'équipements recensés dans les parcs.\n\n```{r}\n#| label: tbl-exampleHclust1DF\n#| tbl-cap: Équipements recensés dans les différents parcs de Montréal\n#| echo: false\n#| message: false\n#| warning: false\nlibrary(kableExtra)\ndf <- data.frame(\n  equipement = c(\"Aire de jeux\", \"Pataugeoire\", \"Jeux d’eau\",\n                 \"Baseball\", \"Soccer (football)\", \"Basketball\", \"Tennis\" , \" Football\", \"Volleyball\", \"Athlétisme\",\n                 \"Patinoire extérieure\", \"Glissade\", \"Piste de ski de fond\", \"Piste de raquette\",\n                 \"Parc de planches à roulettes\", \"Patins à roues alignées\",\n                 \"Piscine intérieure\", \"Chemin de randonnée\"),\n  N = c(601,161,28,188,169,144,125,36,24,20,241,30,14,9,18,8,92,15)\n)\n\nmy_table <- knitr::kable(df,\n           format.args = list(decimal.mark = ',', big.mark = \" \"),\n\t\t       col.names = c(\"Équipements\" , \"N\"), \n\t\t       row.names = FALSE,\n           align=c(\"l\", \"r\")\n           )\n\ngroup_rows(my_table,\n           index = c(\"Équipements pour les 0 à 4 ans\" = 3,\n                     \"Terrains de sport\" = 7,\n                     \"Équipements d'hiver\" = 4,\n                     \"Équipements spécialisés\" = 2,\n                     \"Autres équipements\" = 2)\n                   )\n```\n\nPuisque notre jeu de données ne comporte que des variables qualitatives, nous utilisons la distance du $\\Phi^2$ pour construire notre matrice de distance entre les parcs. Notons que, dans l'article original, la distance euclidienne au carré avait été utilisée, alors nous n'obtiendrons probablement pas les mêmes résultats, car la distance du $\\Phi^2$ tient compte des fréquences d'occurrence des modalités des variables qualitatives.\n\n#### Calcul de la matrice de distance {#sec-13341}\n\nLa première étape consiste donc à charger notre jeu de données et à calculer la matrice de distance.\n\n```{r}\n#| label: exampleHclust1\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\n# chargement du jeu de données et sélection des colonnes pour l'analyse\nparcs <- read.csv(\"data/classification/Parcs.txt\", header = TRUE, stringsAsFactors = FALSE)\nX <- parcs[c(5:22, 27)]\n```\n\nPour calculer la distance du $\\Phi^2$, nous utilisons la fonction `dist` du *package* `proxy` avec le paramètre `method = \"Phi-squared\"`. Elle requiert que l'ensemble des variables catégorielles soient converties en variables binaires. Pour cela, nous pouvons utiliser la fonction `dummy_cols` du *package* `fastDummies`.\n\n```{r}\n#| label: exampleHclust2\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\nlibrary(fastDummies)\nlibrary(proxy)\n\nX <- dummy_cols(X, select_columns = \"HaTypo\", remove_selected_columns = TRUE)\nparc_distances <- dist(as.matrix(X), method = \"Phi-squared\")\n```\n\n#### Application de l'algorithme de classification ascendante hiérarchique {#sec-13342}\n\nUne fois la matrice obtenue, il ne reste plus qu'à appliquer la fonction `hclust` disponible de base dans R pour obtenir le dendrogramme. Comme dans l'article, nous utilisons le critère d'agrégation de Ward pour la création des groupes.\n\n```{r}\n#| label: exampleHclust3\n#| fig-align: center\n#| message: false\n#| warning: false\n\ndendogramme_parcs <- hclust(parc_distances, method = \"ward.D\")\n```\n\nPuisque nous n'utilisons pas la distance euclidienne, nous optons ici pour l'indice de silhouette pour déterminer le nombre adéquat de groupes à former. Nous testons toutes les valeurs comprises entre 2 et 10.\n\n```{r}\n#| label: fig-exampleHclust4\n#| fig-align: center\n#| fig-cap: Valeur de l'indice de silhouette pour différents nombres de groupes\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\nlibrary(cluster)\nks <- 2:10\n\n# Calcul des indices de silhouette pour les différentes valeurs de k\nvalues <- sapply(ks, function(k){\n  # découpage du dendrogramme\n  groupes <- cutree(dendogramme_parcs, k = k)\n  # calcul des valeurs de silhouette\n  sil <- silhouette(groupes, dist = parc_distances)\n  # extraction de l'indice global (moyenne des moyennes)\n  idx <- mean(summary(sil)$clus.avg.widths)\n  return(idx)\n})\n\n# Création d'un graphique avec les résultats\ndf <- data.frame(k = ks,  silhouette = values)\nggplot(df) + \n  geom_line(aes(x = k, y = silhouette)) + \n  geom_point(aes(x = k, y = silhouette), color = \"red\") + \n  labs(x = \"nombre de groupes\", y = \"indice global de silhouette\")\n```\nSi nous écartons d'emblée les résultats pour k = 2 et k = 3 (trop peu de groupes pour l'interprétation), nous constatons que la solution optimale selon ce critère est k = 5. Dans l'article original, la solution k = 6 avait été retenue en examinant le dendrogramme. Comparons les résultats pour k = 5 et k = 6.\n\n```{r}\n#| label: exampleHclust5\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\nresk5 <- cutree(dendogramme_parcs, k = 5)\nresk6 <- cutree(dendogramme_parcs, k = 6)\nsil5 <- silhouette(resk5, dist = parc_distances)\nsil6 <- silhouette(resk6, dist = parc_distances)\n\n# résumé pour l'indice de silhouette pour k = 5\nsummary(sil5)\n\n# résumé pour l'indice de silhouette pour k = 6\nsummary(sil6)\n  \n```\nNous constatons que le groupe supplémentaire vient séparer le groupe trois comprenant 246 parcs dans la solution avec k = 5. Ce dernier ne comprend plus que 197 parcs pour la solution k = 6 et le nouveau groupe en compte 49. Ce nouveau groupe à un indice de silhouette moyen relativement faible (0,079), et le fait de découper le groupe trois améliore très peu sa propre valeur (passant de -0,12 à -0,10). Nous retenons cependant ici la solution avec k = 6 afin de tenter de reproduire les résultats de l'article.\n\n#### Interprétation des résultats {#sec-13343}\n\nLa dernière étape consiste à identifier les groupes obtenus et leur attribuer un intitulé en fonction de leurs caractéristiques. Dans notre cas, la classification ne comporte que des variables binaires, nous pouvons donc calculer le pourcentage de valeurs à 1 (présence d'un équipement) dans chacun des groupes.\n\n```{r}\n#| label: exampleHclust6\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\n# calcul du nombre de fois où chaque modalité est observée dans un groupe\nX$groupe <- resk6\ndf_groupes <- X %>% \n  group_by(groupe) %>% summarise_all(.funs = sum)\n\n# calcul du nombre d'observations par groupe\nnb_gp <- table(resk6)\ngroupe_ratios <- round(100 * as.matrix(df_groupes)[,2:ncol(df_groupes)] / as.vector(nb_gp),1)\ngroupe_ratios <- as.data.frame(t(groupe_ratios))\nnames(groupe_ratios) <- paste0(\"groupe \", 1:ncol(groupe_ratios))\n\n# calcul du nombre moyen d'équipements par catégorie par parc\nequip_class <- list(\n  c(\"AIRE_JEUX\", \"JEUX_EAU\", \"PATAUGEOIRE\"),\n  c(\"ATHLETISME\", \"BASEBALL_S\", \"BASKETBALL\", \"FOOTBALL\", \"SOCCER\", \"TENNIS\", \"VOLLEY_BALL\"),\n  c(\"TOBBOGAN_G\", \"PATINOIRE_E\", \"RAQUETTES\", \"SKI_FOND\"),\n  c(\"PATIN_ROUE\", \"ROULI_ROUL\"),\n  c(\"PISC_EXT\", \"RANDONNEE\")\n)\n\nclass_compte <- data.frame(sapply(equip_class, function(equip){\n  rowSums(X[equip])\n}))\nnames(class_compte) <- c(\"enfants\", \"terrain_sport\", \"hiver\", \"specialise\", \"autre\")\nclass_compte$groupe <- resk6\ndf_class_equip <- class_compte %>% \n  group_by(groupe) %>% \n  summarise_all(mean)\n\ndf_class_equip <- t(df_class_equip[2:ncol(df_class_equip)])\ncolnames(df_class_equip) <- paste0(\"groupe \", 1:ncol(df_class_equip))\n\n# comptage du nombre moyen total d'équipements\ndf_equip_tot <- data.frame(\n  nb = rowSums(X[1:18]),\n  groupe = resk6\n)\ndf_equip_tot_mean <- df_equip_tot %>% \n  group_by(groupe) %>% \n  summarize_all(mean)\n\n# mise dans l'ordre de la première partie du tableau\nall_types <- do.call(c, equip_class)\nidxs <- match(all_types, row.names(groupe_ratios[1:length(all_types),]))\ngroupe_ratios <- rbind(groupe_ratios[idxs,],\n                       groupe_ratios[(length(all_types)+1):nrow(groupe_ratios),])\n\n# combinaison des deux tableaux\ngroupe_ratios <- rbind(groupe_ratios, df_class_equip, df_equip_tot_mean$nb, as.integer(nb_gp))\n```\n\nIl est ensuite possible d'afficher le tableau obtenu pour l'interpréter. Les résultats sont ici rapportés au @tbl-exampleHclust7.\n\n```{r}\n#| label: tbl-exampleHclust7\n#| tbl-cap: Caractéristiques des groupes obtenus lors de la CAH\n#| echo: false\n#| fig-align: center\n#| message: false\n#| warning: false\n\nlibrary(kableExtra)\nrow.names(groupe_ratios) <- c(\n  \"Aire de jeux\", \"Jeux d'eau\", \"Pataugeoire\", \"Athlétisme\", \"Baseball\", \"Basketball\", \"Football américain\", \"Soccer (football)\",\n  \"Tennis\", \"Volleyball\", \"Glissade\", \"Patinoire\", \"Piste de ski de fond\", \"Raquettes\", \"Parc de planches à roulettes\",\n  \"Patins à roues alignées\", \"Piscine extérieure\", \"Chemin de randonnée\", \"Moins d'un hectare\", \"1 à 5 hectares\", \"5 à 10 hectares\",\n  \"10 à 20 hectares\", \"20 hectares et plus\", \"Équipements pour les 0 à 4 ans\", \"Terrains de sport\", \"Équipements d'hiver\", \n  \"Équipements spécialisés\", \"Autres équipements\", \"Tous les équipements\", \"\"\n)\nmy_table <- knitr::kable(round(groupe_ratios,1), row.names = TRUE, \n                       format.args = list(decimal.mark = ',', big.mark = \" \"),\n\t\t\t\t\t             col.names = paste0(\"groupe \",1:6),\n                       align = c(\"r\", \"r\", \"r\", \"r\", \"r\", \"r\"))\ngroup_rows(my_table,\n           index = c(\n             \"Équipements pour les 0 à 4 ans (%)\" = 3,\n             \"Terrains de sport (%)\" = 7,\n             \"Équipements d'hiver (%)\" = 4,\n             \"Équipements spécialisés (%)\" = 2,\n             \"Autres équipements (%)\" = 2,\n             \"Superficie (%)\" = 5,\n             \"Nombre moyen d'équipements selon le type\" = 6,\n             \"Nombre d'observations par groupe\"\n             )\n  )\n\n```\n\n* Le premier groupe correspond à de grands parcs (superficie généralement comprise entre 5 et plus de 20 hectares), il comporte 116 observations. Ces grands parcs sont en moyenne équipés de deux terrains de sport et d'un équipement d'hiver. Il s'agit vraisemblablement des grands parcs identifiés dans l'article original, dans lesquels se retrouvent également les parcs à vocation métropolitaine.\n\n* Le second groupe (212 parcs) correspond à de très petits parcs (moins d'un hectare) comportant uniquement une aire de jeu.\n\n* Le troisième groupe (197 parcs) correspond à de petits parcs (entre 1 et 5 hectares), souvent équipés d'une piscine extérieure (27,4 % des cas), et en moyenne de deux terrains de sports (essentiellement des terrains de tennis et de soccer). Ces parcs comprennent en moyenne plus de 4 équipements et doivent donc correspondre à la classe D dans l'article original (Petit parc (1 à 5 ha) avec en moyenne six équipements, dont une patinoire et une piscine).\n\n* Le quatrième groupe (49 parcs) comprend de petits parcs (entre 1 et 5 hectares) qui ressemblent aux parcs du groupe 2 mais tendent à disposer en plus d'un terrain de sport (baseball ou basketball).\n\n* Le quatrième groupe (84 parcs) correspond à de petits parcs, il est caractérisé par une présence plus marquée de pataugeoires (39 %).\n\n* Le cinquième groupe (35 parcs) est très similaire au second groupe (uniquement une aire de jeux), excepté sont les parcs qui s'y trouvent sont de taille supérieure (de 1 à 5 hectares).\n\nConsidérant les différences minimes entre certains des groupes que nous avons obtenus, il est clair que retenir seulement trois ou cinq groupes serait préférable. Notez également l'importance du choix de la distance, car nous obtenons des résultats sensiblement différents de ceux de l'article original en ayant opté pour la distance du $\\Phi^2$ plutôt que la distance euclidienne au carré.\n\n#### Utilisation de la matrice de distance euclidienne au carré {#sec-13344}\n\nPour obtenir des résultats plus proches de ceux de l'article original, nous pouvons reprendre notre analyse et utiliser cette fois-ci une distance euclidienne au carré.\n\n```{r}\n#| label: fig-exampleHclust8\n#| fig-align: center\n#| fig-cap: Valeur de l'indice de silhouette pour différents nombres de groupes (distance euclidienne au carré)\n#| out-width: \"75%\"\n#| message: false\n#| warning: false\n\nX$groupe <- NULL\n# calcule de la matrice de distance\nparc_distances_euc <- dist(as.matrix(X), method = \"Euclidean\")**2\n\n# Application de la CAH\ndendogramme_parcs_euc <- hclust(parc_distances_euc, method = \"ward.D\")\n\n# calcul de l'indice de silhouette\nks <- 2:10\nvalues <- sapply(ks, function(k){\n  # découpage du dendrogramme\n  groupes <- cutree(dendogramme_parcs_euc, k = k)\n  # calcul des valeurs de silhouette\n  sil <- silhouette(groupes, dist = parc_distances_euc)\n  # extraction de l'indice global (moyenne des moyennes)\n  idx <- mean(summary(sil)$clus.avg.widths)\n  return(idx)\n})\n\n# création d'un graphique avec les résultats\n\ndf <- data.frame(\n  k = ks,\n  silhouette = values\n)\n\nggplot(df) + \n  geom_line(aes(x = k, y = silhouette)) + \n  geom_point(aes(x = k, y = silhouette), color = \"red\") + \n  labs(x = \"nombre de groupes\", y = \"indice global de silhouette\")\n```\n\nNous constatons cette fois-ci, que quatre groupes serait probablement le meilleur choix et qu'au-delà de ce nombre, l'indice global de silhouette ne fait que diminuer. Tentons cependant de reproduire les résultats de l'article avec k = 6.\n\n```{r}\n#| label: exampleHclust9\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\nresk6 <- cutree(dendogramme_parcs_euc, k = 6)\n\n# calcul du nombre de fois ou chaque modalité est observée dans un groupe\nX$groupe <- resk6\ndf_groupes <- X %>% \n  group_by(groupe) %>% summarise_all(.funs = sum)\n\n# calcul du nombre d'observations par groupe\nnb_gp <- table(resk6)\n\ngroupe_ratios <- round(100 * as.matrix(df_groupes)[,2:ncol(df_groupes)] / as.vector(nb_gp),1)\ngroupe_ratios <- as.data.frame(t(groupe_ratios))\nnames(groupe_ratios) <- paste0(\"groupe \", 1:ncol(groupe_ratios))\n\n# calcul du nombre moyen d'équipements par catégorie par parc\nequip_class <- list(\n  c(\"AIRE_JEUX\", \"JEUX_EAU\", \"PATAUGEOIRE\"),\n  c(\"ATHLETISME\", \"BASEBALL_S\", \"BASKETBALL\", \"FOOTBALL\", \"SOCCER\", \"TENNIS\", \"VOLLEY_BALL\"),\n  c(\"TOBBOGAN_G\", \"PATINOIRE_E\", \"RAQUETTES\", \"SKI_FOND\"),\n  c(\"PATIN_ROUE\", \"ROULI_ROUL\"),\n  c(\"PISC_EXT\", \"RANDONNEE\")\n)\n\nclass_compte <- data.frame(sapply(equip_class, function(equip){\n  rowSums(X[equip])\n}))\nnames(class_compte) <- c(\"enfants\", \"terrain_sport\", \"hiver\", \"specialise\", \"autre\")\nclass_compte$groupe <- resk6\ndf_class_equip <- class_compte %>% \n  group_by(groupe) %>% \n  summarise_all(mean)\n\ndf_class_equip <- t(df_class_equip[2:ncol(df_class_equip)])\ncolnames(df_class_equip) <- paste0(\"groupe \", 1:ncol(df_class_equip))\n\n# comptage du nombre moyen d'équipements\ndf_equip_tot <- data.frame(\n  nb = rowSums(X[1:18]),\n  groupe = resk6\n)\ndf_equip_tot_mean <- df_equip_tot %>% \n  group_by(groupe) %>% \n  summarize_all(mean)\n\n# mise dans l'ordre de la première partie du tableau\nall_types <- do.call(c, equip_class)\nidxs <- match(all_types, row.names(groupe_ratios[1:length(all_types),]))\ngroupe_ratios <- rbind(groupe_ratios[idxs,],\n                       groupe_ratios[(length(all_types)+1):nrow(groupe_ratios),])\n\n# combinaison des deux tableaux\ngroupe_ratios <- rbind(groupe_ratios, df_class_equip, df_equip_tot_mean$nb, as.integer(nb_gp))\n\n```\n\nRecréons le tableau final des résultats au @tbl-exampleHclust10. Si vous comparez ce tableau avec celui de l'article original, vous verrez que notre groupe 3 correspond exactement à la classe A et que notre groupe 5 correspond exactement à la classe F. Pour les autres groupes, nous pouvons observer de légères variations, ce qui correspond vraisemblablement à des divergences d'implémentation des algorithmes entre le logiciel utilisé pour l'article (SAS) et R.\n\n```{r}\n#| label: tbl-exampleHclust10\n#| tbl-cap: Caractéristiques des groupes obtenus lors de la CAH (distance euclidienne au carré)\n#| echo: false\n#| fig-align: center\n#| message: false\n#| warning: false\nlibrary(kableExtra)\nrow.names(groupe_ratios) <- c(\n  \"Aire de jeux\", \"Jeux d'eau\", \"Pataugeoire\", \"Athlétisme\", \"Baseball\", \"Basketball\", \"Football\", \"Soccer (football)\",\n  \"Tennis\", \"Volleyball\", \"Glissade\", \"Patinoire\", \"Piste de ski de fond\", \"Raquettes\", \"Parc de planches à roulettes\",\n  \"Patins à roues alignées\", \"Piscine extérieure\", \"Chemin de randonnée\", \"Moins d'un hectare\", \"1 à 5 hectares\", \"5 à 10 hectares\",\n  \"10 à 20 hectares\", \"20 hectares et plus\", \"Équipements pour les 0 à 4 ans\", \"Terrains de sport\", \"Équipements d'hiver\", \n  \"Équipements spécialisés\", \"Autres équipements\", \"Tous les équipements\", \"\"\n)\nmy_table <- knitr::kable(round(groupe_ratios,1),\n                        row.names = TRUE, \n                        format.args = list(decimal.mark = ',', big.mark = \" \"),\n\t\t\t\t\t\t            col.names = paste0(\"groupe \",1:6),\n                        align=c(\"r\" , \"r\" , \"r\" , \"r\" , \"r\" , \"r\"))\ngroup_rows(my_table,\n           index = c(\n             \"Équipements pour les 0 à 4 ans (%)\" = 3,\n             \"Terrains de sport (%)\" = 7,\n             \"Équipements d'hiver (%)\" = 4,\n             \"Équipements spécialisés (%)\" = 2,\n             \"Autres équipements (%)\" = 2,\n             \"Superficie (%)\" = 5,\n             \"Nombre moyen d'équipements selon le type\" = 6,\n             \"Nombre d'observations par groupe\"\n             )\n  )\n```\n\n## Nuées dynamiques {#sec-134}\n\nLes méthodes des nuées dynamiques regroupent plusieurs algorithmes, tous plus ou moins liés avec l'algorithme le plus connu : *k-means*, originalement proposé par James MacQueen [-@macqueen1967]. Nous présentons également ici plusieurs variantes du *k-means*, soit le *k-medians*, le *k-medioids*, le *c-means* et le *c-medians*.\n\n### *K-means* {#sec-1341}\n\n#### Fonctionnement de l'algorithme {#sec-13411}\n\nNous commençons ici par détailler le fonctionnement de cet algorithme afin de mieux le cerner. D'emblée, cet algorithme nécessite que certains éléments soient définis d'avance : \n\n* Une matrice de données *X* comportant *n* lignes (nombre d'observations) et *p* colonnes (nombre de variables). Chaque variable de cette matrice doit être quantitative et continue et de préférence dans une échelle standardisée (par exemple des variables centrées réduites).\n\n* Le nombre de groupes à identifier *k* doit être choisi par l'utilisateur ou l'utilisatrice. \n\n* La distance *d* à utiliser entre les observations.\n\nLe fonctionnement classique du *k-means* est le suivant :\n\n1. Définir *k* centres de groupes de façon aléatoire.\n\n2. Déterminer pour chaque observation le centre de son groupe le plus proche en utilisant la fonction de distance.\n\n3. Pour chacun des groupes ainsi formés, recalculer le centre du groupe en calculant le centroïde (moyennes le plus souvent) des observations appartenant à ce groupe.\n\n4. Répéter l'opération 2 avec les nouveaux centres.\n\n5. Calculer l'inertie expliquée par la nouvelle classification.\n\n6. Comparer cette inertie expliquée avec celle obtenue lors de l'itération précédente.\n\n7. Si la variation entre les deux valeurs est supérieure à une certaine limite, reprendre à l'étape 2, sinon, l'algorithme prend fin.\n\nAinsi, l'algorithme *k-means* part d'une première classification obtenue aléatoirement et la raffine jusqu'au point où l'amélioration de la classification devient négligeable. Du fait de ce point de départ aléatoire, cet algorithme est dit heuristique, car deux exécutions risquent de ne pas donner exactement le même résultat. Par conséquent, en relaçant l'algorithme, vous pourriez obtenir des résultats légèrement différents, avec par exemple des groupes similaires, mais obtenus dans un autre ordre, le groupe 1 étant devenu le groupe 3 et vice-versa. Il est aussi possible d'obtenir des résultats radicalement différents d'une tentative à l'autre, ce qui signifie que les groupes formés sont très instables et ne sont pas représentatifs de la population étudiée.\n\n\n::: bloc_astuce\n::: bloc_astuce-header\n::: bloc_astuce-icon\n:::\n**Réplicabilité des résultats dans R**\n:::\n::: bloc_astuce-body\nLorsqu'une méthode heuristique ou faisant appel au hasard est utilisée dans R, il est nécessaire de s'assurer que les résultats sont reproductibles. Cela permet notamment de relancer le même code et de réobtenir exactement les mêmes résultats : l'idée étant de figer le hasard.\n\nUltimement, un programme informatique est incapable de générer un résultat véritablement aléatoire, car il ne fait que suivre une suite d'opérations prédéterminées. Pour générer des résultats qui ressemblent au hasard, des algorithmes ont été proposés, partant d'une configuration initiale et appliquant une série d'opérations complexes permettant de générer des nombres semblant se distribuer aléatoirement. Si nous connaissons le point de départ de la suite d'opérations et que nous réappliquons ces dernières, alors nous sommes certains d'obtenir le même résultat. Il est possible, dans R, de définir un *état initial de hasard* à l'aide de la fonction `set.seed`. Avec ce point de départ défini, nous sommes certains d'obtenir les mêmes résultats en relançant les mêmes opérations. \n\nPrenons un exemple concret en sélectionnant aléatoire 3 chiffres dans un vecteur allant de 1 à 10.\n\n```{r}\n#| warning: false\n#| message: false\nvec <- 1:10\n\n# prenons un premier échantillon\nsample(vec, size = 3)\n\n# et un second échantillon \nsample(vec, size = 3)\n```\nNous obtenons bien deux échantillons différents. Recommençons en utilisant la fonction `set.seed` pour obtenir cette fois-ci des résultats identiques.\n\n```{r}\n#| warning: false\n#| message: false\nvec <- 1:10\n\n# prenons un premier échantillon\nset.seed(123)\nsample(vec, size = 3)\n\n# et un second échantillon \nset.seed(123)\nsample(vec, size = 3)\n\n# prenons un troisème échantillon\nset.seed(4568997)\nsample(vec, size = 3)\n\n# et un quatrième échantillon \nset.seed(4568997)\nsample(vec, size = 3)\n```\n\nVous constatez que nous utilisons cette fonction plusieurs fois au cours de cette section. Elle nous permet de nous assurer que les résultats obtenus ne changent pas entre le moment où nous écrivons le livre et le moment où nous le formatons. Sinon, le texte pourrait ne plus être en phase avec les images ou les tableaux.\n:::\n:::\n\nPour mieux comprendre le fonctionnement du *k-means*, nous proposons ici une visualisation de ses différentes itérations (@fig-kmeansA). \nNous pouvons constater que, pour ce jeu de données relativement simple, l'algorithme converge très rapidement et que sa solution varie peu au-delà de la troisième itération. \nL'amination de la @fig-kmeansA illustre pourquoi le *k-means* est appelé algorithme de nuées dynamiques.\n\n![Algorithme K-means](images/Chap13/kmeansGif.gif){#fig-kmeansA width=\"80%\" fig-align=\"center\"}\n\n```{r}\n#| label: fig-kmeansA2\n#| eval: false\n#| message: false\n#| warning: false\n#| include: false\nX <- data.frame(\n  x = c(rnorm(50,0,0.5), rnorm(50,5,1.5), rnorm(50,7,1), rnorm(50,6,1), rnorm(50,2.75,1.5)),\n  y = c(rnorm(50,0,0.5), rnorm(50,2.5,1), rnorm(50,0,0.5), rnorm(50,2,1), rnorm(50,1,0.5))\n)\n\nX$oid <- 1:nrow(X)\n  \ncenters <- data.frame(\n  x = c(1,7.5,4,7),\n  y = c(2,2.5,5,3)\n)\n\ngp <- apply(X,1, function(r){\n  dists <- sqrt((r[[1]] - centers$x)**2 + (r[[2]] - centers$y)**2)\n  (1:length(dists))[dists == min(dists)]\n})\nX$groupe <- as.factor(gp)\ncenters$groupe <- as.factor(1:nrow(centers))\n\n# CAS 1 : Sortie HTML (un joli Gif)\nif(knitr::is_latex_output() == FALSE){\n  library(gganimate)\n  X$iter <- 1\n  centers$iter <- 1\n  \n  Xtot <- X\n  centerstot <- centers\n  \n  for (i in 2:4){\n    rez <- kmeans(X[, c(1,2)], centers = centers[, c(1,2)], iter.max = 1)\n    X$groupe <- as.factor(rez$cluster)\n    centers <<- data.frame(rez$centers)\n    centers$groupe <- as.factor(1:nrow(centers))\n    X$iter <- i\n    centers$iter <- i\n    \n    Xtot <- rbind(Xtot, X)\n    centerstot <- rbind(centerstot, centers)\n  }\n  \n  ggplot(Xtot) + \n      geom_point(aes(x = x, y = y, color = groupe, group = oid)) + \n      geom_point(data = centerstot, mapping = aes(x = x, y = y, fill = groupe),\n                 pch = 21, colour = \"black\", size = 3) + \n      transition_states(iter, transition_length = 2,\n                      state_length = 1) + \n    ggtitle(\"Itération {closest_state}\")  \n\n}else{\n  X2 <- X\n  # CAS 2 : Sortie PDF (une simple figure)\n\n  plot1 <- ggplot(X2) + \n      geom_point(aes(x = x, y = y, color = groupe)) + \n      geom_point(data = centers, mapping = aes(x = x, y = y, fill = groupe),\n                 pch = 21, colour = \"black\", size = 3) + \n    ggtitle(\"Itération 1\")\n  \n  plots <- list(plot1)\n  for(i in 2:4){\n    centers$groupe <- NULL\n    rez <- kmeans(X[, c(1,2)], centers = centers[, c(1,2)], iter.max = 1)\n    \n    X2$groupe <- as.factor(rez$cluster)\n    \n    centers <- data.frame(rez$centers)\n    centers$groupe <- as.factor(1:nrow(centers))\n    \n    new_plot <- ggplot(X2) + \n      geom_point(aes(x = x, y = y, color = groupe)) + \n      geom_point(data = centers, mapping = aes(x = x, y = y, fill = groupe),\n                 pch = 21, colour = \"black\", size = 3)+ \n    ggtitle(paste0(\"Itération \", i))\n    plots[[i]] <- new_plot\n    \n  }\n  ggarrange(plotlist = plots, common.legend = TRUE, ncol = 2, nrow = 2, legend = \"none\")\n  \n}\n```\n\n::: bloc_notes\n::: bloc_notes-header\n::: bloc_notes-icon\n:::\n**Centre de groupe et _k-means_**\n:::\n::: bloc_notes-body\nÀ nouveau, puisque chaque itération du *k-means* nécessite de recalculer les centres des groupes formés, des problèmes peuvent être rencontrés avec certains types de distance. C'est pourquoi il est recommandé d'utiliser la distance euclidienne avec le *k-means* original. Si des distances plus complexes doivent être utilisées, il est préférable d'utiliser la classification ascendante hiérarchique.\n:::\n:::\n\n#### Choix du nombre optimal de groupes {#sec-13412}\n\nComme pour la CAH, le principal enjeu avec le *k-means* est de déterminer le nombre idéal de groupes pour effectuer la classification. Si ce nombre n'est pas connu à l'avance et qu'aucune forte justification théorique n'existe, il est possible d'utiliser les mêmes techniques que pour la CAH, soit la méthode du coude, l'indicateur de silhouette ou la méthode GAP.\n\n\n### K-médianes {#sec-1342}\n\nLe *k-medians* est une variante du *k-means*. Contrairement au *k-means* privilégiant la distance euclidienne, le *k-medians* est à utiliser en priorité avec une distance de Manhattan. En effet, le centre d'un groupe n'est pas déterminé comme la moyenne des variables des observations appartenant à ce groupe (*k-means*), mais comme la médiane pour chaque variable (*k-medians*). En dehors de ces deux spécificités, il reprend le fonctionnement décrit plus haut pour le *k-means*. Il est particulièrement pertinent de l'utiliser quand un jeu de données comprend un très grand nombre de colonnes, car dans ce contexte, la distance euclidienne peine à représenter les différences entre les observations. De plus, l'utilisation de la médiane le rend moins sensible aux valeurs extrêmes.\n\n\n### K-médoïds {#sec-1343}\n\nLe *k-médoïds* est également une variante du *k-means*. Le *k-means* crée des groupes en cherchant les centres de ces groupes dans l'espace multidimensionnel des données. Ces centres de groupes peuvent très bien ne pas correspondre à un point du jeu de données, au même titre que la moyenne d'une variable ne coïncide que rarement avec une observation réelle de cette variable. Pour le *k-médoïds*, les groupes sont formés en cherchant les centres de ces groupes **parmi** les observations du jeu de données. Ainsi, chaque groupe est centré sur une observation réelle, la plus similaire à l'ensemble des observations du groupe.\n\nL'algorithme effectue les opérations suivantes : \n\n1. Sélectionner aléatoirement *k* observations du jeu de données, elles constituent les centres des groupes initiaux.\n\n2. Attribuer chaque observation au centre du groupe le plus proche.\n\n3. Tant que la nouvelle solution est plus efficace, effectuer les opérations suivantes : \n  * pour chaque centre *m* et pour chaque observation *o*,\n    * considérer l'inversion de *m* et *o*\n    * si cette permutation est meilleure que les précédentes, la conserver en mémoire\n  * effectuer la meilleure permutation retenue si elle améliore la classification, sinon l'algorithme prend fin.\n  \nLe *k-médoïds* est moins utilisé que le *k-means*, mais il est plus performant quand des distances autres que la distance euclidienne sont utilisées ou encore que des valeurs aberrantes/extrêmes sont présentes dans les données.\n\n### Mise en œuvre dans R {#sec-1344}\n\nPour cet exemple, nous proposons d'utiliser le jeu de données spatiales `LyonIris` du *package* `geocmeans`. Ce jeu de données spatiales pour l'agglomération lyonnaise (France) comprend dix variables, dont quatre environnementales (EN) et six socioéconomiques (SE), pour les îlots regroupés pour l'information statistique (IRIS) de l'agglomération lyonnaise (@tbl-datageocmeans et @fig-datacartoacp). Nous proposons de réaliser une analyse similaire à celle de l'article de @2021_4, soit de classer les IRIS de Lyon selon ces caractéristiques pour déterminer si certains groupes d'IRIS combinent des situations désavantageuses sur les plans sociaux et environnementaux, dans une perspective d'équité environnementale.\n\nNotez ici que la fonction `st_drop_geometry` provenant du package `sf` permet de retirer l'information géographique du jeu de données `LyonIris` pour obtenir un simple `dataframe`.\n\n```{r}\n#| label: tbl-datageocmeans\n#| tbl-cap: Statistiques descriptives du jeu de données LyonIris\n#| echo: false\n#| message: false\n#| warning: false\nlibrary(geocmeans)\nlibrary(sf)\n\ndata(LyonIris)\nData <- st_drop_geometry(LyonIris[c(\"Lden\" , \"NO2\" , \"PM25\" , \"VegHautPrt\",\n                        \"Pct0_14\" , \"Pct_65\" , \"Pct_Img\",\n                        \"TxChom1564\" , \"Pct_brevet\" , \"NivVieMed\")])\n\nintitule <- c(\"Bruit routier (Lden dB(A))\",\n              \"Dioxyde d'azote (ug/m^3^)\",\n              \"Particules fines (PM$_{2,5}$)\",\n              \"Canopée (%)\",\n              \"Moins de 15 ans (%)\",\n              \"65 ans et plus (%)\",\n              \"Immigrants (%)\",\n              \"Taux de chômage\",\n              \"Personnes à faible scolarité (%)\",\n              \"Médiane du niveau de vie (Euros)\" )\n\nstats <- data.frame(variable = names(Data),\n                    nom = intitule,\n                    type = c(\"EN\" , \"EN\" , \"EN\" , \"EN\" , \"SE\" , \"SE\" , \"SE\" , \"SE\" , \"SE\" , \"SE\"),\n                    moy = round(sapply(Data, mean), 2),\n                    et = round(sapply(Data, sd), 2), \n                    minimum = round(sapply(Data, min), 2), \n                    maximum = round(sapply(Data, max), 2)\n                    )\nknitr::kable(stats,\n            format.args = list(decimal.mark = ',', big.mark = \" \"),\n\t\t\t      digits = 1,\n            col.names = c(\"Nom\" , \"Intitulé\" , \"Type\" , \"Moy.\", \"E.-T.\", \"Min.\", \"Max.\"),\n            align= c(\"l\" , \"l\", \"c\" , \"r\", \"r\", \"r\", \"r\")\n            )\n```\n\n\n#### Préparation des données {#sec-13441}\n\nLa première étape consiste donc à charger les données et à les préparer pour l'analyse. Toutes les variables que nous utilisons sont des variables continues. Cependant, elles ne sont pas exprimées dans la même échelle, nous proposons donc de les standardiser ici en les centrant (moyenne = 0) et en les réduisant (écart-type = 1). Cette opération peut être effectuée simplement dans R en utilisant la fonction `scale`.\n\n```{r}\n#| label: kmeansB\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n# Chargement des données\nlibrary(geocmeans)\nlibrary(sf)\n\ndata(LyonIris)\n\n# NB : LyonIris est un objet spatial, il faut donc extraire uniquement son DataFrame\nX <- st_drop_geometry(LyonIris[c(\"Lden\" , \"NO2\" , \"PM25\" , \"VegHautPrt\" , \"Pct0_14\" , \"Pct_65\" , \"Pct_Img\",\n                     \"TxChom1564\" , \"Pct_brevet\" , \"NivVieMed\")])\n\n# Centrage et réduction de chaque colonne du DataFrame\nfor (col in names(X)){\n  X[[col]] <- scale(X[[col]], center = TRUE, scale = TRUE)\n}\n\n```\n\n#### Choix du nombre de groupes optimal {#sec-13442}\n\nLa seconde étape consiste à déterminer le nombre de groupes optimal. Pour cela, nous comparons les résultats des trois méthodes proposées : la méthode du coude, l'indice de silhouette et la méthode GAP. Pour chaque méthode, nous testons les nombres de groupes de 2 à 10.\n\n##### Méthode du coude {#sec-134421}\n\nCommençons par appliquer la méthode du coude. Nous calculons donc l'inertie expliquée par la classification pour différentes valeurs de *k* (nombre de groupes) avant de construire la @fig-kmeansC.\n\n```{r}\n#| label: fig-kmeansC\n#| message: false\n#| fig-cap: Inertie expliquée pour différents nombres de groupes pour le k-means\n#| warning: false\n#| out-width: \"75%\"\nks <- 2:10\n\n## ---- Méthode du coude ---- ##\ninertie_exps <- sapply(ks, function(k){\n  # calcul du kmeans avec k\n  resultat <- kmeans(X, centers = k)\n  # calcul de l'inertie expliquée (1 - inertie intragroupe / inertie totale)\n  inertie_exp <- 1-(sum(resultat$withinss) / resultat$totss)\n  return(inertie_exp)\n})\n\ndf <- data.frame(\n  k = ks,\n  inertie_exp = inertie_exps\n)\n\nggplot(df) + \n  geom_line(aes(x = k, y = inertie_exp)) + \n  geom_point(aes(x = k, y = inertie_exp), color = \"red\") + \n  labs(x = \"nombre de groupes\", y = \"inertie expliquée (%)\")\n\n\n```\nDans l'article original, quatre groupes avaient été retenus. Nous pouvons constater ici qu'un coude fort se situe à k = 3 et qu'au-delà de cette limite, l'ajout d'un groupe supplémentaire contribue à expliquer une plus petite partie de l'inertie supplémentaire comparativement au précédent.\n\n##### Indice de silhouette {#sec-134422}\n\nPoursuivons avec l'indice de silhouette calculé de nouveau avec des valeurs de *k* allant de 2 à 10. Notez que nous devons au préalable créer une matrice de distances entre les observations du jeu de données pour construire notre indice de silhouette. Puisque nous utilisons l'algorithme *k-means*, nous utilisons la distance euclidienne.\n\n```{r}\n#| label: fig-kmeansD\n#| fig-cap: Indice de silhouette pour différents nombres de groupes pour le k-means\n#| out-width: \"75%\"\n#| message: false\n#| warning: false\nks <- 2:10\n\n# calcul d'une matrice de distance euclidienne entre les observations\ndist_mat <- dist(X, method = \"euclidean\")\n\n## ---- indice de silhouette ---- ##\nvalues <- sapply(ks, function(k){\n  resultat <- kmeans(X, centers = k)\n  groupes <- resultat$cluster\n  # calcul des valeurs de silhouette\n  sil <- silhouette(groupes, dist = dist_mat)\n  # extraction de l'indice global (moyenne des moyennes)\n  idx <- mean(summary(sil)$clus.avg.widths)\n  return(idx)\n})\n\ndf <- data.frame(\n  k = ks,\n  silhouette = values\n)\n\nggplot(df) + \n  geom_line(aes(x = k, y = silhouette)) + \n  geom_point(aes(x = k, y = silhouette), color = \"red\") + \n  labs(x = \"nombre de groupes\", y = \"Indice de silhouette\")\n\n\n```\n\nÀ nouveau, la @fig-kmeansD indique que le nombre de groupes optimal est trois selon l'indice de silhouette.\n\n##### Méthode GAP {#sec-134423}\n\nPour appliquer la méthode GAP, nous proposons d'utiliser la fonction `clusGap` du *package* `NbClust`. Pour l'utiliser, il est nécessaire de définir une fonction renvoyant pour le nombre de groupes *k* et le jeu de données *x* une liste comprenant un vecteur attribuant chaque observation à chaque groupe. Il est possible de considérer ce type de fonction comme un « adaptateur ».\n\n```{r}\n#| label: fig-kmeansE\n#| out-width: \"75%\"\n#| fig-cap: Méthode GAP pour différents nombres de groupes pour le k-means\n#| fig-align: center\n#| message: false\n#| warning: false\nlibrary(NbClust)\n\n# définition de la fonction adaptateur\nadaptor <- function(x, k){\n  clust <- kmeans(x, k)\n  return(list(\n    \"cluster\" = clust$cluster\n  ))\n}\n\n# calcul de la méthode GAP\nvals <- clusGap(X, adaptor, K.max = 10, verbose = FALSE)\ntab <- data.frame(vals$Tab)\ntab$k <- 1:nrow(tab)\n\n# détermination des valeurs de k retenues par la méthode (1ere et 2e)\nis_valid <- sapply(2:nrow(tab), function(i){\n  tab[i-1,\"gap\"] >= (tab[i,\"gap\"] - tab[i,\"SE.sim\"])\n})\nvalids <- subset(tab, is_valid)[1,]\nvalids2 <- subset(tab, is_valid)[2,]\n\n# réalisation du graphique\nggplot(tab) + \n  geom_line(aes(x = k, y = gap)) + \n  geom_segment(x = valids$k, xend = valids$k, y = min(tab$gap), yend = valids$gap, \n               linetype = \"dashed\") +\n  geom_segment(x = valids2$k, xend = valids2$k, y = min(tab$gap), yend = valids2$gap, \n               linetype = \"dashed\") +\n  geom_point(aes(x = k, y = gap), color = \"red\") + \n  scale_x_continuous(breaks = 1:10) + \n  labs(x = \"nombre de groupes\", y = \"GAP\")\n  \n\n```\nLa @fig-kmeansE indique également que le nombre de groupes à retenir est trois. Nous retenons cependant quatre groupes pour pouvoir plus facilement comparer nos résultats avec ceux de l'article original.\n\n#### Application l'algorithme du *k-means* {#sec-13443}\n\nMaintenant que nous avons choisi le nombre de groupes à former, nous pouvons simplement appliquer la fonction `kmeans` présente de base dans R.\n\n```{r}\n#| label: kmeansF\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\nset.seed(145)\nresultats <- kmeans(X, centers = 4)\n```\n\n#### Interprétation des résultats {#sec-13444}\n\nUne fois les groupes obtenus, l'étape la plus importante est de parvenir à interpréter ces groupes. Pour cela, il est nécessaire de les explorer en profondeur au travers des variables utilisées pour les constituer. Dans notre cas, le jeu de données `LyonIris` est spatialisé, nous pouvons donc commencer par cartographier les groupes.\n\n```{r}\n#| label: fig-kmeansG\n#| fig-cap: Cartographie des groupes obtenus avec la méthode du k-means\n#| fig-align: center\n#| out-width: \"75%\"\n#| message: false\n#| warning: false\nlibrary(tmap)\nLyonIris$groupes <- paste(\"groupe\", resultats$cluster, sep = \" \")\n\ntm_shape(LyonIris) + \n  tm_polygons(col = \"groupes\", palette =\n                c(\"#EFBE89\", \"#4A6A9F\", \"#7DB47C\", \"#FAF29C\"), lty = 1, lwd = 0.1)\n\n```\n\nIl est ainsi possible de constater que le groupe 3 forme un ensemble assez compact d'IRIS au centre de Lyon. Le groupe 4 correspond quant à lui à des IRIS situés en périphérie plutôt éloignée, essentiellement à l'ouest. Le groupe 1 correspond à une périphérie proche du groupe 2 et apparaît comme un ensemble d'enclaves dispersées.\n\nPour distinguer rapidement les profils des différents groupes, il est possible d'utiliser un graphique en radar. La construction d'un tel graphique peut être un peu fastidieuse dans R, cependant le *package* `geocmeans` propose une fonction assez pratique : `spiderPlots`.\n\n```{r}\n#| label: fig-kmeansH\n#| fig-cap: Graphiques en radar pour les groupes issus du k-means\n#| fig-align: center\n#| out-width: \"90%\"\n#| message: false\n#| warning: false\n\nlibrary(geocmeans)\n\n# création d'une matrice d'appartenance binaire des groupes\nmatrice_gp <- fastDummies::dummy_cols(resultats$cluster, remove_selected_columns = TRUE)\n\n# réalisation du graphique\npar(mfrow=c(3,2), mai = c(0.1,0.1,0.1,0.1))\nplots <- spiderPlots(X, matrice_gp, \n                     chartcolors = c(\"#EFBE89\", \"#4A6A9F\", \"#7DB47C\", \"#FAF29C\"))\n\n```\n\nIl est ainsi possible de constater, à la @fig-kmeansH, que le groupe 3 est caractérisé par un niveau de vie élevé, mais par des niveaux de concentration de pollution atmosphérique plus élevés également. Le groupe 4 en revanche est caractérisé par un important couvert végétal, un niveau de vie médian élevé et une plus forte proportion de personnes de plus de 65 ans. Le groupe 1 est quant à lui marqué par des niveaux sonores plus élevés. Enfin, le groupe 2 se caractérise par une plus grande proportion de population ayant obtenu comme diplôme le plus élevé le brevet des collèges, d'immigrants, de jeunes de moins de 15 ans et un taux de chômage plus élevé.\n\nNotez que ces graphiques nous permettent rapidement de nous faire une idée des caractéristiques des groupes, mais uniquement sur une échelle relative. En effet, ils ne nous indiquent à aucun moment la taille des écarts entre les groupes. Pour cela, il est nécessaire de réaliser des graphiques en violon pour chaque variable. Pour ce type de graphique, il est préférable d'utiliser les données originales non transformées pour pouvoir mieux appréhender si les différences entre les groupes sont importantes ou négligeables.\n\n```{r}\n#| label: fig-kmeansI\n#| fig-cap: Graphiques en violon pour les groupes issus du k-means\n#| out-width: \"100%\"\n#| fig-align: center\n#| message: false\n#| warning: false\nlibrary(ggpubr)\nX2 <- st_drop_geometry(LyonIris[c(\"Lden\" , \"NO2\" , \"PM25\" , \"VegHautPrt\" , \"Pct0_14\",\n                                  \"Pct_65\" , \"Pct_Img\" , \"TxChom1564\" , \"Pct_brevet\",\n                                  \"NivVieMed\")])\n\nplots <- violinPlots(X2, as.character(resultats$cluster))\nggarrange(plotlist = plots, ncol = 2, nrow = 5)\n```\n\nIl est également recommandé de calculer des statistiques descriptives par groupe et de les rapporter dans un tableau.\n\n```{r}\n#| label: kmeansJ\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"100%\"\n# obtention d'un tableau par groupe\ntableaux <- summarizeClusters(X2, matrice_gp, dec = 1, silent = TRUE)\n\n# concaténation des tableaux\ntableau_tot <- do.call(rbind, tableaux)\n```\n\n```{r}\n#| label: tbl-kmeansK\n#| tbl-cap: Descriptions des quatre groupes obtenus\n#| echo: false\n#| fig-align: center\n#| message: false\n#| warning: false\nlibrary(fastDummies)\n\ntableau_tot2 <- apply(tableau_tot, MARGIN = 2, unlist)\ntableau_tot <- data.frame(tableau_tot2)\ntableau_tot$stat <- rep(row.names(tableaux[[1]]),4)\ntableau_tot <- tableau_tot[, c(\"stat\", names(X2))]\n\nok_col_names <- gsub(\"_\" , \"\", names(X2), fixed = TRUE)\n\nmy_table <- knitr::kable(tableau_tot, row.names = FALSE, \n                       format.args = list(decimal.mark = ',', big.mark = \" \"),\n\t\t\t\t\t   digits = 1,\n                       col.names = c(\"\", ok_col_names), \n                       align = c(\"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"))\n\nmy_table2 <- group_rows(my_table,\n           index = c(\n             \"groupe 1\" = 9,\n             \"groupe 2\" = 9,\n             \"groupe 3\" = 9,\n             \"groupe 4\" = 9\n             )\n  )\n\nmy_table2 <- gsub(pattern = \"NApadding\", replacement = \"padding\",x = my_table2, fixed = TRUE)\nmy_table2 <- gsub(pattern = \"<th>\", replacement = '<th style = \"padding : 0.2em\"; font-size = \"13px\">',x = my_table2, fixed = TRUE)\nmy_table2\n```\n\nLes constats que nous avons faits précédemment sont confirmés par la @fig-kmeansI et le @tbl-kmeansK. Nous retrouvons ici les groupes originaux décrits dans l'article de @2021_4 : \n\n* **Groupe 1** : les espaces interstitiels, formant une périphérie proche du centre et relativement hétérogène sur les variables étudiées, mais caractérisée par des niveaux de bruit importants.\n\n* **Groupe 2** : les banlieues jeunes et défavorisées, avec des niveaux d'exposition aux pollutions atmosphérique et sonore relativement élevés comparativement à l'ensemble de la région.\n\n* **Groupe 3** : les quartiers centraux aisés, mais marqués par les plus hauts niveaux de pollution atmosphérique.\n\n* **Groupe 4** : les communes rurales, aisées et vieillissantes.\n\n::: bloc_astuce\n::: bloc_astuce-header\n::: bloc_astuce-icon\n:::\n**Interprétation interactive**\n:::\n::: bloc_astuce-body\nSi, comme dans notre exemple, vos données comportent une dimension spatiale, le *package* `geocmeans` propose une fonction intéressante appelée `sp_clust_explorer` démarrant une application permettant d'explorer les résultats de votre classification. Le seul enjeu est de créer un objet de la classe `FCMres`. Voici un court exemple : \n\n```{r}\n#| label: kmeansK2\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"100%\"\n#| eval: false\n\n# création d'une matrice binaire d'appartenance\nkmeans_mat <- dummy_cols(resultats$cluster, remove_selected_columns = TRUE)\n\n# extraction des centres de notre classification\ncentres <- resultats$centers\n\n# création de l'objet FCMres\nkmeansres <- FCMres(list(\n  \"Centers\" = centres,\n  \"Belongings\" = kmeans_mat,\n  \"Data\" = X2,\n  \"m\" = 1,\n  \"algo\" = \"kmeans\"\n))\n\n# démarrage de l'application shiny\nsp_clust_explorer(object = kmeansres, spatial = LyonIris)\n\n```\n:::\n:::\n\n#### K-médianes et K-médoides {#sec-13455}\n\nNous présentons simplement ici comment effectuer la même analyse en utilisant les variantes du *k-means*, soit le *k-medians* et le *k-mediods*.\n\nIl existe relativement peu d'implémentation du *k-medians* dans R, nous optons donc ici pour la fonction `kGmedian` du *package* `Gmedian`. Pour le *k-mediods*, nous avons retenu la fonction `pam` du *package* `cluster`.\n\n```{r}\n#| label: kmeansL\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"100%\"\n\nlibrary(Gmedian)\nk_median_res <- kGmedian(X, 4)\n\nlibrary(cluster)\nk_mediods_res <- pam(X,4)\n\n```\n\nJuste pour le plaisir des yeux, nous pouvons cartographier les trois classifications obtenues en nous assurant au préalable de faire coïncider les groupes les plus similaires de nos trois classifications.\n\n```{r}\n#| label: fig-kmeansM\n#| fig-cap: Comparaison géographique des résultats obtenus pour le k-means, le k-medians et le k-medoids\n#| fig-align: center\n#| out-width: \"100%\"\n#| message: false\n#| warning: false\n\nmatrice_gp_kmeans <- dummy_cols(resultats$cluster,\n                                remove_selected_columns = TRUE)\nmatrice_gp_kmedians <- dummy_cols(as.vector(k_median_res$cluster),\n                                  remove_selected_columns = TRUE)\nmatrice_gp_kmedioids <- dummy_cols(k_mediods_res$cluster,\n                                   remove_selected_columns = TRUE)\n\n# Appariement des groupes du k-medians avec ceux du kmeans\nmatrice_gp_kmedians <- geocmeans::groups_matching(as.matrix(matrice_gp_kmeans),\n                                                 as.matrix(matrice_gp_kmedians))\n\n# Appariement des groupes du k-medioids avec ceux du kmeans\nmatrice_gp_kmedioids <- geocmeans::groups_matching(as.matrix(matrice_gp_kmeans),\n                                                   as.matrix(matrice_gp_kmedioids))\n\n# ajouts des colonnes nécessaires à LyonIris\ncolnames(matrice_gp_kmeans) <- paste0(\"groupe_\", 1:4)\ncolnames(matrice_gp_kmedians) <- paste0(\"groupe_\", 1:4)\ncolnames(matrice_gp_kmedioids) <- paste0(\"groupe_\", 1:4)\n\nLyonIris$kmeans <- colnames(matrice_gp_kmeans)[max.col(matrice_gp_kmeans)]\nLyonIris$kmedians <- colnames(matrice_gp_kmedians)[max.col(matrice_gp_kmedians)]\nLyonIris$kmedioids <- colnames(matrice_gp_kmedioids)[max.col(matrice_gp_kmedioids)]\n\n# construction de la figure\ncouleurs <- c(\"#EFBE89\", \"#4A6A9F\", \"#7DB47C\", \"#FAF29C\")\n\nmap1 <- tm_shape(LyonIris) + \n  tm_polygons(col = \"kmeans\", palette = couleurs, lty = 1, lwd = 0.1)\nmap2 <- tm_shape(LyonIris) + \n  tm_polygons(col = \"kmedians\", palette = couleurs, lty = 1, lwd = 0.1)\nmap3 <- tm_shape(LyonIris) + \n  tm_polygons(col = \"kmedioids\", palette = couleurs, lty = 1, lwd = 0.1)\n\ntmap_arrange(map1, map2, map3, \n             ncol = 2, nrow = 2)\n\n```\n\nLes trois cartes sont très similaires (@fig-kmeansM), ce qui signifie que les trois algorithmes tendent à attribuer les observations aux mêmes groupes. Cependant, nous observons des différences, notamment au nord avec des observations alternant entre les groupes 2 et 3 selon la méthode employée. Cela peut notamment signifier que ces observations sont « indécises », qu'il est difficile de les attribuer définitivement à une catégorie en particulier. Pour prendre en compte cette forme d'incertitude, il est possible d'opter pour des méthodes de classification en logique floue.\n\n### Extensions en logique floue : *c-means*, *c-medoids* {#sec-1346}\n\nComme nous l'avons mentionné en introduction de cette section, les méthodes de classification floues ont pour objectif d'évaluer le degré d'appartenance de chaque observation à chaque groupe plutôt que d'attribuer chaque observation à un seul groupe. Il est ainsi possible de repérer des observations incertaines, à cheval entre plusieurs groupes. Nous présentons ici deux algorithmes appartenant à cette famille : le *c-means* et le *c-medoids*. Il s'agit dans les deux cas d'extensions des *k-means* et *k-medoids* vus précédemment.\n\nPour ces deux méthodes, comme pour le *k-means*, le nombre de groupes *k* doit être spécifié. Elles comprennent cependant un paramètre supplémentaire : *m*, appelé paramètre de floutage qui contrôle à quel point le résultat obtenu sera flou ou strict. Une valeur de 1 produit une classification stricte (chaque observation appartient à un seul groupe) et une valeur plus grande conduit à des classifications de plus en plus floues, jusqu'à ce que chaque observation appartienne à un degré identique à chacun des groupes. Il est recommandé de sélectionner *m* en même temps que *k*, car ces deux valeurs influencent simultanément la qualité de la classification. La meilleure approche consiste à tester un ensemble de combinaisons de *m* et de *k* et à comparer les valeurs obtenues pour différents indicateurs de qualité de classification floue. Parmi ces indicateurs, il est notamment recommandé d'utiliser le pourcentage de l'inertie expliquée, l'indice de silhouette pour classification floue, l'indice de Xie et Beni [-@xie1991validity], et de Fukuyama et Sugeno [@fukuyama1989].\n\n#### Mise en œuvre du *c-means* dans R {#sec-13461}\n\nLe *package* `fclust` comprend un très grand nombre de méthodes pour effectuer des classifications floues, nous l'utilisons donc en priorité ici en combinaison avec des fonctions d'interprétation du *package* `geocmeans`.\n\n##### Préparation des données\n\nComme pour le *k-means*, cette méthode nécessite de disposer d'un jeu de données ne comprenant que des variables quantitatives dans la même échelle. Nous commençons donc à nouveau par standardiser nos données. Pour varier les plaisirs, nous optons cette fois-ci pour une transformation des variables dans une échelle allant de 0 à 100.\n\n```{r}\n#| label: cmeansA\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\nlibrary(fclust)\n\ndata(LyonIris)\n\n# NB : LyonIris est un objet spatial, il faut donc extraire uniquement son DataFrame\nX <- st_drop_geometry(LyonIris[c(\"Lden\" , \"NO2\" , \"PM25\" , \"VegHautPrt\" , \"Pct0_14\",\n                                 \"Pct_65\" , \"Pct_Img\",\n                                 \"TxChom1564\" , \"Pct_brevet\" , \"NivVieMed\")])\n\n# changement d'échelle des données (0 à 100)\nto_0_100 <- function(x){\n  return((x-min(x)) / (max(x) - min(x)) * 100)\n}\n\nfor (col in names(X)){\n  X[[col]] <- to_0_100(X[[col]])\n}\n\n```\n\n##### Sélection de *k* et de *m*\n\nLa seconde étape consiste à sélectionner les valeurs optimales pour *k* et *m*. Nous testons ici toutes les valeurs de k de 2 à 7, et les valeurs de *m* de 1,5 à 2,5 (avec des écarts de 0,1).\n\n```{r}\n#| label: fig-cmeansB\n#| fig-align: center\n#| fig-cap: Sélection des paramètres k et m pour l'algorithme c-means\n#| out-width: \"75%\"\n#| message: false\n#| warning: false\nlibrary(e1071)\nset.seed(123)\nms <- seq(1.5,2.5,by = 0.1)\nks <- 2:7\n\n\n# calcul de toutes les combinaisons\ncombinaisons <- expand.grid(ms, ks)\n\neval_indices <- c(\"Explained.inertia\", \"Silhouette.index\", \"FukuyamaSugeno.index\")\n\nvalues <- apply(combinaisons, MARGIN = 1, FUN = function(row){\n  m <- row[[1]]\n  k <- row[[2]]\n  resultats <- FKM(X, k, m)\n  idx <- geocmeans::calcqualityIndexes(as.matrix(X),\n                                       as.matrix(resultats$U), \n                                       m = m,\n                                       indices = eval_indices)\n  return(c(k, m, unlist(idx)))\n})\n\ndf_scores <- data.frame(t(values))\nnames(df_scores) <- c(\"k\", \"m\", \"inertie\", \"silhouette\", \"FukuyamaSugeno\")\n\n# changer l'échelle de l'indice pour un graphique plus joli\ndf_scores$FukuyamaSugeno <- round(df_scores$FukuyamaSugeno/10000,2)\n\n# création de trois figures pour représenter les trois indicateurs\nlibrary(viridis)\n\nplot1 <- ggplot(df_scores) + \n  geom_raster(aes(x = k, y = m, fill = inertie)) + \n  scale_fill_viridis() + \n  scale_x_continuous(breaks = c(2,3,4,5,6,7)) +\n  coord_fixed(ratio=4) + \n  guides(fill = guide_colourbar(barwidth = 5, barheight = 0.5)) +\n  labs(fill = \"Inertie expliquée\") + \n  theme(legend.position = \"bottom\", legend.box = \"horizontal\",\n        legend.title = element_text( size=9), legend.text=element_text(size=8))\n\nplot2 <- ggplot(df_scores) + \n  geom_raster(aes(x = k, y = m, fill = silhouette)) + \n  scale_fill_viridis() + \n  scale_x_continuous(breaks = c(2,3,4,5,6,7)) +\n  coord_fixed(ratio=4) + \n  guides(fill = guide_colourbar(barwidth = 5, barheight = 0.5)) +\n  labs(fill = \"Indice de silhouette\") + \n  theme(legend.position = \"bottom\", legend.box = \"horizontal\",\n        legend.title = element_text( size=9), legend.text=element_text(size=8))\n\nplot3 <- ggplot(df_scores) + \n  geom_raster(aes(x = k, y = m, fill = FukuyamaSugeno)) + \n  scale_fill_viridis() + \n  scale_x_continuous(breaks = c(2,3,4,5,6,7)) +\n  coord_fixed(ratio=4) + \n  guides(fill = guide_colourbar(barwidth = 5, barheight = 0.5)) +\n  labs(fill = \"Indice de Fukuyama et Sugeno\") + \n  theme(legend.position = \"bottom\", legend.box = \"horizontal\",\n        legend.title = element_text( size=9), legend.text=element_text(size=8))\n\nggarrange(plot1, plot2, plot3, ncol = 2, nrow = 2)\n```\n\nLes trois graphiques à la @fig-cmeansB semblent indiquer des solutions différentes. Sans surprise, augmenter le niveau de flou (*m*) réduit l'inertie expliquée, alors qu'augmenter le nombre de groupes (*k*) augmente l'inertie expliquée. L'indice de silhouette indique assez clairement que le nombre de trois groupes serait le meilleur choix, suivi par deux ou quatre groupes, si *m* est inférieur à 1,8. Cependant, ne retenir que trois groupes ne permet d'expliquer que 30% de l'inertie. Afin de nous rapprocher des résultats de l'article original [@2021_4], nous retenons `m = 1,5` et `k = 4`.\n\n##### Application l'algorithme *c-means*\n\nAvec *k* et *m* définis, il ne reste plus qu'à appliquer l'algorithme à nos observations.\n\n```{r}\n#| label: cmeansC\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\nset.seed(123)\ncmeans_resultats <- FKM(X, 4, 1.5)\n```\n\nL'objet obtenu `cmeans_resultats` contient les résultats de la classification. Plus spécifiquement, `cmeans_resultats$U` est la matrice d'appartenance, soit une matrice de taille *n* x *k*, dont chaque case $U_{ij}$ indique la probabilité pour l'observation *i* d'appartenir au groupe *j*. `cmeans_resultats$H` contient le centre des groupes, et `cmeans_resultats$Clus`, le groupe auquel chaque observation à le plus de chances d'appartenir. Pour comparer plus facilement nos résultats avec ceux du *k-means*, nous pouvons changer l'ordre des groupes obtenus pour les faire coïncider avec les groupes les plus similaires obtenus avec la méthode *k-means*.\n\n```{r}\n#| label: cmeansC2\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"75%\"\n\n# changeons l'ordre des groupes\nU <- cmeans_resultats$U\nU2 <- geocmeans::groups_matching(as.matrix(matrice_gp_kmeans), as.matrix(U))\n\n# mais aussi du centre des classes\nidx <- as.integer(gsub(\"Clus \" , \"\", colnames(U2), fixed = TRUE))\nH2 <- cmeans_resultats$H[idx,]\n\n# et recalcul du groupe le plus probable\nClus2 <- data.frame(\n  \"Cluster\" = (1:4)[max.col(U2, ties.method=\"first\")],\n  \"Membership degree\" = apply(U2, MARGIN = 1, max)\n)\n\ncolnames(U2) <- paste(\"Clus\",1:4, sep = \" \")\nrownames(H2) <- paste(\"Clus\",1:4, sep = \" \")\n\ncmeans_resultats$U <- U2\ncmeans_resultats$H <- H2\ncmeans_resultats$Clus <- Clus2\n```\n\n##### Interprétation des résultats\n\nGlobalement, les approches pour interpréter les résultats issus d'une classification obtenue par *c-means* sont les mêmes que pour une classification obtenue par *k-means*.\n\nCommençons donc par créer plusieurs cartes des probabilités d'appartenir aux différents groupes.\n\n```{r}\n#| label: fig-cmeansD\n#| fig-align: center\n#| fig-cap: Cartographie des probabilités d'appartenir aux quatre groupes identifiés par l'algorithme c-means\n#| message: false\n#| warning: false\n#| out-width: \"100%\"\nmaps <- mapClusters(LyonIris, cmeans_resultats$U)\n\ntmap_arrange(maps$ProbaMaps, ncol = 2, nrow = 2)\n```\n\nSur les cartes de la @fig-cmeansD, l'intensité de bleu correspond à la probabilité pour chaque IRIS d'appartenir aux différents groupes. Nous retrouvons les principales structures spatiales que nous avons identifiées avec le *k-means*; cependant, nous pouvons à présent constater que le groupe 1 est bien plus incertain que les autres. Nous pouvons une fois encore générer un graphique en radar pour comparer les profils des quatre groupes.\n\n```{r}\n#| label: fig-cmeansE\n#| fig-align: center\n#| fig-cap: Graphique en radar pour les résultats du c-means\n#| out-width: \"100%\"\n#| message: false\n#| warning: false\n\npar(mfrow=c(3,2), mai = c(0.1,0.1,0.1,0.1))\nspiderPlots(X, cmeans_resultats$U,\n            chartcolors = c(\"#EFBE89\", \"#4A6A9F\", \"#7DB47C\", \"#FAF29C\"))\n\n```\n\nSans surprise, nous retrouvons essentiellement les profils que nous avons obtenus avec le *k-means* dans la @fig-cmeansE. Pour compléter la lecture des résultats, il est nécessaire de se pencher sur le tableau des statistiques descriptives des différents groupes. Une fois encore, nous proposons d'utiliser la fonction `summarizeClusters` du *package* `geocmeans`. Notez que cette fonction calcule les statistiques descriptives pondérées en fonction de l'appartenance des observations aux groupes. Ainsi, une observation ayant une faible chance d'appartenir à un groupe ne contribue que faiblement aux statistiques descriptives de ce groupe.\n\n```{r}\n#| label: cmeansF\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"100%\"\ndf <-  st_drop_geometry(LyonIris[c(\"Lden\" , \"NO2\" , \"PM25\" , \"VegHautPrt\" , \"Pct0_14\",\n                                   \"Pct_65\" , \"Pct_Img\", \"TxChom1564\",\n                                   \"Pct_brevet\" , \"NivVieMed\")])\n\ntableaux <- summarizeClusters(data = df, \n                  belongmatrix = cmeans_resultats$U,\n                  weighted = TRUE, dec = 1)\n\ntableau_tot <- do.call(rbind, tableaux)\n```\n\n```{r}\n#| label: tbl-cmeansG\n#| tbl-cap: Description des groupes avec la méthode c-means\n#| echo: false\n#| message: false\n#| warning: false\ntableau_tot <- apply(tableau_tot, MARGIN = 2, unlist)\ntableau_tot <- data.frame(tableau_tot)\ntableau_tot$stat <- rep(row.names(tableaux[[1]]),4)\ntableau_tot <- tableau_tot[, c(\"stat\", names(X2))]\n\nok_col_names <- gsub(\"_\" , \"\", names(X2), fixed = TRUE)\n\nmy_table <- knitr::kable(tableau_tot, row.names = FALSE, \n                       format.args = list(decimal.mark = ',', big.mark = \" \"),\n\t\t\t\t\t   col.names = c(\"\", ok_col_names),\n                       align=c(\"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"))\n\nmy_table2 <- group_rows(my_table,\n           index = c(\n             \"groupe 1\" = 9,\n             \"groupe 2\" = 9,\n             \"groupe 3\" = 9,\n             \"groupe 4\" = 9\n             )\n  )\n\nmy_table2 <- gsub(pattern = \"NApadding\", replacement = \"padding\",x = my_table2, fixed = TRUE)\nmy_table2 <- gsub(pattern = \"padding-left: 2em\", replacement = \"padding-left: 0em\",x = my_table2, fixed = TRUE)\nmy_table2 <- gsub(pattern = \"<th>\", replacement = '<th style = \"padding : 0em\" ; font-size = \"13px\">',x = my_table2, fixed = TRUE)\nmy_table2\n```\n\n#### Mise en œuvre du *c-medoids* dans R {#sec-13462}\n\nLa méthode du *c-medoids* dans R peut être mise en œuvre avec la fonction `FKM.med` du *package* `fclust`. Le processus d'analyse et de validation est identique à celui présenté ci-dessus pour le *c-means*. Nous ne donnons donc pas un exemple complet de la méthode.\n\n::: bloc_aller_loin\n::: bloc_aller_loin-header\n::: bloc_aller_loin-icon\n:::\n**Stabilité des groupes obtenus par les méthodes de nuées dynamiques**\n:::\n::: bloc_aller_loin-body\nPuisque la méthode *k-means* et ses variantes reposent sur une initialisation aléatoire de leur algorithme, les résultats peuvent varier en fonction de cet état de départ. Dans certains contextes, il est possible que les résultats obtenus varient significativement, ce qui signifie que les groupes obtenus ne sont pas représentatifs de la population étudiée. Une solution pour vérifier si ce problème se pose est simplement de relancer l'algorithme un grand nombre de fois (généralement 1000) et de comparer les résultats obtenus au cours de ces réplications.\n\nCette méthode est rarement implémentée directement et requiert d'écrire sa propre fonction. `geocmeans` dispose d'une fonction déjà existante, mais ne pouvant être appliquée qu'avec l'algorithme *c-means*. Nous proposons ici une implémentation pour la méthode *k-means* qui peut facilement être adaptée aux autres méthodes de classifications heuristiques.\n\nLa démarche à suivre est la suivante : \n\n1. Appliquer l'algorithme une première fois pour obtenir une classification de référence à laquelle nous comparerons toutes les réplications.\n\n2. Effectuer 1000 itérations au cours desquelles : \n    + Une nouvelle classification est calculée.\n    + Les groupes obtenus sont comparés à ceux de la classification de référence.\n    + L'indice de Jacard est calculé entre les groupes des deux classifications.\n    + Les valeurs de l'indice de Jacard sont enregistrées.\n    + Les centres des groupes sont enregistrés.\n  \nAinsi, nous obtenons 1000 valeurs de l'indice de Jacard pour chaque groupe. Cet indice permet de mesurer le degré d'accord entre deux variables (ici les probabilités d'appartenance des observations au même groupe pour deux classifications différentes.) Une valeur moyenne en dessous de 0,5 indique qu'un groupe est très instable, car nous obtenons des résultats très différents lors des réplications. Une valeur entre 0,6 et 0,75 indique qu'un groupe semble bien exister dans les données, bien que marqué par une certaine incertitude. Une valeur au-dessus de 0,8 indique un groupe bien identifié et stable.\n\nNous obtenons également les centres des groupes des 1000 classifications. Il est ainsi possible de représenter leurs histogrammes et de déterminer si les centres des groupes sont stables (unimodalité et faible variance) ou incertains (plusieurs modes et/ou forte variance).\n\n```{r}\n#| label: kmeansStab1\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"100%\"\n\n# X sera le jeu de données pour la classification\n# clust_ref sera le vecteur indiquant le groupe de chaque observation obtenu par kmeans\n# nsim sera le nombre de simulations à effectuer\nkmeans_stability <- function(X, clust_ref, nsim, verbose = TRUE){\n  \n  # définition de la matrice d'appartenance originale\n  k <- length(unique(clust_ref))\n  ref_mat <- dummy_cols(clust_ref, remove_selected_columns = TRUE)\n  colnames(ref_mat) <- paste0(\"groupe_\",1:k)\n  \n  # lancement des itérations\n  sim_resultats <- lapply(1:nsim, function(i){\n    \n     # afficher la progression si requis\n    if(verbose){\n      print(paste0(\"iteration numeros : \", i, \"/\", nsim))\n    }\n    # calculer le kmeans\n    km_res <- kmeans(X, k)\n    sim_mat <- dummy_cols(km_res$cluster, remove_selected_columns = TRUE)\n    \n    # ajustement de l'ordre des groupes avec geocmeans\n    sim_mat <- groups_matching(as.matrix(ref_mat), as.matrix(sim_mat))\n    \n    # calcul des indices de jacard\n    jac_idx <- sapply(1:k, function(j){\n      calc_jaccard_idx(sim_mat[,j], ref_mat[,j])\n    })\n    \n    # récuperation des centres des groupes\n    idx <- as.integer(gsub(\".data_\" , \"\", colnames(sim_mat), fixed = TRUE))\n    centers <- data.frame(km_res$centers)\n    centers <- centers[idx,]\n    centers$groupe <- 1:k\n    \n    return(list(\n      \"jac_idx\" = jac_idx,\n      \"centers\" = centers\n    ))\n    \n  })\n  \n  # les simulations sont finies, nous pouvons combiner les résultats\n  all_jac_values <- do.call(rbind, lapply(sim_resultats, function(x){x$jac_idx}))\n  all_centers <- do.call(rbind, lapply(sim_resultats, function(x){x$centers}))\n  return(list(\n    \"jacard_values\" = all_jac_values,\n    \"centers\" = all_centers\n  ))\n}\n\n```\n\nIl ne nous reste plus qu'à utiliser notre nouvelle fonction pour déterminer si les groupes obtenus avec notre *k-means* sont stables.\n\n```{r}\n#| label: kmeansStab2\n#| fig-align: center\n#| message: false\n#| warning: false\n#| out-width: \"100%\"\ndata(LyonIris)\nset.seed(123)\n\n# NB : LyonIris est un objet spatial, il faut donc extraire uniquement son DataFrame\nX <- st_drop_geometry(LyonIris[c(\"Lden\" , \"NO2\" , \"PM25\" , \"VegHautPrt\" , \"Pct0_14\" , \"Pct_65\" , \"Pct_Img\",\n                     \"TxChom1564\" , \"Pct_brevet\" , \"NivVieMed\")])\n\n# Centrage et réduction de chaque colonne du DataFrame\nfor (col in names(X)){\n  X[[col]] <- scale(X[[col]], center = TRUE, scale = TRUE)\n}\n\n# calcul du kmeans de référence\nkmeans_ref <- kmeans(X, 4)\n\n# application de la fonction de stabilité\nstab_results <- kmeans_stability(X, kmeans_ref$cluster, nsim = 1000, verbose = FALSE)\n\n```\n\nNous pouvons à présent vérifier la stabilité de nos quatre groupes.\n\n```{r}\n#| label: fig-kmeansStab3\n#| fig-align: center\n#| fig-cap: Indices de Jacard obtenus sur 1000 réplications du k-means\n#| message: false\n#| warning: false\n#| out-width: \"90%\"\n\njacard_values <- data.frame(stab_results$jacard_values)\nnames(jacard_values) <- paste(\"groupe\", 1:4, sep = \"_\")\n\ndf <- reshape2::melt(jacard_values)\ndf$groupes <- as.factor(df$variable)\n\nggplot(df) + \n  geom_histogram(aes(x = value), bins = 30) + \n  facet_wrap(vars(groupes), ncol = 2) + \n  labs(x = \"\", y = \"Indice de Jacard\")\n\n```\n\nLa @fig-kmeansStab3 indique clairement que les groupes 1 et 2 sont très stables, car les valeurs de Jacard obtenues sont le plus souvent supérieures à 0,75. Le groupe 3 a le plus souvent des valeurs légèrement inférieures aux deux premiers groupes, mais tout de même bien supérieures à 0,5. En revanche, le groupe 4 a un grand nombre de valeurs inférieures à 0,5 indiquant une tendance du groupe à se dissoudre lors des réplications.\n\nConsidérant que le dernier groupe est le plus instable, nous décidons d'observer les valeurs des centres qu'il obtient pour les différentes réplications.\n\n```{r}\n#| label: fig-kmeansStab4\n#| fig-align: center\n#| fig-cap: Distributions des valeurs des centres du groupe 4 sur 1000 itérations\n#| message: false\n#| warning: false\n#| out-width: \"90%\"\n\ncenters_groupe4 <- subset(stab_results$centers, stab_results$centers$groupe == 4)\ncenters_groupe4$groupe <- NULL\n\ndf <- reshape2::melt(centers_groupe4)\ndf$variable <- as.factor(df$variable)\n\nggplot(df) + \n  geom_histogram(aes(x = value), bins = 30) + \n  facet_wrap(vars(variable), ncol = 3, scales = \"free\") + \n  labs(x = \"\", y = \"\")\n\n```\n\nLes différents histogrammes de la @fig-kmeansStab4 indiquent clairement que pour plusieurs variables (`Lden`, `NO2`, `PM25`, `Pct_Img`, et `NivVieMed`) les caractéristiques du groupe 4 varient grandement sur l'ensemble des réplications. Nous pouvons comparer ce graphique à celui du groupe 2 qui est bien plus stable.\n\n```{r}\n#| label: fig-kmeansStab5\n#| fig-cap: Distributions des valeurs des centres du groupe 2 sur 1000 itérations\n#| fig-align: center\n#| out-width: \"90%\"\n#| message: false\n#| warning: false\n\ncenters_groupe2 <- subset(stab_results$centers, stab_results$centers$groupe == 2)\ncenters_groupe2$groupe <- NULL\n\ndf <- reshape2::melt(centers_groupe2)\ndf$variable <- as.factor(df$variable)\n\nggplot(df) + \n  geom_histogram(aes(x = value), bins = 30) + \n  facet_wrap(vars(variable), ncol = 3, scales = \"free\") + \n  labs(x = \"\", y = \"\")\n\n```\n\nNous pouvons constater une plus faible variance des résultats obtenus (en regardant notamment l'axe horizontal) pour les centres des groupes à la @fig-kmeansStab5.\n:::\n:::\n\n\n## Conclusion sur la cinquième partie {#sec-135}\n\nDans le cadre de cette cinquième partie du livre, nous avons abordé les principales méthodes factorielles et les principales méthodes de classification non supervisée. Les premières sont des méthodes de réduction de données puisqu'elles permettent de résumer l'information d'un tableau en quelques nouvelles variables synthétiques. Les secondes permettent de regrouper les observations d'un tableau en plusieurs groupes homogènes. Il existe donc une complémentarité évidente entre ces deux groupes de méthodes : si le tableau initial comprend un grand nombre de variables, il est possible de lui appliquer une méthode factorielle produisant de nouvelles variables synthétiques qui sont ensuite utilisées pour calculer une méthode de classification non supervisée.\n\n![Complémentarité entre les méthodes factorielles et les méthodes de classification non supervisée](images/Chap13/AnalysesFactoClassif.png){#fig-MFMCComplementarite width=\"45%\" fig-align=\"center\"}\n\n## Quiz de révision du chapitre {#sec-136}\n\n```{r}\n#| label: quizChapitre13\n#| echo: false\n#| warning: false\n#| results: asis\nsource(\"code_complementaire/QuizzFunctions.R\")\nquizz_classif <- quizz(\"quiz/Chapitre13_classification.yml\", \"quizz_classif\")\nrender_quizz(quizz_classif)\n```\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["css/quizlib.min.css"],"output-file":"13-MethodeClassification.html"},"language":{"toc-title-document":"Table des matières","toc-title-website":"Sur cette page","related-formats-title":"Autres formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"La source","section-title-abstract":"Résumé","section-title-appendices":"Annexes","section-title-footnotes":"Notes de bas de page","section-title-references":"Les références","section-title-reuse":"Réutilisation","section-title-copyright":"Droits d'auteur","section-title-citation":"Citation","appendix-attribution-cite-as":"Veuillez citer ce travail comme suit :","appendix-attribution-bibtex":"BibTeX","title-block-author-single":"Auteur·rice","title-block-author-plural":"Auteurs","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Date de publication","title-block-modified":"Modifié","callout-tip-title":"Astuce","callout-note-title":"Note","callout-warning-title":"Avertissement","callout-important-title":"Important","callout-caution-title":"Mise en garde","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Montrer tout le code","code-tools-hide-all-code":"Cacher tout le code","code-tools-view-source":"Voir les sources","code-tools-source-code":"Code source","code-line":"Ligne","code-lines":"Lignes","copy-button-tooltip":"Copier vers le presse-papier","copy-button-tooltip-success":"Copié","repo-action-links-edit":"Modifier cette page","repo-action-links-source":"Voir la source","repo-action-links-issue":"Signaler un problème ou<br>formuler une suggestion","back-to-top":"Retour au sommet","search-no-results-text":"Pas de résultats","search-matching-documents-text":"documents trouvés","search-copy-link-title":"Copier le lien vers la recherche","search-hide-matches-text":"Cacher les correspondances additionnelles","search-more-match-text":"correspondance de plus dans ce document","search-more-matches-text":"correspondances de plus dans ce document","search-clear-button-title":"Effacer","search-detached-cancel-button-title":"Annuler","search-submit-button-title":"Envoyer","search":"Recherche","toggle-section":"Basculer la section","toggle-sidebar":"Basculer la barre latérale","toggle-dark-mode":"Basculer le mode sombre","toggle-reader-mode":"Basculer en mode lecteur","toggle-navigation":"Basculer la navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Théorème","crossref-lem-title":"Lemme","crossref-cor-title":"Corollaire","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Définition","crossref-exm-title":"Exemple","crossref-exr-title":"Exercice","crossref-ch-prefix":"Chapitre","crossref-apx-prefix":"Annexe","crossref-sec-prefix":"Section","crossref-eq-prefix":"Équation","crossref-lof-title":"Liste des Figures","crossref-lot-title":"Liste des Tables","crossref-lol-title":"Liste des Listings","environment-proof-title":"Preuve","environment-remark-title":"Remarque","environment-solution-title":"Solution","listing-page-order-by":"Trier par","listing-page-order-by-default":"Ordre par défaut","listing-page-order-by-date-asc":"Le plus ancien","listing-page-order-by-date-desc":"Le plus récent","listing-page-order-by-number-desc":"Descendant","listing-page-order-by-number-asc":"Ascendant","listing-page-field-date":"Date","listing-page-field-title":"Titre","listing-page-field-description":"Description","listing-page-field-author":"Auteur·rice","listing-page-field-filename":"Nom de fichier","listing-page-field-filemodified":"Modifié","listing-page-field-subtitle":"Sous-titre","listing-page-field-readingtime":"Temps de lecture","listing-page-field-categories":"Catégories","listing-page-minutes-compact":"{0} min.","listing-page-category-all":"Tous","listing-page-no-matches":"Aucun article correspondant"},"metadata":{"lang":"fr","fig-responsive":true,"quarto-version":"1.3.353","license":"CC BY-SA","crossref":{"fig-prefix":"figure","tbl-prefix":"tableau","sec-prefix":"section","eq-prefix":"équation","fig-title":"Figure","tbl-title":"Tableau","eq-title":"Équation"},"bibliography":["references.bib"],"csl":"StyleINRS.csl","colorlinks":true,"theme":{"light":["cosmo","css/r4ds.scss"]},"fontsize":"11pt","mainfont":"Helvetica Neue,Helvetica,Arial,sans-serif","monofont":"SFMono-Regular,Menlo,Monaco,Consolas,\"Liberation Mono\",\"Courier New\",monospace"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}