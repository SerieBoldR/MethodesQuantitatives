{
  "hash": "f66025bbcfa3a25d920d34aeb04bf5d9",
  "result": {
    "markdown": "# Relation linéaire entre deux variables quantitatives {#sec-chap04}\n\nDans le cadre de ce chapitre, nous présentons les trois principales méthodes permettant d'explorer la relation linéaire entre deux variables quantitatives, soit la covariance, la corrélation et la régression linéaire simple.\n\n\n::: bloc_package\n::: bloc_package-header\n::: bloc_package-icon\n:::\n**Liste des *packages* utilisés dans ce chapitre**\n:::\n::: bloc_package-body\n* Pour créer des graphiques :\n  - `ggplot2`, le seul, l'unique!\n  - `ggpubr` pour combiner des graphiques et réaliser des diagrammes quantiles-quantiles.\n* Pour manipuler des données : \n  - `dplyr` notamment pour les fonctions `group_by`, `summarize` et les pipes %>%.\n* Pour les corrélations : \n  - `boot` pour réaliser des corrélations avec *bootstrap*.\n  - `correlation`, de l'ensemble de packages `easy_stats`, offrant une large panoplie de mesures de corrélation.\n  - `corrplot` pour créer des graphiques de matrices de corrélation.\n  - `Hmisc` pour calculer des corrélations de Pearson et Spearman.\n  - `ppcor` pour calculer des corrélations partielles.\n  - `psych` pour obtenir une matrice de  corrélation (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de *p*.\n  - `stargazer` pour créer de beaux tableaux d’une matrice de corrélation en HTML, en LaTeX ou en ASCII.\n* Autres *packages* :\n  - `foreign` pour importer des fichiers externes.\n  - `MASS` pour générer des échantillons normalement distribués.\n  - `stargazer` pour imprimer des tableaux.\n:::\n:::\n\n\n::: bloc_objectif\n::: bloc_objectif-header\n::: bloc_objectif-icon\n:::\n**Deux variables continues varient-elles dans le même sens ou bien en sens contraire?**\n:::\n::: bloc_objectif-body\nRépondre à cette question est une démarche exploratoire classique en sciences sociales puisque les données socioéconomiques sont souvent associées linéairement. En d'autres termes, lorsque l'une des deux variables tant à augmenter, l'autre augmente également ou diminue systématiquement.\n\nEn études urbaines, nous pourrions vouloir vérifier si certaines variables socioéconomiques sont associées positivement ou négativement à des variables environnementales jugées positives (comme la couverture végétale ou des mesures d’accessibilité spatiale aux parcs) ou négatives (pollutions atmosphériques et sonores). \n\nPar exemple, au niveau des secteurs de recensement d’une ville canadienne, nous pourrions vouloir vérifier si le revenu médian des ménages et le coût moyen du loyer varient dans le même sens que la couverture végétale; ou encore s'ils varient en sens inverse des niveaux moyens de dioxyde d’azote ou de bruit routier.\n\nPour évaluer la linéarité entre deux variables continues, deux statistiques descriptives sont utilisées : la **covariance** ([section @sec-042]) et la **corrélation** ([section @sec-043]).\n:::\n:::\n\n\n## Bref retour sur le postulat de la relation linéaire {#sec-041}\n\nVérifier le postulat de la linéarité consiste à évaluer si deux variables quantitatives varient dans le même sens ou bien en sens contraire. Toutefois, la relation entre deux variables quantitatives n’est pas forcément linéaire. En guise d'illustration, la @fig-fig2 permet de distinguer quatre types de relations :\n\n* Le cas **a** illustre une relation linéaire positive entre les deux variables puisqu’elles vont dans le même sens. Autrement dit, quand les valeurs de *X* augmentent, celles de *Y* augmentent aussi. En guise d'exemple, pour les secteurs de recensement d'une métropole donnée, il est fort probable que le coût moyen du loyer soit associé positivement avec le revenu médian des ménages. Graphiquement parlant, il est clair qu'une droite dans ce nuage de points résumerait efficacement la relation entre ces deux variables.\n\n* Le cas **b** illustre une relation linéaire négative entre les deux variables puisqu’elles vont en sens inverse. Autrement dit, quand les valeurs de *X* augmentent, celles de *Y* diminuent, et inversement. En guise d'exemple, pour les secteurs de recensement d'une métropole donnée, il est fort probable que le revenu médian des ménages soit associé négativement avec le taux de chômage. De nouveau, une droite résumerait efficacement cette relation.\n\n* Pour le cas **c**, il y a une relation entre les deux variables, mais qui n’est pas linéaire. Le nuage de points entre les deux variables prend d’ailleurs une forme parabolique qui traduit une relation curvilinéaire. Concrètement, nous observons une relation positive jusqu'à un certain seuil, puis une relation négative. \n \n* Pour le cas **d**, la relation entre les deux variables est aussi curvilinéaire; d'abord négative, puis positive.\n\n\n![Relations linéaires et curvilinéaires entre deux variables continues](images/Chap04/LineaireCurvilineaire.jpg){#fig-fig2 width=\"85%\" fig-align=\"center\"}\n\nPrenons un exemple concret pour illustrer le cas **c**. Dans une étude portant sur l'équité environnementale et la végétation à Montréal, Pham *et al.* [-@PhamApparicioSeguin2012] ont montré qu'il existe une relation curvilinéaire entre l'âge médian des bâtiments résidentiels (axe des abscisses) et les couvertures végétales (axes des ordonnées) :\n\n* La couverture de la végétation totale et celle des arbres augmentent quand l'âge médian des bâtiments croît jusqu'à atteindre un pic autour de 60 ans (autour de 1950). Nous pouvons supposer que les secteurs récemment construits, surtout ceux dans les banlieues, présentent des niveaux de végétation plus faibles. Au fur et à mesure que le quartier vieillit, les arbres plantés lors du développement résidentiel deviennent matures — canopée plus importante –, d'où l'augmentation des valeurs de la couverture végétale totale et de celle des arbres.\n\n*  Par contre, dans les secteurs développés avant les années 1950, la densité du bâti est plus forte, laissant ainsi moins de place pour la végétation, ce qui explique une diminution des variables relatives à la couverture végétale (@fig-fig3).\n\n\n![Exemples de relations curvilinéaires](images/Chap04/ExRelCurvi.jpg){#fig-fig3 width=\"65%\" fig-align=\"center\"}\n\nDans les sous-sections suivantes, nous décrivons deux statistiques descriptives et exploratoires – la covariance ([section @sec-042]) et la corrélation ([section @sec-043]) – utilisées pour évaluer la **relation linéaire** entre deux variables continues (cas **a** et **b** à la @fig-fig2). Ces deux mesures permettent de mesurer le degré d'association entre deux variables, sans que l'une soit la variable dépendante (variable à expliquer) et l'autre, la variable indépendante (variable explicative). Puis, nous décrivons la régression linéaire simple ([section @sec-044])  qui permet justement de prédire une variable dépendante (_Y_) à partir d'une variable indépendante (_X_).\n\n## Covariance {#sec-042}\n\n### Formulation {#sec-0421} \n\nLa covariance (@eq-cov), écrite $cov(x,y)$, est égale à la moyenne du produit des écarts des valeurs des deux variables par rapport à leurs moyennes respectives :\n\n\n$$\ncov(x,y) = \\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1} = \\frac{covariation}{n-1}\n$$ {#eq-cov}\n\navec $n$ étant le nombre d’observations; $\\bar{x}$ et $\\bar{y}$ (prononcez x et y barre) étant les moyennes respectives des variables *X* et *Y*.\n\n### Interprétation {#sec-0422} \n\nLe numérateur de l'@eq-cov représente la covariation, soit la somme du produit des déviations des valeurs $x_{i}$ et $y_{i}$ par rapport à leurs moyennes respectives ($\\bar{x}$ et $\\bar{y}$). La covariance est donc la covariation divisée par le nombre d’observations, soit la moyenne de la covariation. Sa valeur peut être positive ou négative :  \n\n* Positive quand les deux variables varient dans le même sens, c'est-à-dire lorsque les valeurs de la variable _X_ s'éloignent de la moyenne, les valeurs de _Y_ s'éloignent aussi dans le même sens; et elle est négative pour une situation inverse. \n\n* Quand la covariance est égale à 0, il n’y a pas de relation entre les variables _X_ et _Y_. Plus sa valeur absolue est élevée, plus la relation entre les deux variables *X* et *Y* est importante. \n \nAinsi, la covariance correspond à un centrage des variables, c’est-à-dire à soustraire à chaque valeur de la variable sa moyenne correspondante. L'inconvénient majeur de l'utilisation de la covariance est qu'elle est tributaire des unités de mesure des deux variables. Par exemple, si nous calculons la covariance entre le pourcentage de personnes à faible revenu et la densité de population (habitants au km^2^) au niveau des secteurs de recensement de la région métropolitaine de Montréal, nous obtenons une valeur de covariance de 33 625. En revanche, si la densité de population est exprimée en milliers d'habitants au km^2^, la valeur de la covariance sera de 33,625, alors que la relation linéaire entre les deux variables reste la même comme illustré à la @fig-covariance. Pour remédier à ce problème, nous privilégions l'utilisation du coefficient de corrélation.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Covariance et unités de mesure](04-bivarieeQuantiQuanti_files/figure-html/fig-covariance-1.png){#fig-covariance fig-align='center' width=75%}\n:::\n:::\n\n\n\n## Corrélation {#sec-043}\n\n### Formulation {#sec-0431} \nLe coefficient de corrélation de Pearson ($r$) est égal à la covariance (numérateur) divisée par le produit des écarts-types des deux variables *X* et *Y* (dénominateur). Il représente une standardisation de la covariance. Autrement dit, le coefficient de corrélation repose sur un centrage (moyenne = 0) et une réduction (variance = 1) des deux variables, c’est-à-dire qu'il faut soustraire de chaque valeur sa moyenne correspondante et la diviser par son écart-type. Il correspond ainsi à la moyenne du produit des deux variables centrées réduites. Il s'écrit alors :\n\n$$\nr_{xy} = \\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{(n-1)\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2(y_i - \\bar{y})^2}}=\\sum_{i=1}^n\\frac{Zx_iZy_i}{n-1}\n$$ {#eq-cor}\n\navec $n$ étant le nombre d’observations; $\\bar{x}$ et $\\bar{y}$ étant les moyennes respectives des variables *X* et *Y*.\n\nLa syntaxe ci-dessous démontre que le coefficient de corrélation de Pearson est bien égal à la moyenne du produit de deux variables centrées réduites.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"MASS\")\nN <- 1000     # nombre d'observations\nmoy_x <- 50   # moyenne de x\nmoy_y <- 40   # moyenne de y\nsd_x <- 10    # écart-type de x\nsd_y <- 8     # écart-type de y\nrxy <- .80 # corrélation entre X et Y\n## création de deux variables fictives normalement distribuées et corrélées entre elles\n# Création d'une matrice de covariance\ncov <- matrix(c(sd_x^2,  rxy*sd_x*sd_y, rxy*sd_x*sd_y, sd_y^2), nrow = 2)\n# Création du tableau de données avec deux variables\ndf1 <- as.data.frame(mvrnorm(N, c(moy_x, moy_y), cov))\n# Centrage et réduction des deux variables\ndf1$zV1 <- scale(df1$V1, center = TRUE, scale = TRUE)\ndf1$zV2 <- scale(df1$V2, center = TRUE, scale = TRUE)\n# Corrélation de Pearson\ncor1 <- cor(df1$V1, df1$V2)\ncor2 <- sum(df1$zV1*df1$zV2) / (nrow(df1)-1)\ncat(\"Corrélation de Pearson = \", round(cor1,5),\n    \"\\nMoyenne du produit des variables centrées réduites =\", round(cor2,5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorrélation de Pearson =  0.78657 \nMoyenne du produit des variables centrées réduites = 0.78657\n```\n:::\n:::\n\n\n\n### Interprétation {#sec-0432} \n\nLe coefficient de corrélation $r$ varie de −1 à 1 avec :\n\n* 0 quand il n’y a pas de relation linéaire entre les variables _X_ et _Y_;\n* −1 quand il y a une relation linéaire négative parfaite;\n* 1 quand il y a une relation linéaire positive parfaite (@fig-coeffPearson). \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Relations entre deux variables continues et coefficients de corrélation de Pearson](04-bivarieeQuantiQuanti_files/figure-html/fig-coeffPearson-1.png){#fig-coeffPearson fig-align='center' width=75%}\n:::\n:::\n\n\nConcrètement, le signe du coefficient de corrélation indique si la relation est positive ou négative et la valeur absolue du coefficient indique le degré d’association entre les deux variables. Reste à savoir comment déterminer qu’une valeur de corrélation est faible, moyenne ou forte. En sciences sociales, nous utilisons habituellement les intervalles de valeurs reportés au @tbl-tableIntervallesCorrelation. Toutefois, ces seuils sont tout à fait arbitraires. En effet, dépendamment de la discipline de recherche (sciences sociales, sciences de la santé, sciences physiques, etc.) et des variables à l’étude, l’interprétation d’une valeur de corrélation peut varier. Par exemple, en sciences sociales, une valeur de corrélation de 0,2 est considérée comme très faible alors qu’en sciences de la santé, elle pourrait être considérée comme intéressante. À l’opposé, une valeur de 0,9 en sciences physiques pourrait être considérée comme faible. Il convient alors d’utiliser ces intervalles avec précaution.\n\n\n::: {#tbl-tableIntervallesCorrelation .cell tbl-cap='Intervalles pour l’interprétation du coefficient de corrélation habituellement utilisés en sciences sociales'}\n::: {.cell-output-display}\n|Corrélation |Négative       |Positive     |\n|:-----------|:--------------|:------------|\n|Faible      |de −0,3 à 0,0  |de 0,0 à 0,3 |\n|Moyenne     |de −0,5 à −0,3 |de 0,3 à 0,5 |\n|Forte       |de −1,0 à −0,5 |de 0,5 à 1,0 |\n:::\n:::\n\n\nLe coefficient de corrélation mis au carré représente le coefficient de détermination et indique la proportion de la variance de la variable _Y_ expliquée par la variable _X_ et inversement. Par exemple, un coefficient de corrélation de −0,70 signale que 49 % de la variance de la variable de _Y_ est expliquée par _X_ (@fig-coefCorrVar).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Coefficient de corrélation et proportion de la variance expliquée](04-bivarieeQuantiQuanti_files/figure-html/fig-coefCorrVar-1.png){#fig-coefCorrVar fig-align='center' width=75%}\n:::\n:::\n\n\n**Condition d'application.** L'utilisation du coefficient de corrélation de Pearson nécessite que les deux variables continues soient normalement distribuées et qu'elles ne comprennent pas de valeurs aberrantes ou extrêmes. D’ailleurs, plus le nombre d’observations est réduit, plus la présence de valeurs extrêmes a une répercussion importante sur le résultat du coefficient de corrélation de Pearson. En guise d’exemple, dans le nuage de points à gauche de la @fig-ValExtremes, il est possible d’identifier des valeurs extrêmes qui se démarquent nettement dans le jeu de données : six observations avec une densité de population supérieure à 20 000 habitants au km^2^ et deux observations avec un pourcentage de 65 ans et plus supérieur à 55 %. Si l'on supprime ces observations (ce qui est défendable dans ce contexte) – soit moins d'un pour cent des observations du jeu de données initial –, la valeur du coefficient de corrélation passe de −0,158 à −0,194, signalant une augmentation du degré d'association entre les deux variables.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Illustation de l’effet des valeurs extrêmes sur le coefficient de Pearson](04-bivarieeQuantiQuanti_files/figure-html/fig-ValExtremes-1.png){#fig-ValExtremes fig-align='center' width=75%}\n:::\n:::\n\n\n\n### Corrélations pour des variables anormalement distribuées (coefficient de Spearman, tau de Kendall) {#sec-0433} \n\nLorsque les variables sont fortement anormalement distribuées, le coefficient de corrélation de Pearson est peu adapté pour analyser leurs relations linéaires. Il est alors conseillé d'utiliser deux statistiques non paramétriques : principalement, le coefficient de corrélation de Spearman (_rho_) et secondairement, le tau ($\\tau$) de Kendall, qui varient aussi tous deux de −1 à 1. \nCalculé sur les rangs des deux variables, le **coefficient de Spearman** est le rapport entre la covariance des deux variables de rangs sur les écarts-types des variables de rangs. En d'autres termes, il représente simplement le coefficient de Pearson calculé sur les rangs des deux variables :\n\n$$\nr_{xy} = \\frac{cov(rg_{x}, rg_{y})}{\\sigma_{rg_{x}}\\sigma_{rg_{y}}}\n$$ {#eq-spearman}\n\nLa syntaxe  ci-dessous démontre clairement que le coefficient de Spearman est bien le coefficient de Pearson calculé sur les rangs ([section @sec-0431]).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read.csv(\"data/bivariee/sr_rmr_mtl_2016.csv\")\n# Transformation des deux variables en rangs\ndf$HabKm2_rang <- rank(df$HabKm2)\ndf$A65plus_rang <- rank(df$A65plus)\n# Coefficient de Spearman avec la fonction cor et la méthode spearman\ncat(\"Coefficient de Spearman = \", \n    round(cor(df$HabKm2, df$A65plus, method = \"spearman\"),5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficient de Spearman =  -0.11953\n```\n:::\n\n```{.r .cell-code}\n# Coefficient de Pearson sur les variables transformées en rangs\ncat(\"Coefficient de Pearson calculé sur les variables transformées en rangs = \", \n    round(cor(df$HabKm2_rang, df$A65plus_rang, method = \"pearson\"),5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficient de Pearson calculé sur les variables transformées en rangs =  -0.11953\n```\n:::\n\n```{.r .cell-code}\n# Vérification avec l'équation\ncat(\"Covariance divisée par le produit des écarts-types sur les rangs :\",\n    round(cov(df$HabKm2_rang, df$A65plus_rang) / (sd(df$HabKm2_rang)*sd(df$A65plus_rang)),5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCovariance divisée par le produit des écarts-types sur les rangs : -0.11953\n```\n:::\n:::\n\n\nLe **tau de Kendall** est une autre mesure non paramétrique calculée comme suit :\n\n$$\n\\tau = \\frac{n_{c}-n_{d}}{\\frac{1}{2}n(n-1)}\n$$ {#eq-tau}\n\navec $n_{c}$ et $n_{d}$ qui sont respectivement les nombres de paires d'observations **c**oncordantes et **d**iscordantes; et le dénominateur étant le nombre total de paires d'observations. Des paires sont dites concordantes quand les valeurs des deux observations vont dans le même sens pour les deux variables ($x_{i}>x_{j}$ et $y_{i}>y_{j}$ ou  $x_{i}<x_{j}$ et $y_{i}<y_{j}$), et discordantes quand elles vont en sens contraire ($x_{i}>x_{j}$ et $y_{i}<y_{j}$ ou $x_{i}<x_{j}$ et $y_{i}>y_{j}$). Contrairement au calcul du coefficient de Spearman, celui du tau Kendall peut être chronophage : plus le nombre d'observations est élevé, plus les temps de calcul et la mémoire utilisée sont importants. En effet, avec _n_ = 1000, le nombre de paires d'observations (${\\mbox{0,5}\\times n(n-1)}$) est de 499 500, contre près de 50 millions avec _n_ = 10 000 (49 995 000).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Comparaison des coefficients de Pearson, Spearman et Kendall sur deux variables anormalement distribuées](04-bivarieeQuantiQuanti_files/figure-html/fig-PearsonSpearmanKendall-1.png){#fig-PearsonSpearmanKendall fig-align='center' width=75%}\n:::\n:::\n\n\nÀ la lecture des deux histogrammes à la @fig-PearsonSpearmanKendall, il est clair que les variables *densité de population* et *pourcentage de personnes ayant 65 ou plus* sont très anormalement distribuées. Dans ce contexte, l'utilisation du coefficient de Pearson peut nous amener à mésestimer la relation existant entre les deux variables. Notez que les coefficients de Spearman et de Kendall sont tous les deux plus faibles.\n\n### Corrélations robustes (*Biweight midcorrelation*, *Percentage bend correlation* et la corrélation *pi* de Shepherd) {#sec-0434}\n\nDans l'exemple donné à la @fig-ValExtremes, nous avions identifié des valeurs extrêmes et les avons retiré du jeu de données. Cette pratique peut tout à fait se justifier quand les données sont erronées (un capteur de pollution renvoyant une valeur négative, un questionnaire rempli par un mauvais plaisantin, etc.), mais parfois les cas extrêmes font partie du phénomène à analyser. Dans ce contexte, les identifier et les retirer peut paraître arbitraire. Une solution plus élégante est d'utiliser des méthodes dites **robustes**, c'est-à-dire moins sensibles aux valeurs extrêmes. Pour les corrélations, la *Biweight midcorrelation* [@wilcox1994percentage] est au coefficient de Pearson ce que la médiane est à la moyenne. Il est donc pertinent de l'utiliser pour des jeux de données présentant potentiellement des valeurs extrêmes. Elle est calculée comme suit : \n\n$$\n\\begin{aligned}\n&u_{i} = \\frac{x_{i} - med(x)}{9 \\times (med(|x_{i} - med(x)|))} \\text{ et } v_{i} = \\frac{y_{i} - med(y)}{9 \\times (med(|y_{i} - med(y)|))}\\\\\n&w_{i}^{(x)} = (1 - u_{i}^2)^2 I(1 - |u_{i}|) \\text{ et } w_{i}^{(y)} = (1 - v_{i}^2)^2 I(1 - |v_{i}|)\\\\\n&I(x) = \n\\begin{cases}\n1, \\text{si } x = 1\\\\\n0, \\text{sinon}\n\\end{cases}\\\\\n&\\tilde{x}_{i} = \\frac{(x_{i} - med(x))w_{i}^{(x)}}{\\sqrt{(\\sum_{j=1}^m)[(x_{j} - med(x))w_{j}^{(x)}]^2}} \\text{ et } \\tilde{y}_{i} = \\frac{(y_{i} - med(y))w_{i}^{(y)}}{\\sqrt{(\\sum_{j=1}^m)[(y_{j} - med(y))w_{j}^{(y)}]^2}}\\\\\n&bicor(x,y) = \\sum_{i=1}^m \\tilde{x_i}\\tilde{y_i}\n\\end{aligned}\n$$ {#eq-bicor}\n\n\nComme le souligne l'@eq-bicor, la *Biweight midcorrelation* est basée sur les écarts à la médiane, plutôt que sur les écarts à la moyenne.\n\nAssez proche de la *Biweight midcorrelation*, la *Percentage bend correlation* se base également sur la médiane des variables *X* et *Y*. Le principe général est de donner un poids plus faible dans le calcul de cette corrélation à un certain pourcentage des observations (20 % sont généralement recommandés) dont la valeur est éloignée de la médiane. Pour une description complète de la méthode, vous pouvez lire l'article de @wilcox1994percentage.\n\nEnfin, une autre option est l'utilisation de la corrélation $pi$ de Sherphred [@Schwarzkopf2012]. Il s'agit simplement d'une méthode en deux étapes. Premièrement, les valeurs extrêmes sont identifiées à l'aide d'une approche par *bootstrap* utilisant la distance de Mahalanobis (calculant les écarts multivariés entre les observations). Deuxièmement, le coefficient de *Spearman* est calculé sur les observations restantes.\n\nAppliquons ces corrélations aux données précédentes. Notez que ce simple code d'une dizaine de lignes permet d'explorer rapidement la corrélation entre deux variables selon six mesures de corrélation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"correlation\")\ndf1 <- read.csv(\"data/bivariee/sr_rmr_mtl_2016.csv\")\nmethods <- c(\"Pearson\" , \"Spearman\" , \"Biweight\" , \"Percentage\" , \"Shepherd\")\nrs <- lapply(methods, function(m){\n  test <- correlation::cor_test(data = df1, x = \"Hab1000Km2\", y = \"A65plus\", method = m, ci=0.95)\n  return(c(test$r, test$CI_low, test$CI_high))\n  })\ndfCorr <- data.frame(do.call(rbind, rs))\nnames(dfCorr) <- c(\"r\" , \"IC_2.5\" , \"IC_97.5\")\ndfCorr$method <- methods\n\n# Impression du tableau avec le package stargazer\nlibrary(stargazer)\nstargazer(dfCorr, type = \"text\", summary = FALSE, rownames = FALSE, align = FALSE, digits = 3,\n          title = \"Comparaison de différentes corrélations pour les deux variables\")\n```\n:::\n\n::: {#tbl-robcorr .cell tbl-cap='Comparaison de différentes corrélations pour les deux variables'}\n::: {.cell-output-display}\n|      r| IC 2,5 %| IC 97,5 %|Méthode    |\n|------:|--------:|---------:|:----------|\n| -0.158|   -0.219|    -0.095|Pearson    |\n| -0.120|   -0.184|    -0.055|Spearman   |\n| -0.137|   -0.199|    -0.074|Biweight   |\n| -0.174|   -0.235|    -0.111|Percentage |\n| -0.121|   -0.187|    -0.054|Shepherd   |\n:::\n:::\n\n\nIl est intéressant de mentionner que ces trois corrélations sont rarement utilisées malgré leur pertinence dans de nombreux cas d'application. Nous faisons face ici à un cercle vicieux dans la recherche : les méthodes les plus connues sont les plus utilisées, car elles sont plus facilement acceptées par la communauté scientifique. Des méthodes plus élaborées nécessitent davantage de justification et de discussion, ce qui peut conduire à de multiples sessions de corrections/resoumissions pour qu'un article soit accepté, malgré le fait qu'elles puissent être plus adaptées au jeu de données à l'étude.\n\n### Significativité des coefficients de corrélation {#sec-0435} \n\nQuelle que soit la méthode utilisée, il convient de vérifier si le coefficient de corrélation est ou non statistiquement différent de 0. En effet, nous travaillons la plupart du temps avec des données d'échantillonnage, et très rarement avec des populations complètes. En collectant un nouvel échantillon, aurions-nous obtenu des résultats différents? Le calcul de ce degré de significativité permet de quantifier le niveau de certitude quant à l'existence d'une corrélation entre les deux variables, positive ou négative. Cet objectif est réalisé en calculant la valeur de _t_ et le nombre de degrés de liberté : $t=\\sqrt{\\frac{n-2}{1-r^2}}$ et $dl = n-2$ avec $r$ et $n$ étant respectivement le coefficient de corrélation et le nombre d'observations. De manière classique, nous utiliserons la table des valeurs critiques de la distribution de $t$ : si la valeur de $t$ est supérieure à la valeur critique (avec  _p_ = 0,05 et le nombre de degrés de liberté), alors le coefficient est significatif à 5 %. En d'autres termes, si la vraie corrélation entre les deux variables (calculable uniquement à partir des populations complètes) était 0, alors la probabilité de collecter notre échantillon serait inférieure à 5 %. Dans ce contexte, nous pouvons raisonnablement rejeter l'hypothèse nulle (corrélation de 0). \n\nLa courte syntaxe ci-dessous illustre comment calculer la valeur de $t$, le nombre de degrés de liberté et la valeur de _p_ pour une corrélation donnée.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read.csv(\"data/bivariee/sr_rmr_mtl_2016.csv\")\nr <- cor(df$A65plus, df$LogTailInc)     # Corrélation\nn <- nrow(df)                           # Nombre d'observations\ndl <- nrow(df)-2                        # degrés de liberté\nt <-  r*sqrt((n-2)/(1-r^2))             # Valeur de T\np <- 2*(1-pt(abs(t), dl))                # Valeur de p\ncat(\"\\nCorrélation =\", round(r, 4),       \n    \"\\nValeur de t =\", round(t, 4),\n    \"\\nDegrés de liberté =\", dl,\n    \"\\np = \", round(p, 4))        \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCorrélation = -0.0693 \nValeur de t = -2.1413 \nDegrés de liberté = 949 \np =  0.0325\n```\n:::\n:::\n\n\nPlus simplement, la fonction `cor.test` permet d'obtenir en une seule ligne de code le coefficient de corrélation, l'intervalle de confiance à 95 % et les valeurs de _t_ et de _p_, comme illustré dans la syntaxe  ci-dessous. Si l'intervalle de confiance est à cheval sur 0, c'est-à-dire que la borne inférieure est négative et la borne supérieure positive, alors le coefficient de corrélation n'est pas significatif au seuil choisi (95 % habituellement). Dans l'exemple ci-dessous, la relation linéaire entre les deux variables est significativement négative avec une corrélation de Pearson de −0,158 (*p* = 0,000) et un intervalle de confiance à 95 % de −0,219 à −0,095.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Intervalle de confiance à 95 %\ncor.test(df$HabKm2, df$A65plus, conf.level = .95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  df$HabKm2 and df$A65plus\nt = -4.9318, df = 949, p-value = 9.616e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2194457 -0.0954687\nsample estimates:\n       cor \n-0.1580801 \n```\n:::\n\n```{.r .cell-code}\n# Vous pouvez accéder à chaque sortie de la fonction cor.test comme suit :\np <- cor.test(df$HabKm2, df$A65plus)\ncat(\"Valeur de corrélation = \", round(p$estimate,3), \"\\n\",\n    \"Intervalle à 95 % = [\", round(p$conf.int[1],3), \" \", round(p$conf.int[2],3), \"]\", \"\\n\",\n    \"Valeur de t = \", round(p$statistic,3), \"\\n\",\n    \"Valeur de p = \", round(p$p.value,3),\"\\n\", sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValeur de corrélation = -0.158\nIntervalle à 95 % = [-0.219 -0.095]\nValeur de t = -4.932\nValeur de p = 0\n```\n:::\n\n```{.r .cell-code}\n# Corrélation de Spearman\ncor.test(df$HabKm2, df$A65plus, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tSpearman's rank correlation rho\n\ndata:  df$HabKm2 and df$A65plus\nS = 160482182, p-value = 0.0002202\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.1195333 \n```\n:::\n\n```{.r .cell-code}\n# Corrélation de Kendall\ncor.test(df$HabKm2, df$A65plus, method = \"kendall\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKendall's rank correlation tau\n\ndata:  df$HabKm2 and df$A65plus\nz = -3.7655, p-value = 0.0001662\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n        tau \n-0.08157061 \n```\n:::\n:::\n\n\nOn pourra aussi modifier l'intervalle de confiance, par exemple à 90 % ou 99 %. L'intervalle de confiance et le seuil de significativité doivent être définis avant l'étude. Leur choix doit s'appuyer sur les standards de la littérature du domaine étudié, du niveau de preuve attendu et de la quantité de données.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Intervalle à 90 %\ncor.test(df$HabKm2, df$A65plus, method = \"pearson\", conf.level = .90)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  df$HabKm2 and df$A65plus\nt = -4.9318, df = 949, p-value = 9.616e-07\nalternative hypothesis: true correlation is not equal to 0\n90 percent confidence interval:\n -0.2096826 -0.1055995\nsample estimates:\n       cor \n-0.1580801 \n```\n:::\n\n```{.r .cell-code}\n# Intervalle à 99 %\ncor.test(df$HabKm2, df$A65plus, method = \"pearson\", conf.level = .99)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  df$HabKm2 and df$A65plus\nt = -4.9318, df = 949, p-value = 9.616e-07\nalternative hypothesis: true correlation is not equal to 0\n99 percent confidence interval:\n -0.23839910 -0.07561336\nsample estimates:\n       cor \n-0.1580801 \n```\n:::\n:::\n\n\n**Corrélation et _bootstrap_.** Il est possible d'estimer la corrélation en mobilisant la notion de _bootstrap_, soit des méthodes d'inférence statistique basées sur des réplications des données initiales par rééchantillonnage. Concrètement, la méthode du _bootstrap_ permet une mesure de la corrélation avec un intervalle de confiance à partir de _r_ réplications, comme illustré à partir de la syntaxe ci-dessous.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"boot\")\ndf <- read.csv(\"data/bivariee/sr_rmr_mtl_2016.csv\")\n# Fonction pour la corrélation\ncorrelation <- function(df, i, X, Y, cor.type = \"pearson\"){\n  # Paramètres de la fonction :\n  # data : DataFrame\n  # X et Y : noms des variables X et Y\n  # cor.type : type de corrélation : c(\"pearson\" , \"spearman\" , \"kendall\")\n  # i : indice qui sera utilisé par les réplications (à ne pas modifier)\n  cor(df[[X]][i], df[[Y]][i], method=cor.type)\n}\n# Calcul du Bootstrap avec 5000 réplications\ncorBootstraped <- boot(data = df, # nom du tableau\n                     statistic = correlation, # appel de la fonction à répliquer \n                     R = 5000, # nombre de réplications\n                     X = \"A65plus\",\n                     Y = \"HabKm2\", \n                     cor.type = \"pearson\")\n# Histogramme pour les valeurs de corrélation issues du Bootstrap\nplot(corBootstraped)\n# Corrélation \"bootstrapée\"\ncorBootstraped\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = df, statistic = correlation, R = 5000, X = \"A65plus\", \n    Y = \"HabKm2\", cor.type = \"pearson\")\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* -0.1580801 -0.0006694822  0.04048709\n```\n:::\n\n```{.r .cell-code}\n# Intervalle de confiance du bootstrap à 95 %\nboot.ci(boot.out = corBootstraped, conf = 0.95, type = \"all\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 5000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = corBootstraped, conf = 0.95, type = \"all\")\n\nIntervals : \nLevel      Normal              Basic         \n95%   (-0.2368, -0.0781 )   (-0.2407, -0.0839 )  \n\nLevel     Percentile            BCa          \n95%   (-0.2322, -0.0755 )   (-0.2209, -0.0531 )  \nCalculations and Intervals on Original Scale\n```\n:::\n\n```{.r .cell-code}\n# Comparaison de l'intervalle classique basé sur la valeur de T\np <- cor.test(df$HabKm2, df$A65plus)\ncat(round(p$estimate,5), \" [\", round(p$conf.int[1],4), \" \", round(p$conf.int[2],4), \"]\", sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-0.15808 [-0.2194 -0.0955]\n```\n:::\n\n::: {.cell-output-display}\n![Histogramme pour les valeurs de corrélation issues du Bootstrap](04-bivarieeQuantiQuanti_files/figure-html/fig-fig9-1.png){#fig-fig9 fig-align='center' width=75%}\n:::\n:::\n\n\nLe _bootstrap_ renvoie un coefficient de corrélation de Pearson de −0,158. Les intervalles de confiance obtenus à partir des différentes méthodes d'estimation (normale, basique, pourcentage et BCa) ne sont pas à cheval sur 0, indiquant que le coefficient est significatif à 5 %.\n\n### Corrélation partielle {#sec-0436} \n\n::: bloc_objectif\n::: bloc_objectif-header\n::: bloc_objectif-icon\n:::\n**Relation entre deux variables une fois prise en compte une autre variable dite de contrôle**\n:::\n::: bloc_objectif-body\nLa corrélation partielle permet d'évaluer la relation linéaire entre deux variables quantitatives continues, après avoir contrôlé une ou plusieurs autres variables quantitatives (dites variables de contrôle).\n\nEn études urbaines, nous pourrions vouloir vérifier si deux variables sont ou non associées après avoir contrôlé la densité de population ou encore la distance au centre-ville.\n:::\n:::\n\n\nLe coefficient de corrélation partielle peut être calculé pour plusieurs mesures de corrélation (notamment, Pearson, Spearman et Kendall). Variant aussi de −1 à 1, il est calculé comme suit :\n\n$$\nr_{ABC} = \\frac{r_{AB}-r_{AC}r_{BC}}{\\sqrt{(1-r_{AC}^2)(1-r_{BC}^2)}}\n$$ {#eq-corpartielle}\n\navec _A_ et _B_ étant les deux variables pour lesquelles nous souhaitons évaluer la relation linéaire, une fois contrôlée la variable _C_; $r$ étant le coefficient de corrélation (Pearson, Spearman ou Kendall) entre deux variables.\n\nDans l'exemple ci-dessous, nous voulons estimer la relation linéaire entre le pourcentage de personnes à faible revenu et la couverture végétale au niveau des îlots de l'île de Montréal, une fois contrôlée la densité de population. En effet, plus cette dernière est forte, plus la couverture végétale est faible ($r$ de Pearson = −0,563). La valeur du $r$ de Pearson s'élève à −0,513 entre le pourcentage de personnes à faible revenu dans la population totale de l'îlot et la couverture végétale. Une fois la densité de population contrôlée, elle chute à −0,316. Pour calculer la corrélation partielle, nous pouvons utiliser la fonction `pcor.test` du package `ppcor`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"foreign\")\nlibrary(\"ppcor\")\ndfveg <- read.dbf(\"data/bivariee/IlotsVeg2006.dbf\")\n# Corrélation entre les trois variables\nround(cor(dfveg[, c(\"VegPct\", \"Pct_FR\" , \"LogDens\")], method = \"p\"), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        VegPct Pct_FR LogDens\nVegPct   1.000 -0.513  -0.563\nPct_FR  -0.513  1.000   0.513\nLogDens -0.563  0.513   1.000\n```\n:::\n\n```{.r .cell-code}\n# Corrélation partielle avec la fonction pcor.test entre :\n# la couverture végétale de l'îlot (%) et\n# le pourcentage de personnes à faible revenu\n# une fois contrôlée la densité de population\npcor.test(dfveg$Pct_FR, dfveg$VegPct, dfveg$LogDens, method = \"p\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    estimate       p.value statistic     n gp  Method\n1 -0.3155194 8.093159e-235 -33.59772 10213  1 pearson\n```\n:::\n\n```{.r .cell-code}\n# Calcul de la corrélation partielle avec la formule\ncorAB <- cor(dfveg$VegPct, dfveg$Pct_FR, method = \"p\")\ncorAC <- cor(dfveg$VegPct, dfveg$LogDens, method = \"p\")\ncorBC <- cor(dfveg$Pct_FR, dfveg$LogDens, method = \"p\")\nCorP  <- (corAB - (corAC*corBC)) / sqrt((1-corAC^2)*(1-corBC^2))\ncat(\"Corr. partielle avec ppcor  = \", \n    round(pcor.test(dfveg$Pct_FR,  dfveg$VegPct, dfveg$LogDens, method=\"p\")$estimate,5),\n    \"\\nCorr. partielle (formule)  = \", round(CorP, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorr. partielle avec ppcor  =  -0.31552 \nCorr. partielle (formule)  =  -0.31552\n```\n:::\n:::\n\n\n### Mise en œuvre dans R {#sec-0437}\n\nComme vous l'aurez compris, il est possible d'arriver au même résultat par différents moyens. Pour calculer les corrélations, nous avons utilisé jusqu'à présent les fonctions de base `cor` et `cor.test`. Il est aussi possible de recourir à des fonctions d'autres _packages_, dont notamment :\n\n* `Hmisc`, dont la fonction `rcorr` permet de calculer des corrélations de Pearson et de Spearman (mais non celle de Kendall) avec les valeurs de _p_.\n\n* `psych`, dont la fonction `corr.test` permet d'obtenir une matrice de corrélation (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de _p_.\n\n* `stargazer` pour créer de beaux tableaux d'une matrice de corrélation en *HTML*, en *LaTeX* ou en ASCII.\n\n* `apaTables`  pour créer un tableau avec une matrice de corrélation dans un fichier Word.\n\n* `correlation` pour aller plus loin et explorer les corrélations bayésiennes, robustes, non linéaires ou multiniveaux.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf1 <- read.csv(\"data/bivariee/sr_rmr_mtl_2016.csv\")\nlibrary(\"Hmisc\")\nlibrary(\"stargazer\")\nlibrary(\"apaTables\")\nlibrary(\"dplyr\")\n# Corrélations de Pearson et Spearman et valeurs de p \n# avec la fonction rcorr de Hmisc pour deux variables\nHmisc::rcorr(df1$RevMedMen, df1$Locataire, type = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      x     y\nx  1.00 -0.78\ny -0.78  1.00\n\nn= 951 \n\n\nP\n  x  y \nx     0\ny  0   \n```\n:::\n\n```{.r .cell-code}\nHmisc::rcorr(df1$RevMedMen, df1$Locataire, type = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      x     y\nx  1.00 -0.91\ny -0.91  1.00\n\nn= 951 \n\n\nP\n  x  y \nx     0\ny  0   \n```\n:::\n\n```{.r .cell-code}\n# Matrice de corrélation avec la fonction rcorr de Hmisc pour plus de variables\n# Nous créons au préalable un vecteur avec les noms des variables à sélectionner\nVars <- c(\"RevMedMen\" , \"Locataire\", \"LogTailInc\" , \"A65plus\" , \"ImgRec\", \"HabKm2\", \"FaibleRev\")\nHmisc::rcorr(df1[, Vars] %>% as.matrix())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           RevMedMen Locataire LogTailInc A65plus ImgRec HabKm2 FaibleRev\nRevMedMen       1.00     -0.78      -0.46   -0.07  -0.46  -0.49     -0.74\nLocataire      -0.78      1.00       0.56    0.00   0.64   0.71      0.88\nLogTailInc     -0.46      0.56       1.00   -0.07   0.82   0.48      0.62\nA65plus        -0.07      0.00      -0.07    1.00  -0.06  -0.16     -0.01\nImgRec         -0.46      0.64       0.82   -0.06   1.00   0.56      0.68\nHabKm2         -0.49      0.71       0.48   -0.16   0.56   1.00      0.64\nFaibleRev      -0.74      0.88       0.62   -0.01   0.68   0.64      1.00\n\nn= 951 \n\n\nP\n           RevMedMen Locataire LogTailInc A65plus ImgRec HabKm2 FaibleRev\nRevMedMen            0.0000    0.0000     0.0441  0.0000 0.0000 0.0000   \nLocataire  0.0000              0.0000     0.9594  0.0000 0.0000 0.0000   \nLogTailInc 0.0000    0.0000               0.0325  0.0000 0.0000 0.0000   \nA65plus    0.0441    0.9594    0.0325             0.0682 0.0000 0.6796   \nImgRec     0.0000    0.0000    0.0000     0.0682         0.0000 0.0000   \nHabKm2     0.0000    0.0000    0.0000     0.0000  0.0000        0.0000   \nFaibleRev  0.0000    0.0000    0.0000     0.6796  0.0000 0.0000          \n```\n:::\n\n```{.r .cell-code}\n# Avec la fonction corr.test du package psych pour avoir la matrice de corrélation\n# (Pearson, Spearman et Kendall), les intervalles de confiance et les valeurs de p\nprint(psych::corr.test(df[, Vars], \n              method = \"kendall\",  \n              ci = TRUE, alpha = 0.05), short = FALSE) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:psych::corr.test(x = df[, Vars], method = \"kendall\", alpha = 0.05, \n    ci = TRUE)\nCorrelation matrix \n           RevMedMen Locataire LogTailInc A65plus ImgRec HabKm2 FaibleRev\nRevMedMen       1.00     -0.74      -0.43   -0.03  -0.41  -0.45     -0.79\nLocataire      -0.74      1.00       0.45   -0.03   0.47   0.56      0.75\nLogTailInc     -0.43      0.45       1.00   -0.01   0.59   0.41      0.51\nA65plus        -0.03     -0.03      -0.01    1.00  -0.02  -0.08      0.01\nImgRec         -0.41      0.47       0.59   -0.02   1.00   0.50      0.52\nHabKm2         -0.45      0.56       0.41   -0.08   0.50   1.00      0.50\nFaibleRev      -0.79      0.75       0.51    0.01   0.52   0.50      1.00\nSample Size \n[1] 951\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n           RevMedMen Locataire LogTailInc A65plus ImgRec HabKm2 FaibleRev\nRevMedMen        0.0      0.00        0.0    1.00      0   0.00         0\nLocataire        0.0      0.00        0.0    1.00      0   0.00         0\nLogTailInc       0.0      0.00        0.0    1.00      0   0.00         0\nA65plus          0.4      0.37        0.7    0.00      1   0.07         1\nImgRec           0.0      0.00        0.0    0.58      0   0.00         0\nHabKm2           0.0      0.00        0.0    0.01      0   0.00         0\nFaibleRev        0.0      0.00        0.0    0.77      0   0.00         0\n\n Confidence intervals based upon normal theory.  To get bootstrapped values, try cor.ci\n            raw.lower raw.r raw.upper raw.p lower.adj upper.adj\nRvMdM-Loctr     -0.77 -0.74     -0.71  0.00     -0.78     -0.69\nRvMdM-LgTlI     -0.49 -0.43     -0.38  0.00     -0.51     -0.36\nRvMdM-A65pl     -0.09 -0.03      0.04  0.40     -0.11      0.05\nRvMdM-ImgRc     -0.46 -0.41     -0.35  0.00     -0.48     -0.33\nRvMdM-HbKm2     -0.50 -0.45     -0.39  0.00     -0.52     -0.37\nRvMdM-FblRv     -0.81 -0.79     -0.76  0.00     -0.82     -0.75\nLoctr-LgTlI      0.40  0.45      0.50  0.00      0.38      0.52\nLoctr-A65pl     -0.09 -0.03      0.03  0.37     -0.11      0.05\nLoctr-ImgRc      0.42  0.47      0.52  0.00      0.39      0.54\nLoctr-HbKm2      0.51  0.56      0.60  0.00      0.49      0.62\nLoctr-FblRv      0.72  0.75      0.78  0.00      0.71      0.79\nLgTlI-A65pl     -0.08 -0.01      0.05  0.70     -0.09      0.06\nLgTlI-ImgRc      0.54  0.59      0.63  0.00      0.52      0.65\nLgTlI-HbKm2      0.36  0.41      0.46  0.00      0.34      0.48\nLgTlI-FblRv      0.46  0.51      0.56  0.00      0.44      0.58\nA65pl-ImgRc     -0.08 -0.02      0.05  0.58     -0.10      0.06\nA65pl-HbKm2     -0.14 -0.08     -0.02  0.01     -0.17      0.00\nA65pl-FblRv     -0.05  0.01      0.07  0.77     -0.05      0.07\nImgRc-HbKm2      0.46  0.50      0.55  0.00      0.43      0.57\nImgRc-FblRv      0.47  0.52      0.57  0.00      0.45      0.59\nHbKm2-FblRv      0.46  0.50      0.55  0.00      0.43      0.57\n```\n:::\n\n```{.r .cell-code}\n# Création d'un tableau pour une matrice de corrélation\n# changer le paramètre type pour 'html' or 'latex' si souhaité\np <- cor(df1[, Vars], method=\"pearson\")\nstargazer(p, title = \"Correlation Matrix\", type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCorrelation Matrix\n=========================================================================\n           RevMedMen Locataire LogTailInc A65plus ImgRec HabKm2 FaibleRev\n-------------------------------------------------------------------------\nRevMedMen      1      -0.785     -0.461   -0.065  -0.458 -0.489  -0.743  \nLocataire   -0.785       1       0.562    -0.002  0.645  0.708    0.879  \nLogTailInc  -0.461     0.562       1      -0.069  0.816  0.475    0.622  \nA65plus     -0.065    -0.002     -0.069      1    -0.059 -0.158  -0.013  \nImgRec      -0.458     0.645     0.816    -0.059    1    0.561    0.678  \nHabKm2      -0.489     0.708     0.475    -0.158  0.561    1      0.642  \nFaibleRev   -0.743     0.879     0.622    -0.013  0.678  0.642      1    \n-------------------------------------------------------------------------\n```\n:::\n\n```{.r .cell-code}\n# Créer un tableau avec la matrice de corrélation \n# dans un fichier Word (.doc)\napaTables::apa.cor.table(df1[, c(\"RevMedMen\" , \"Locataire\" , \"LogTailInc\")], \n                         filename = \"data/bivariee/TitiLaMatrice.doc\",\n                         show.conf.interval = TRUE,\n                         landscape = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\nMeans, standard deviations, and correlations with confidence intervals\n \n\n  Variable      M        SD       1            2         \n  1. RevMedMen  66065.50 26635.27                        \n                                                         \n  2. Locataire  45.05    26.33    -.78**                 \n                                  [-.81, -.76]           \n                                                         \n  3. LogTailInc 5.54     4.82     -.46**       .56**     \n                                  [-.51, -.41] [.52, .60]\n                                                         \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\nThe confidence interval is a plausible range of population correlations \nthat could have caused the sample correlation (Cumming, 2014).\n * indicates p < .05. ** indicates p < .01.\n \n```\n:::\n:::\n\n\n::: bloc_astuce\n::: bloc_astuce-header\n::: bloc_astuce-icon\n:::\n**Une image vaut mille mots, surtout pour une matrice de corrélation!**\n:::\n::: bloc_astuce-body\nLe package `corrplot` vous permet justement de construire de belles figures avec une matrice de corrélation (figures [-@fig-corrplot1] et [-@fig-corrplot2]). L'intérêt de ce type de figure est de repérer rapidement des associations intéressantes lorsque nous calculons les corrélations entre un grand nombre de variables.\n:::\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"corrplot\")\nlibrary(\"ggpubr\")\ndf1 <- read.csv(\"data/bivariee/sr_rmr_mtl_2016.csv\")\nVars <- c(\"RevMedMen\" , \"Locataire\", \"LogTailInc\" , \"A65plus\" , \"ImgRec\", \"HabKm2\", \"FaibleRev\")\np <- cor(df1[, Vars], method=\"pearson\")\ncouleurs <- colorRampPalette(c(\"#053061\", \"#2166AC\" , \"#4393C3\", \"#92C5DE\",\n                               \"#D1E5F0\", \"#FFFFFF\", \"#FDDBC7\", \"#F4A582\",\n                               \"#D6604D\", \"#B2182B\", \"#67001F\"))\ncorrplot::corrplot(p, addrect = 3, method=\"number\",\n                   diag = FALSE, col=couleurs(100))\n```\n\n::: {.cell-output-display}\n![Matrice de corrélation avec corrplot (chiffres)](04-bivarieeQuantiQuanti_files/figure-html/fig-corrplot1-1.png){#fig-corrplot1 fig-align='center' width=60%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfig2 <- corrplot.mixed(p, lower=\"number\", lower.col = \"black\", \n                      upper = \"ellipse\", upper.col=couleurs(100))\n```\n\n::: {.cell-output-display}\n![Matrice de corrélation avec corrplot (chiffres et ellipses)](04-bivarieeQuantiQuanti_files/figure-html/fig-corrplot2-1.png){#fig-corrplot2 fig-align='center' width=60%}\n:::\n:::\n\n\n\n### Comment rapporter des valeurs de corrélations? {#sec-0438}\n\nBien qu'il n'y ait pas qu'une seule manière de reporter des corrélations, voici quelques lignes directrices pour vous guider : \n\n* Signaler si la corrélation est faible, modérée ou forte.\n\n* Indiquer si la corrélation est positive ou négative. Toutefois, ce n'est pas une obligation, car nous pouvons rapidement le constater avec le signe du coefficient.\n\n* Mettre le *r* et le *p* en italique et en minuscules.\n\n* Deux décimales uniquement pour le $r$ (sauf si une plus grande précision se justifie dans le domaine d'étude).\n\n* Trois décimales pour la valeur de *p*. Si elle est inférieure à 0,001, écrire plutôt *p* < 0,001.\n\n* Indiquer éventuellement le nombre de degrés de liberté, soit $r(dl)=...$\n\nVoici des exemples :\n\n* La corrélation entre les variables *revenu médian des ménages* et *pourcentage de locataires* est fortement négative (*r* = −0,78, *p* < 0,001).\n\n* La corrélation entre les variables *revenu médian des ménages* et *pourcentage de locataires* est forte (*r*(949) = −0,78, *p* < 0,001).\n\n* La corrélation entre les variables *densité de population* et *revenu médian des ménages* est modérée (*r* = −0,49, *p* < 0,001).\n\n* La corrélation entre les variables *densité de population* et *pourcentage de 65 ans et plus* n'est pas significative (*r* = −0,08, *p* = 0,07).\n\nPour un texte en anglais, référez-vous à : [https://www.socscistatistics.com/tutorials/correlation/default.aspx](https://www.socscistatistics.com/tutorials/correlation/default.aspx){target=\"_blank\"}.\n\n## Régression linéaire simple  {#sec-044}\n\n::: bloc_objectif\n::: bloc_objectif-header\n::: bloc_objectif-icon\n:::\n**Comment expliquer et prédire une variable continue en fonction d'une autre variable?**\n:::\n::: bloc_objectif-body\nRépondre à cette question relève de la statistique inférentielle. Il s'agit en effet d'établir une équation simple du type $Y = a + bX$ pour expliquer et prédire les valeurs d'une variable dépendante (*Y*) à partir d'une variable indépendante (*X*). L'équation de la régression est construite grâce à un jeu de données (un échantillon). À partir de cette équation, il est possible de prédire la valeur attendue de *Y* pour n'importe quelle valeur de *X*. Nous appelons cette équation un modèle, car elle cherche à représenter la réalité de façon simplifiée.\n\nLa régression linéaire simple relève ainsi de la statistique inférentielle et se distingue ainsi de la **covariance** ([section @sec-042]) et de la **corrélation** ([section @sec-043]) qui relèvent quant à eux de la statistique bivariée descriptive et exploratoire. \n\nPar exemple, la régression linéaire simple pourrait être utilisée pour expliquer les notes d'un groupe d'étudiants et d'étudiantes à un examen (variable dépendante *Y*) en fonction du nombre d'heures consacrées à la révision des notes de cours (variable indépendante *X*). Une fois l'équation de régression déterminée et si le modèle est efficace, nous pourrons prédire les notes des personnes inscrites au cours la session suivante en fonction du temps qu'ils ou qu'elles prévoient passer à étudier, et ce, avant l'examen. \n\nFormulons un exemple d'application de la régression linéaire simple en études urbaines. Dans le cadre d'une étude sur les îlots de chaleur urbains, la température de surface (variable dépendante) pourrait être expliquée par la proportion de la superficie de l'îlot couverte par de la végétation (variable indépendante). Nous supposons alors que plus cette proportion est importante, plus la température est faible et inversement, soit une relation linéaire négative. Si le modèle est efficace, nous pourrions prédire la température moyenne des îlots d'une autre municipalité pour laquelle nous ne disposons pas d'une carte de température, et repérer ainsi les îlots de chaleur potentiels. Bien entendu, il est peu probable que nous arrivions à prédire efficacement la température moyenne des îlots avec uniquement la couverture végétale comme variable explicative. En effet, bien d'autres caractéristiques de la forme urbaine peuvent influencer ce phénomène comme la densité du bâti, la couleur des toits, les occupations du sol présentes, l'effet des canyons urbains, etc. Il faudrait alors inclure non pas une, mais plusieurs variables explicatives (indépendantes).\n\nAinsi, nous distinguons la **régression linéaire simple** (une seule variable indépendante) de la **régression linéaire multiple** (plusieurs variables indépendantes); cette dernière est largement abordée au [chapitre @sec-chap07].\n:::\n:::\n\nDans cette section, nous décrivons succinctement la régression linéaire simple. Concrètement, nous voyons comment déterminer la droite de régression, interpréter ses différents paramètres du modèle et évaluer la qualité d'ajustement du modèle. Nous n'abordons ni les hypothèses liées au modèle de régression linéaire des moindres carrés ordinaires (MCO) ni les conditions d'application. Ces éléments sont expliqués au [chapitre @sec-chap07], consacré à la régression linéaire multiple.\n\n\n::: bloc_attention\n::: bloc_attention-header\n::: bloc_attention-icon\n:::\n**Corrélation, régression simple et causalité : attention aux raccourcis!**\n:::\n::: bloc_attention-body\nSi une variable *X* explique et prédit efficacement une variable *Y*, cela ne veut pas dire pour autant qu'*X* cause *Y*. Autrement dit, la corrélation, soit le degré d'association entre deux variables, ne signifie pas qu'il existe un lien de causalité entre elles.\n\nPremièrement, la variable explicative (*X*, indépendante) doit absolument précéder la variable à expliquer (*Y*, dépendante). Par exemple, l'âge (*X*) peut influencer le sentiment de sécurité (*Y*). Mais, le sentiment de sécurité ne peut en aucun cas influencer l'âge. Par conséquent, l'âge ne peut conceptuellement pas être la variable dépendante dans cette relation.\n\nDeuxièmement, bien qu'une variable puisse expliquer efficacement une autre variable, elle peut être un **facteur confondant**. Prenons deux exemples bien connus :\n\n* Avoir les doigts jaunes est associé au cancer du poumon. Bien entendu, les doigts jaunes ne causent pas le cancer : c'est un facteur confondant puisque fumer augmente les risques du cancer du poumon et jaunit aussi les doigts.\n\n* Dans un article intitulé *Chocolate Consumption, Cognitive Function, and Nobel Laureates*, Messerli [-@Messerli] a trouvé une corrélation positive entre la consommation de chocolat par habitant et le nombre de prix Nobel pour dix millions d'habitants pour 23 pays. Ce résultat a d'ailleurs été rapporté par de nombreux médias, sans pour autant que Messerli [-@Messerli] et les journalistes concluent à un lien de causalité entre les deux variables :\n\n    - [Radio Canada](https://ici.radio-canada.ca/nouvelle/582457/chocolat-consommateurs-nobels)\n\n    - [La Presse](https://www.lapresse.ca/vivre/sante/nutrition/201210/11/01-4582347-etude-plus-un-pays-mange-de-chocolat-plus-il-a-de-prix-nobel.php)\n\n    - [Le Point](https://www.lepoint.fr/insolite/le-chocolat-dope-aussi-l-obtention-de-prix-nobel-12-10-2012-1516159_48.php). \n\nLes chercheurs et les chercheuses savent bien que la consommation de chocolat ne permet pas d'obtenir des résultats intéressants et de les publier dans des revues prestigieuses; c'est plutôt le café ! Plus sérieusement, il est probable que les pays les plus riches investissent davantage dans la recherche et obtiennent ainsi plus de prix Nobel. Dans les pays les plus riches, il est aussi probable que l'on consomme plus de chocolat, considéré comme un produit de luxe dans les pays les plus pauvres.\n\nPour approfondir le sujet sur la confusion entre corrélation, régression simple et causalité, vous pouvez visionner cette courte [vidéo ludique de vulgarisation](https://www.youtube.com/embed/A-_naeATJ6o).\n\nL'association entre deux variables peut aussi être simplement le fruit du hasard. Si nous explorons de très grandes quantités de données (avec un nombre impressionnant d'observations et de variables), soit une démarche relevant du forage ou de la fouille de données (*data mining* en anglais), le hasard fera que nous risquons d'obtenir des corrélations surprenantes entre certaines variables. Prenons un exemple concret : admettons que nous ayons collecté 100 variables et que nous calculons les corrélations entre chaque paire de variables. Nous obtenons une matrice de corrélation de 100 x 100, à laquelle nous pouvons enlever la diagonale et une moitié de la matrice, ce qui nous laisse un total de 4950 corrélations différentes. Admettons que nous choisissions un seuil de significativité de 5 %, nous devons alors nous attendre à ce que le hasard produise des résultats significatifs dans 5 % des cas. Sur 4950 corrélations, cela signifie qu'environ 247 corrélations seront significatives, et ce, indépendamment de la nature des données. Nous pouvons aisément illustrer ce fait avec la syntaxe  suivante :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"Hmisc\")\nnbVars <- 100 # nous utilisons 100 variables générées aléatoirement pour l'expérience\nnbExperiment <- 1000 # nous reproduirons 1000 fois l'expérience avec les 100 variables\n# Le nombre de variables significatives par expérience est enregistré dans Results\nResults <- c()\n# Itérons pour chaque expérimentation (1000 fois)\nfor(i in 1:nbExperiment){\n  Datas <- list()\n  # Générons 100 variables aléatoires normalement distribuées\n  for (j in 1:nbVars){\n    Datas[[j]] <- rnorm(150)\n  }\n  DF <- do.call(\"cbind\", datas)\n  # Calculons la matrice de corrélation pour les 100 variables\n  cor_mat <- rcorr(DF)\n  # Comptons combien de fois les corrélations étaient significatives\n  Sign <- table(cor_mat$P<0.05)\n  NbPairs <- Sign[[\"TRUE\"]]/2\n  # Ajoutons les résultats dans Results\n  Results <- c(Results, NbPairs)\n}\n# Transformons Results en un DataFrame\ndf <- data.frame(Values = Results)\n# Affichons le résultat dans un graphique\nggplot(df, aes(x = Values)) + \n  geom_histogram(aes(y =..density..), \n                 colour = \"black\", \n                 fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = mean(df$Values), \n                sd = sd(df$Values)), color = \"blue\")+\n  geom_vline(xintercept = mean(df$Values), color = \"red\", size = 1.2)+\n  annotate(\"text\", x=0, y = 0.028, \n           label = paste(\"Nombre moyen de corrélations significatives\\n\n                         sur 1000 réplications : \",\n                         round(mean(df$Values),0), sep = \"\"), hjust=\"left\")+\n  xlab(\"Nombre de corrélations significatives\")+\n  ylab(\"densité\")\n```\n:::\n\n\n![Corrélations significatives obtenues aléatoirement](images/Chap04/ReplicationHist.png){#fig-replicationhist width=\"50%\" fig-align=\"center\"}\n\n:::\n:::\n\n### Principe de base de la régression linéaire simple {#sec-0441}\nLa régression linéaire simple vise à déterminer une droite (une fonction linéaire) qui résume le mieux la relation linéaire entre une variable dépendante (*Y*) et une variable indépendante (*X*) :\n$$\n\\widehat{y_i} = \\beta_{0} + \\beta_{1}x_{i}\n$$ {#eq-regsimple}\n\navec $\\widehat{y_i}$ et $x_{i}$ qui sont respectivement la valeur prédite de la variable dépendante et la valeur de la variable indépendante pour l'observation $i$. $\\beta_{0}$ est la constante (*intercept* en anglais) et représente la valeur prédite de la variable *Y* quand *X* est égale à 0. $\\beta_{1}$ est le coefficient de régression pour la variable *X*, soit la pente de la droite. Ce coefficient nous informe sur la relation entre les deux variables : s’il est positif, la relation est positive; s’il est négatif, la relation est négative; s'il est proche de 0, la relation est nulle (la droite est alors horizontale). Plus la valeur absolue de $\\beta_{1}$ est élevée, plus la pente est forte et plus la variable *Y* varie à chaque changement d’une unité de la variable *X*.\n\nConsidérons un exemple fictif de dix municipalités d'une région métropolitaine pour lesquelles nous disposons de deux variables : le pourcentage de personnes occupées se rendant au travail principalement à vélo et la distance entre chaque municipalité et le centre-ville de la région métropolitaine (@tbl-regfictives).\n\n\n\n::: {#tbl-regfictives .cell tbl-cap='Données fictives sur l\\'utilisation du vélo par municipalité'}\n::: {.cell-output-display}\n|Municipalité | Vélo |   KMCV|\n|:------------|:----:|------:|\n|A            | 12,5 | 14,135|\n|B            | 13,5 | 10,065|\n|C            | 15,8 |  7,762|\n|D            | 15,9 | 11,239|\n|E            | 17,6 |  7,706|\n|F            | 18,5 |  7,195|\n|G            | 21,2 |  7,953|\n|H            | 23,0 |  4,293|\n|I            | 25,3 |  5,225|\n|J            | 30,2 |  2,152|\n:::\n:::\n\n\nD'emblée, à la lecture du nuage de points (@fig-reg), nous décelons une forte relation linéaire négative entre les deux variables : plus la distance entre la municipalité et le centre-ville de la région métropolitaine augmente, plus le pourcentage de cyclistes est faible, ce qui est confirmé par le coefficient de corrélation (*r* = −0,90). La droite de régression (en rouge à la @fig-reg) qui résume le mieux la relation entre `Vélo` (variable dépendante) et `KmCV` (variable indépendante) s'écrit alors : **Vélo = 30,603 − 1,448 x KmCV**.\n\nLa valeur du coefficient de régression ($\\beta_{1}$) est de −1,448. Le signe de ce coefficient décrit une relation négative entre les deux variables. Ainsi, à chaque ajout d'une unité de la distance entre la municipalité et le centre-ville (exprimée en kilomètres), le pourcentage de cyclistes diminue de 1,448. Retenez que l'unité de mesure de la variable dépendante est très importante pour bien interpréter le coefficient de régression. En effet, si la distance au centre-ville n'était pas exprimée en kilomètres, mais plutôt en mètres, $\\beta_1$ serait égal à −0,001448. Dans la même optique, l'ajout de 10 km de distance entre une municipalité et le centre-ville fait diminuer le pourcentage de cyclistes de −14,48 points de pourcentage.\n\nAvec, cette équation de régression, il est possible de prédire le pourcentage de cyclistes pour n'importe quelle municipalité de la région métropolitaine. Par exemple, pour des distances de 5, 10 ou 20 kilomètres, les pourcentages de cyclistes seraient de :\n\n* $\\widehat{y_i} = \\mbox{30,603} + (\\mbox{-1,448} \\times \\mbox{5 km) = 23,363}$\n* $\\widehat{y_i} = \\mbox{30,603} + (\\mbox{-1,448} \\times \\mbox{10 km) = 8,883}$\n* $\\widehat{y_i} = \\mbox{30,603} + (\\mbox{-1,448} \\times \\mbox{20 km) = 1,643}$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Relation linéaire entre l'utilisation du vélo et la distance au centre-ville](04-bivarieeQuantiQuanti_files/figure-html/fig-reg-1.png){#fig-reg fig-align='center' width=65%}\n:::\n:::\n\n\n\n### Formulation de la droite de régression des moindres carrés ordinaires {#sec-0442}\nReste à savoir comment sont estimés les différents paramètres de l'équation, soit $\\beta_0$ et $\\beta_1$. À la @fig-reg2, les points noirs représentent les valeurs observées ($y_i$) et les points bleus, les valeurs prédites ($\\widehat{y_i}$) par l'équation du modèle. Les traits noirs verticaux représentent, pour chaque observation $i$, l'écart entre la valeur observée et la valeur prédite, dénommé résidu ($\\epsilon_i$, prononcez epsilon de _i_ ou plus simplement le résidu pour _i_ ou le terme d'erreur de _i_). Si un point est au-dessus de la droite de régression, la valeur observée est alors supérieure à la valeur prédite ($y_i > \\widehat{y_i}$) et inversement, si le point est au-dessous de la droite ($y_i < \\widehat{y_i}$). Plus cet écart ($\\epsilon_i$) est important, plus l'observation s'éloigne de la prédiction du modèle et, par extension, moins bon est le modèle. Au @tbl-regfictives2, vous constaterez que la somme des résidus est égale à zéro. La méthode des moindres carrés ordinaires (MCO) vise à minimiser les écarts au carré entre les valeurs observées ($y_i$) et prédites ($\\beta_0+\\beta_1 x_i$, soit $\\widehat{y_i}$) :\n\n$$\nmin\\sum_{i=1}^n{(y_i-(\\beta_0+\\beta_1 x_i))^2}\n$$ {#eq-mco}\n\nPour minimiser ces écarts, le coefficient de régression $\\beta_1$ représente le rapport entre la covariance entre *X* et *Y* et la variance de *Y* (@eq-b1), tandis que la constante $\\beta_0$ est la moyenne de la variable *Y* moins le produit de la moyenne de *X* et de son coefficient de régression (@eq-b0).\n\n$$\n\\beta_1 = \\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} = \\frac{cov(X,Y)}{var(X)}\n$$ {#eq-b1}\n\n$$\n\\beta_0 = \\widehat{Y}-\\beta_1 \\widehat{X}\n$$ {#eq-b0}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Droite de régression, valeurs observées, prédites et résidus](04-bivarieeQuantiQuanti_files/figure-html/fig-reg2-1.png){#fig-reg2 fig-align='center' width=65%}\n:::\n:::\n\n::: {#tbl-regfictives2 .cell tbl-cap='Valeurs observées, prédites et résidus'}\n::: {.cell-output-display}\n|Municipalité | Vélo |   KmCV| Valeur prédite| Résidu| Résidu au carré|\n|:------------|:----:|------:|--------------:|------:|---------------:|\n|A            | 12,5 | 14,135|         10,138|  2,362|           5,579|\n|B            | 13,5 | 10,065|         16,031| -2,531|           6,406|\n|C            | 15,8 |  7,762|         19,365| -3,565|          12,709|\n|D            | 15,9 | 11,239|         14,331|  1,569|           2,462|\n|E            | 17,6 |  7,706|         19,446| -1,846|           3,408|\n|F            | 18,5 |  7,195|         20,186| -1,686|           2,843|\n|G            | 21,2 |  7,953|         19,089|  2,111|           4,456|\n|H            | 23,0 |  4,293|         24,388| -1,388|           1,927|\n|I            | 25,3 |  5,225|         23,038|  2,262|           5,117|\n|J            | 30,2 |  2,152|         27,488|  2,712|           7,355|\n|Somme        |  --  |     --|             --|  0,000|          52,262|\n:::\n:::\n\n\n### Mesure de la qualité d'ajustement du modèle {#sec-0443}\nLes trois mesures les plus courantes pour évaluer la qualité d'ajustement d'un modèle de régression linéaire simple sont l'erreur quadratique moyenne (*root-mean-square error* en anglais, *RMSE*), le coefficient de détermination (*R^2^*) et la statistique *F* de Fisher. Pour mieux appréhender le calcul de ces trois mesures, rappelons que l'équation de régression s'écrit : \n\n$$\ny_i = \\beta_0 + \\beta_1 x_1+ \\epsilon_i \\Rightarrow Y= \\beta_0 + \\beta_1 X + \\epsilon\n$$ {#eq-reg2}\n\nElle comprend ainsi une partie de *Y* qui est expliquée par le modèle et une autre partie non expliquée, soit $\\epsilon$, appelée habituellement le terme d'erreur. Ce terme d'erreur pourrait représenter d'autres variables explicatives qui n'ont pas été prises en compte pour prédire la variable indépendante ou une forme de variation aléatoire inexplicable présente lors de la mesure.\n\n$$\nY  = \\underbrace{\\beta_0 + \\beta_1 X}_{\\mbox{partie expliquée par le modèle}}+ \\underbrace{\\epsilon}_{\\mbox{partie non expliquée}}\n$$ {#eq-reg3}\n\n\nPar exemple, pour la municipalité *A* au @tbl-regfictives2, nous avons : $y_A = \\widehat{y}_A - \\epsilon_A \\Rightarrow \\mbox{12,5} = \\mbox{10,138}+\\mbox{2,362}$. Souvenez-vous que la variance d'une variable est la somme des écarts à la moyenne, divisée par le nombre d'observations. Par extension, il est alors possible de décomposer la variance de *Y* comme suit :\n\n$$\n\\underbrace{\\sum_{i=1}^n (y_{i}-\\bar{y})^2}_{\\mbox{variance de Y}} = \\underbrace{\\sum_{i=1}^n (\\widehat{y}_i-\\bar{y})^2}_{\\mbox{var. expliquée}} + \\underbrace{\\sum_{i=1}^n (y_{i}-\\widehat{y})^2}_{\\mbox{var. non expliquée}} \\Rightarrow \nSCT = SCE + SCR\n$$ {#eq-reg4}\n\n\navec :\n\n* *SCT* est la somme des écarts au carré des valeurs observées à la moyenne (_total sum of squares_ en anglais)\n\n* *SCE* est la somme des écarts au carré des valeurs prédites à la moyenne (_regression sum of squares_ en anglais)\n\n* *SCR* est la somme des carrés des résidus (_sum of squared errors_ en anglais).\n\nAutrement dit, la variance totale est égale à la variance expliquée plus la variance non expliquée. Au @tbl-computeR, vous pouvez repérer les valeurs de *SCT*, *SCE* et *SCR* et constater que 279,30 = 227,04 + 52,26 et 27,93 = 22,70 + 5,23.\n\n\n::: {#tbl-computeR .cell tbl-cap='Calcul du coefficient de détermination'}\n::: {.cell-output-display}\n|Municipalité |  $y_i$| $\\widehat{y}_i$| $\\epsilon_i$| $(y_i-\\bar{y})^2$| $(\\widehat{y}_i-y_i)^2$| $\\epsilon_i^2$|\n|:------------|------:|---------------:|------------:|-----------------:|-----------------------:|--------------:|\n|A            |  12,50|           10,14|         2,36|             46,92|                   84,86|           5,58|\n|B            |  13,50|           16,03|        -2,53|             34,22|                   11,02|           6,41|\n|C            |  15,80|           19,37|        -3,57|             12,60|                    0,00|          12,71|\n|D            |  15,90|           14,33|         1,57|             11,90|                   25,19|           2,46|\n|E            |  17,60|           19,45|        -1,85|              3,06|                    0,01|           3,41|\n|F            |  18,50|           20,19|        -1,69|              0,72|                    0,70|           2,84|\n|G            |  21,20|           19,09|         2,11|              3,42|                    0,07|           4,46|\n|H            |  23,00|           24,39|        -1,39|             13,32|                   25,38|           1,93|\n|I            |  25,30|           23,04|         2,26|             35,40|                   13,60|           5,12|\n|J            |  30,20|           27,49|         2,71|            117,72|                   66,22|           7,36|\n|N            |  10,00|              --|           --|                --|                      --|             --|\n|Somme        | 193,50|              --|         0,00|            279,30|                  227,04|          52,26|\n|Moyenne      |  19,35|              --|         0,00|             27,93|                   22,70|           5,23|\n:::\n:::\n\n\n\n**Calcul de l'erreur quadratique moyenne**\n\nLa somme des résidus au carré (*SCR*) divisée par le nombre d'observations représente donc le carré moyen des erreurs (en anglais, *mean square error - MSE*), soit la variance résiduelle du modèle (52,26 / 10 = 5,23). Plus sa valeur est faible, plus le modèle est efficace pour prédire la variable indépendante. L'erreur quadratique moyenne (en anglais, *root-mean-square error - RMSE*) est simplement la racine carrée de la somme des résidus au carré divisée par le nombre d'observations ($n$) :\n\n$$\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n (y_{i}-\\widehat{y})^2}{n}}\n$$ {#eq-reg5}\n\n\nElle représente ainsi une **mesure absolue des erreurs** qui est exprimée dans l'unité de mesure de la variable dépendante. Dans le cas présent, nous avons : $\\sqrt{5,23}=2,29$. Cela signifie qu'en moyenne, l'écart absolu (ou erreur absolue) entre les valeurs observées et prédites est de 2,29 points de pourcentage. De nouveau, une plus faible valeur de **RMSE** indique un meilleur ajustement du modèle. Mais surtout, le RMSE permet d'évaluer avec quelle précision le modèle prédit la variable dépendante. Il est donc particulièrement important si l'objectif principal du modèle est de prédire des valeurs sur un échantillon d'observations pour lequel la variable dépendante est inconnue.\n\n**Calcul du coefficient de détermination**\n\nNous avons largement démontré que la variance totale est égale à la variance expliquée plus la variance non expliquée. La qualité du modèle peut donc être évaluée avec le coefficient de détermination (*R^2^*), soit le rapport entre les variances expliquée et totale : \n\n$$\nR^2 = \\frac{SCE}{SCT} \\mbox{ avec } R^2 \\in \\left[0,1\\right]\n$$ {#eq-reg6}\n\n\nComparativement au RMSE qui est une mesure absolue, le coefficient de détermination est une **mesure relative** qui varie de 0 à 1. Il exprime la proportion de la variance de *Y* qui est expliquée par la variable *X*; autrement dit, plus sa valeur est élevée, plus *X* influence/est capable de prédire *Y*. Dans le cas présent, nous avons : R^2^ = 227,04 / 279,3 = 0,8129, ce qui signale que 81,3 % de la variance du pourcentage de cyclistes est expliquée par la distance entre la municipalité et le centre-ville de la région métropolitaine. Tel que signalé dans la [section @sec-0432], la racine carrée du coefficient de détermination (*R^2^*) est égale au coefficient de corrélation ($r$) entre les deux variables. \n\n**Calcul de la statistique _F_ de Fisher**\n\nLa statistique _F_ de Fisher permet de vérifier la significativité globale du modèle.\n\n$$\nF = (n-2)\\frac{R^2}{1-R^2} = (n-2)\\frac{SCE}{SCR}\n$$ {#eq-reg7}\n\n\nL'hypothèse nulle (*H~0~* avec $\\beta_1=0$) est rejetée si la valeur calculée de *F* est supérieure à la valeur critique de la table *F* avec *1, n-2* degrés de liberté et un seuil $\\alpha$ (*p* = 0,05 habituellement) (voir la table des valeurs critiques de *F*, [section @sec-142]). Notez que nous utilisons rarement la table *F* puisqu'avec la fonction `pf(f obtenu, 1, n-2, lower.tail = FALSE)`, nous obtenons obtient directement la valeur de *p* associée à la valeur de *F*. Concrètement, si le test _F_ est significatif (avec *p* < 0,05), plus la valeur de *F* est élevée, plus le modèle est efficace (et plus le *R^2^* sera également élevé).\n\nNotez que la fonction `summary` renvoie les résultats du modèle, dont notamment le test _F_ de Fisher.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# utiliser la fonction summary\nsummary(modele)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Velo ~ KmCV, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5652 -1.8062  0.0906  2.2241  2.7125 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  30.6032     2.0729  14.763 4.36e-07 ***\nKmCV         -1.4478     0.2456  -5.895 0.000364 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.556 on 8 degrees of freedom\nMultiple R-squared:  0.8129,\tAdjusted R-squared:  0.7895 \nF-statistic: 34.75 on 1 and 8 DF,  p-value: 0.0003637\n```\n:::\n:::\n\n\nDans le cas présent, $F = (10 - 2)\\frac{\\mbox{0,8129}}{\\mbox{1-0,8129}} = (10-2)\\frac{\\mbox{227,04}}{\\mbox{52,26}} = \\mbox{34,75}$ avec une valeur de $\\mbox{p < 0,001}$. Par conséquent, le modèle est significatif.\n\n### Mise en œuvre dans R {#sec-0444}\nComment calculer une régression linéaire simple dans R. Rien de plus simple avec la fonction `lm(formula = y ~ x, data= DataFrame)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf1 <- read.csv(\"data/bivariee/Reg.csv\", stringsAsFactors = FALSE)\n## Création d'un objet pour le modèle\nmonmodele <- lm(Velo ~ KmCV, df1)\n## Résultats du modèle avec la fonction summary\nsummary(monmodele)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Velo ~ KmCV, data = df1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5652 -1.8062  0.0906  2.2241  2.7125 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  30.6032     2.0729  14.763 4.36e-07 ***\nKmCV         -1.4478     0.2456  -5.895 0.000364 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.556 on 8 degrees of freedom\nMultiple R-squared:  0.8129,\tAdjusted R-squared:  0.7895 \nF-statistic: 34.75 on 1 and 8 DF,  p-value: 0.0003637\n```\n:::\n\n```{.r .cell-code}\n## Calcul du MSE et du RMSE\nMSE <- mean(monmodele$residuals^2)\nRMSE <- sqrt(MSE)\ncat(\"MSE = \", round(MSE, 2), \"; RMSE = \", round(RMSE,2), sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE = 5.23; RMSE = 2.29\n```\n:::\n:::\n\n\n### Comment rapporter une régression linéaire simple {#sec-0445}\n\nNous avons calculé une régression linéaire simple pour prédire le pourcentage d'actifs occupés utilisant le vélo pour se rendre au travail en fonction de la distance entre la municipalité et le centre-ville de la région métropolitaine (en kilomètres). Le modèle obtient un *F* de Fisher significatif (*F*(1,8) = 34,75, *p* < 0,001) et un *R^2^* de 0,813. Le pourcentage de cyclistes peut être prédit par l'équation suivante : 30,603 - 1,448 x (distance au centre-ville en km).\n\n## Quiz de révision du chapitre {#sec-045}\n\n\n<div id=\"Chapitre4_QuantiQuanti\" class=\"card\">\n<link type=\"text/javascript\" src=\"libs/quizlib.1.0.1.min.js\"/>\n<script>var Quiz=function(a,b){this.Classes=Object.freeze({QUESTION:\"quizlib-question\",QUESTION_TITLE:\"quizlib-question-title\",QUESTION_ANSWERS:\"quizlib-question-answers\",QUESTION_WARNING:\"quizlib-question-warning\",CORRECT:\"quizlib-correct\",INCORRECT:\"quizlib-incorrect\",TEMP:\"quizlib-temp\"}),this.unansweredQuestionText=\"Question sans réponse !\",this.container=document.getElementById(a),this.questions=[],this.result=new QuizResult,this.answers=b;for(var c=0;c<this.container.children.length;c++)this.container.children[c].classList.contains(Quiz.Classes.QUESTION)&&this.questions.push(this.container.children[c]);if(this.answers.length!=this.questions.length)throw new Error(\"Number of answers does not match number of questions!\")};Quiz.Classes=Object.freeze({QUESTION:\"quizlib-question\",QUESTION_TITLE:\"quizlib-question-title\",QUESTION_ANSWERS:\"quizlib-question-answers\",QUESTION_WARNING:\"quizlib-question-warning\",CORRECT:\"quizlib-correct\",INCORRECT:\"quizlib-incorrect\",TEMP:\"quizlib-temp\"}),Quiz.prototype.checkAnswers=function(a){void 0===a&&(a=!0);for(var b=[],c=[],d=0;d<this.questions.length;d++){var e=this.questions[d],f=this.answers[d],g=[];this.clearHighlights(e);for(var h,i=e.getElementsByClassName(Quiz.Classes.QUESTION_ANSWERS)[0].getElementsByTagName(\"input\"),j=0;j<i.length;j++)h=i[j],\"checkbox\"===h.type||\"radio\"===h.type?h.checked&&g.push(h.value):\"\"!==h.value&&g.push(h.value);1!=g.length||Array.isArray(f)?0===g.length&&b.push(e):g=g[0],c.push(Utils.compare(g,f))}if(0===b.length||!a)return this.result.setResults(c),!0;for(d=0;d<b.length;d++){var k=document.createElement(\"span\");k.appendChild(document.createTextNode(this.unansweredQuestionText)),k.className=Quiz.Classes.QUESTION_WARNING,b[d].getElementsByClassName(Quiz.Classes.QUESTION_TITLE)[0].appendChild(k)}return!1},Quiz.prototype.clearHighlights=function(a){for(var b=a.getElementsByClassName(Quiz.Classes.QUESTION_WARNING);b.length>0;)b[0].parentNode.removeChild(b[0]);var c,d=[a.getElementsByClassName(Quiz.Classes.CORRECT),a.getElementsByClassName(this.Classes.INCORRECT)];for(i=0;i<d.length;i++)for(;d[i].length>0;)c=d[i][0],c.classList.remove(Quiz.Classes.CORRECT),c.classList.remove(Quiz.Classes.INCORRECT);for(var e=a.getElementsByClassName(Quiz.Classes.TEMP);e.length>0;)e[0].parentNode.removeChild(e[0])},Quiz.prototype.highlightResults=function(a){for(var b,c=0;c<this.questions.length;c++)b=this.questions[c],b.getElementsByClassName(Quiz.Classes.QUESTION_TITLE)[0].classList.add(this.result.results[c]?Quiz.Classes.CORRECT:Quiz.Classes.INCORRECT),void 0!==a&&a(this,b,c,this.result.results[c])};var QuizResult=function(){this.results=[],this.totalQuestions=0,this.score=0,this.scorePercent=0,this.scorePercentFormatted=0};QuizResult.prototype.setResults=function(a){this.results=a,this.totalQuestions=this.results.length,this.score=0;for(var b=0;b<this.results.length;b++)this.results[b]&&this.score++;this.scorePercent=this.score/this.totalQuestions,this.scorePercentFormatted=Math.floor(100*this.scorePercent)};var Utils=function(){};Utils.compare=function(a,b){if(a.length!=b.length)return!1;if(Array.isArray(a)&&Array.isArray(b)){for(var c=0;c<a.length;c++)if(a[c]!==b[c])return!1;return!0}return a===b};\n\nfunction showResults(quiz) {\n    // Check answers and continue if all questions have been answered\n    if (quiz.checkAnswers()) {\n        var quizScorePercent = quiz.result.scorePercentFormatted; // The unformatted percentage is a decimal in range 0 - 1\n        var quizResultElement = document.getElementById('quiz-result');\n        quizResultElement.style.display = 'block';\n        document.getElementById('quiz-percent').innerHTML = quizScorePercent.toString();\n\n        // Change background colour of results div according to score percent\n        if (quizScorePercent > 75) quizResultElement.style.backgroundColor = '#4caf50';\n        else if (quizScorePercent > 50) quizResultElement.style.backgroundColor = '#ffc107';\n        else if (quizScorePercent > 25) quizResultElement.style.backgroundColor = '#ff9800';\n        else if (quizScorePercent > 0) quizResultElement.style.backgroundColor = '#f44336';\n\n        // Highlight questions according to whether they were correctly answered. The callback allows us to highlight/show the correct answer\n        quiz.highlightResults(handleAnswers);\n    }\n}\n\n/** Callback for Quiz.highlightResults. Highlights the correct answers of incorrectly answered questions\n * Parameters are: the quiz object, the question element, question number, correctly answered flag\n */\nfunction handleAnswers(quiz, question, no, correct) {\n    if (!correct) {\n        var answers = question.getElementsByTagName('input');\n        for (var i = 0; i < answers.length; i++) {\n            if (answers[i].type === 'checkbox' || answers[i].type === 'radio'){\n                // If the current input element is part of the correct answer, highlight it\n                if (quiz.answers[no].indexOf(answers[i].value) > -1) {\n                    answers[i].parentNode.classList.add(Quiz.Classes.CORRECT);\n                }\n            } else {\n                // If the input is anything other than a checkbox or radio button, show the correct answer next to the element\n                var correctAnswer = document.createElement('span');\n                correctAnswer.classList.add(Quiz.Classes.CORRECT);\n                correctAnswer.classList.add(Quiz.Classes.TEMP); // quiz.checkAnswers will automatically remove elements with the temp class\n                correctAnswer.innerHTML = quiz.answers[no];\n                correctAnswer.style.marginLeft = '10px';\n                answers[i].parentNode.insertBefore(correctAnswer, answers[i].nextSibling);\n            }\n        }\n    }\n}\n\n  var quizz_Chapitre4_QuantiQuanti;\n  window.onload = function() {\n    quizz_Chapitre4_QuantiQuanti = new Quiz('Chapitre4_QuantiQuanti', ['1','3','1','4','3','1','1','2','3','2']);\n  };</script>\n<link rel=\"stylesheet\" type=\"text/css\" src=\"css/quizlib.min.css\"/>\n<div class=\"card quizlib-question\">\n<div class=\"quizlib-question-title\">D'après le nuage de points, les variables X et Y partagent une relation :</div>\n<img src=\"images/quiz/Quiz_Chap04_Question_01.jpg\" alt=\"\" style=\"width: 50%;display: block;margin-left: auto;margin-right: auto;margin-bottom: 0.5em;margin-top: 0.5em;\"/>\n<div>Relisez au besoin la [section @sec-041].</div>\n<div class=\"quizlib-question-answers\">\n<table class=\"table table-sm table-striped small\">\n<tr>\n<td>\n<label>linéaire positive</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q1\" value=\"1\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>linéaire négative</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q1\" value=\"2\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>curvilinéaire</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q1\" value=\"3\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>absence de relation</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q1\" value=\"4\"/>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"card quizlib-question\">\n<div class=\"quizlib-question-title\">D'après le nuage de points, les deux variables partagent une relation :</div>\n<img src=\"images/quiz/Quiz_Chap04_Question_02.jpg\" alt=\"\" style=\"width: 50%;display: block;margin-left: auto;margin-right: auto;margin-bottom: 0.5em;margin-top: 0.5em;\"/>\n<div>Relisez au besoin la [section @sec-041].</div>\n<div class=\"quizlib-question-answers\">\n<table class=\"table table-sm table-striped small\">\n<tr>\n<td>\n<label>linéaire positive</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q2\" value=\"1\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>linéaire négative</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q2\" value=\"2\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>curvilinéaire</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q2\" value=\"3\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>absence de relation</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q2\" value=\"4\"/>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"card quizlib-question\">\n<div class=\"quizlib-question-title\">La valeur de la covariance peut être positive ou négative. Plus sa valeur absolue est élevée :</div>\n<img src=\"images/quiz/Quiz_Chap04_Question_03.jpg\" alt=\"\" style=\"width: 50%;display: block;margin-left: auto;margin-right: auto;margin-bottom: 0.5em;margin-top: 0.5em;\"/>\n<div>Relisez au besoin la [section @sec-0422].</div>\n<div class=\"quizlib-question-answers\">\n<table class=\"table table-sm table-striped small\">\n<tr>\n<td>\n<label>plus la relation linéaire entre les deux variables est importante</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q3\" value=\"1\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>plus la relation linéaire entre les deux variables est faible</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q3\" value=\"2\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>curvilinéaire</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q3\" value=\"3\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>absence de relation</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q3\" value=\"4\"/>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"card quizlib-question\">\n<div class=\"quizlib-question-title\">Les coefficients de corrélation (Pearson, Speaman, etc.) varient de à :</div>\n<div>Relisez au besoin la [section @sec-0433].</div>\n<div class=\"quizlib-question-answers\">\n<table class=\"table table-sm table-striped small\">\n<tr>\n<td>\n<label>0 à 1</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q4\" value=\"1\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>moins l'infini à plus l'infini</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q4\" value=\"2\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>0 à 100</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q4\" value=\"3\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>-1 à 1</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q4\" value=\"4\"/>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"card quizlib-question\">\n<div class=\"quizlib-question-title\">Cette statistique est tributaire des unités de mesure des deux variables. Cet inconvénient s'applique à :</div>\n<div>Relisez au besoin la [section @sec-0422].</div>\n<div class=\"quizlib-question-answers\">\n<table class=\"table table-sm table-striped small\">\n<tr>\n<td>\n<label>la corrélation de Pearson</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q5\" value=\"1\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>la corrélation de Spearman</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q5\" value=\"2\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>la covariance</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q5\" value=\"3\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>la corrélation de Kendall</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q5\" value=\"4\"/>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"card quizlib-question\">\n<div class=\"quizlib-question-title\">À la lecture du nuage de points, la corrélation de Pearson entre les deux variables est proche de :</div>\n<img src=\"images/quiz/Quiz_Chap04_Question_06.jpg\" alt=\"\" style=\"width: 50%;display: block;margin-left: auto;margin-right: auto;margin-bottom: 0.5em;margin-top: 0.5em;\"/>\n<div>Relisez au besoin la [section @sec-041].</div>\n<div class=\"quizlib-question-answers\">\n<table class=\"table table-sm table-striped small\">\n<tr>\n<td>\n<label>0</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q6\" value=\"1\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>-1</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q6\" value=\"2\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>1</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q6\" value=\"3\"/>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"card quizlib-question\">\n<div class=\"quizlib-question-title\">Le coefficient de Spearman est le coefficient de Pearson calculé sur des variables transformées en :</div>\n<div>Relisez au besoin la [section @sec-0433].</div>\n<div class=\"quizlib-question-answers\">\n<table class=\"table table-sm table-striped small\">\n<tr>\n<td>\n<label>rangs</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q7\" value=\"1\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>scores Z (variables centrées réduites)</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q7\" value=\"2\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>sur une échelle 0 à 100</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q7\" value=\"3\"/>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"card quizlib-question\">\n<div class=\"quizlib-question-title\">Dans une régression linéaire, le résidu pour une observation est :</div>\n<div>Relisez au besoin la [section @sec-0442].</div>\n<div class=\"quizlib-question-answers\">\n<table class=\"table table-sm table-striped small\">\n<tr>\n<td>\n<label>la différence entre la variance de Y et la variance expliquée</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q8\" value=\"1\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>la différence entre la valeur observée et la valeur prédite</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q8\" value=\"2\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>le carré de la différence entre la valeur observée et la valeur prédite</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q8\" value=\"3\"/>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"card quizlib-question\">\n<div class=\"quizlib-question-title\">Le coefficient de détermination (R2) varie de :</div>\n<div>Relisez au besoin la [section @sec-0443].</div>\n<div class=\"quizlib-question-answers\">\n<table class=\"table table-sm table-striped small\">\n<tr>\n<td>\n<label>-1 à 1</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q9\" value=\"1\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>-100 à 100</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q9\" value=\"2\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>0 à 1</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q9\" value=\"3\"/>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"card quizlib-question\">\n<div class=\"quizlib-question-title\">Un facteur confondant est une sorte de fondant au chocolat?</div>\n<div>Relire le deuxième encadré à la [section @sec-044].</div>\n<div class=\"quizlib-question-answers\">\n<table class=\"table table-sm table-striped small\">\n<tr>\n<td>\n<label>Vrai</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q10\" value=\"1\"/>\n</td>\n</tr>\n<tr>\n<td>\n<label>Faux</label>\n</td>\n<td>\n<input type=\"radio\" name=\"q10\" value=\"2\"/>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<button type=\"button\" onclick=\"showResults(quizz_Chapitre4_QuantiQuanti);\" id=\"buttonID\">Vérifier votre résultat</button>\n<div id=\"quiz-result\" class=\"card\">\n<span id=\"quiz-percent\"></span>\n</div>\n</div>\n",
    "supporting": [
      "04-bivarieeQuantiQuanti_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}